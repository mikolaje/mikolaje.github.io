<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Tradingview pine script 学习笔记</title>
    <url>/2021/tradingview_pine_script.html</url>
    <content><![CDATA[<h2 id="表达式、声明-（Expressions-declarations-and-statements）"><a href="#表达式、声明-（Expressions-declarations-and-statements）" class="headerlink" title="表达式、声明 （Expressions, declarations and statements）"></a>表达式、声明 （Expressions, declarations and statements）</h2><h3 id="表达式"><a href="#表达式" class="headerlink" title=":=表达式"></a><code>:=</code>表达式</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">:=</div><div class="line">这个表达方式在之前的python和JavaScript我都没看到过。但好像python3.8会引入这个运算符</div><div class="line"></div><div class="line">在Python3.8中的介绍是这样的：</div><div class="line">正式地，这运算符被叫做：assignment expression</div><div class="line">非正式地：这运算符被称为：walrus operator。海象运算符，因为长得像海象，哈哈。</div><div class="line">它用来赋值，同时也可以计算表达式。</div><div class="line"></div><div class="line"></div><div class="line">我们回到pine script：</div><div class="line">:= 必须用于给一个变量赋一个新的值，这个变量必须在你赋值之前声明</div><div class="line">变量的类型在声明的时候就被定义了。类型要和声明的类型一致，不然的话会报错。</div><div class="line"></div><div class="line">赋值例子:</div><div class="line"></div><div class="line">//@version=4</div><div class="line">study(&quot;My Script&quot;)</div><div class="line">price = close</div><div class="line">if hl2 &gt; price</div><div class="line">    price := hl2</div><div class="line">plot(price)</div></pre></td></tr></table></figure>
<h3 id="if-statement"><a href="#if-statement" class="headerlink" title="if statement"></a>if statement</h3><p>大致形势是这样的：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"></div><div class="line">&lt;var_declarationX&gt; = if &lt;condition&gt;</div><div class="line">    &lt;var_decl_then0&gt;</div><div class="line">    &lt;var_decl_then1&gt;</div><div class="line">    ...</div><div class="line">    &lt;var_decl_thenN&gt;</div><div class="line">else if [optional block]</div><div class="line">    &lt;var_decl_else0&gt;</div><div class="line">    &lt;var_decl_else1&gt;</div><div class="line">    ...</div><div class="line">    &lt;var_decl_elseN&gt;</div><div class="line">else</div><div class="line">    &lt;var_decl_else0&gt;</div><div class="line">    &lt;var_decl_else1&gt;</div><div class="line">    ...</div><div class="line">    &lt;var_decl_elseN&gt;</div><div class="line">    &lt;return_expression_else&gt;</div><div class="line"></div><div class="line">例子：</div><div class="line">// This code compiles</div><div class="line">x = if close &gt; open</div><div class="line">    close</div><div class="line">else</div><div class="line">    open</div><div class="line">// This code doesn&apos;t compile</div><div class="line">x = if close &gt; open</div><div class="line">    close</div><div class="line">else</div><div class="line">    &quot;open&quot;</div></pre></td></tr></table></figure></p>
<h2 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h2><p>总的来说和Python以及JavaScript的语法都有些类似，我这里之说下不一样的。</p>
<h3 id="三元条件运算符-Ternary-conditional-operator"><a href="#三元条件运算符-Ternary-conditional-operator" class="headerlink" title="三元条件运算符 Ternary conditional operator."></a>三元条件运算符 Ternary conditional operator.</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">expr1 ? expr2 : expr3</div><div class="line"></div><div class="line"></div><div class="line">// Draw circles at the bars where open crosses close</div><div class="line">s2 = cross(open, close) ? avg(open,close) : na</div><div class="line">plot(s2, style=plot.style_circles, linewidth=2, color=color.red)</div><div class="line"></div><div class="line">// Combination of ?: operators for &apos;switch&apos;-like logic</div><div class="line">c = timeframe.isintraday ? color.red : timeframe.isdaily ? color.green : timeframe.isweekly ? color.blue : color.gray</div><div class="line">plot(hl2, color=c)</div><div class="line"></div><div class="line">如果expr1 成立的话，返回expr2，否则返回expr3. </div><div class="line">0被认为是false</div><div class="line">注意的是：</div><div class="line">如果你不用else branch的话，在else branch里使用na</div></pre></td></tr></table></figure>
<h3 id="For循环"><a href="#For循环" class="headerlink" title="For循环"></a>For循环</h3><p>To have access to and use the for statement, one should specify the version &gt;= 2 of Pine Script language in the very first line of code, for example: //@version=4<br>var_declarationX = for counter = from_num to to_num [by step_num]<br>  var_decl0<br>  var_decl1<br>  …<br>  continue<br>  …<br>  break<br>  …<br>  var_declN<br>  return_expression<br>where:<br>counter - a variable, loop counter.<br>from_num - start value of the counter<br>to_num - end value of the counter. When the counter becomes greater than to_num (or less than to_num in case from_num &gt; to_num) the loop is broken.<br>step_num - loop step. Can be omitted (in the case loop step = 1).<br>If from_num is greater than to_num loop step will change direction automatically, no need to specify negative numbers.<br>var_decl0, … var_declN, return_expression - body of the loop. It must be shifted by 4 spaces or 1 tab.<br>return_expression - returning value. When a loop is finished or broken, the returning value is given to the var_declarationX.<br>continue - a keyword. Can be used only in loops. It switches the loop to next iteration.<br>break - a keyword. Can be used only in loops. It breaks the loop.</p>
<h3 id="var"><a href="#var" class="headerlink" title="var"></a>var</h3><p>var是用在赋值和一次性初始化变量的关键字<br>通常，一个不包括var关键字的变量赋值语法会导致变量值被一次更新都会被重写。相反，用var关键字赋值的话就可以保留状态</p>
<h2 id="内置variable"><a href="#内置variable" class="headerlink" title="内置variable"></a>内置variable</h2><h3 id="bar-index"><a href="#bar-index" class="headerlink" title="bar_index"></a>bar_index</h3><p>就是bar的序号。比如上市第一天为1，第二天的为2.<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">//plot(bar_index)</div><div class="line">//打印出500天后的收盘价</div><div class="line">plot(bar_index &gt; 500 ? close : 0)</div></pre></td></tr></table></figure></p>
<h3 id="close"><a href="#close" class="headerlink" title="close"></a>close</h3><p>当前收盘价</p>
<h3 id="high"><a href="#high" class="headerlink" title="high"></a>high</h3><p>最高价</p>
<h3 id="strategy-equity"><a href="#strategy-equity" class="headerlink" title="strategy.equity"></a>strategy.equity</h3><p>当前资产 </p>
<h3 id="strategy-position-size"><a href="#strategy-position-size" class="headerlink" title="strategy.position_size"></a>strategy.position_size</h3><p>当前仓位信息, 该变量等于0.0，说明空仓，大于0.0多头仓位，小于0.0空头仓位。</p>
<h2 id="内置方法"><a href="#内置方法" class="headerlink" title="内置方法"></a>内置方法</h2><h3 id="array-new-float"><a href="#array-new-float" class="headerlink" title="array.new_float"></a>array.new_float</h3><p>The function creates a new array object of float type elements.<br>方法创建一个float类型的数组<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">//@version=4</div><div class="line">study(&quot;array.new_float example&quot;)</div><div class="line">length = 5</div><div class="line">a = array.new_float(length, close)</div><div class="line">plot(array.sum(a) / length)</div></pre></td></tr></table></figure></p>
<h3 id="array-pop"><a href="#array-pop" class="headerlink" title="array.pop"></a>array.pop</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">//@version=4</div><div class="line">study(&quot;array.pop example&quot;)</div><div class="line">a = array.new_float(5,high)</div><div class="line">removedEl = array.pop(a)</div><div class="line">plot(array.size(a))</div><div class="line">plot(removedEl)</div></pre></td></tr></table></figure>
<h3 id="array-push"><a href="#array-push" class="headerlink" title="array.push"></a>array.push</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">//@version=4</div><div class="line">study(&quot;array.push example&quot;)</div><div class="line">a = array.new_float(5, 0)</div><div class="line">array.push(a, open)</div><div class="line">plot(array.get(a, 5))</div></pre></td></tr></table></figure>
<h3 id="array-range"><a href="#array-range" class="headerlink" title="array.range"></a>array.range</h3><p>The function returns the difference between the min and max values from a given array.<br>方法返回数组中最大和最小数值的差值<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">//@version=4</div><div class="line">study(&quot;array.range example&quot;)</div><div class="line">a = array.new_float(0)</div><div class="line">for i = 0 to 9</div><div class="line">	array.push(a, close[i])</div><div class="line">plot(array.range(a))</div></pre></td></tr></table></figure></p>
<h3 id="array-sort"><a href="#array-sort" class="headerlink" title="array.sort"></a>array.sort</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">//@version=4</div><div class="line">study(&quot;array.sort example&quot;)</div><div class="line">a = array.new_float(0,0)</div><div class="line">for i = 0 to 5</div><div class="line">	array.push(a, high[i])</div><div class="line">array.sort(a, order.descending)</div><div class="line">if barstate.islast</div><div class="line">	label.new(bar_index, close, tostring(a))</div></pre></td></tr></table></figure>
<h3 id="offset"><a href="#offset" class="headerlink" title="offset"></a>offset</h3><p>Shifts series x on the y bars to the right.<br>ARGUMENTS<br>source (series) Series of values to process.<br>offset (integer) Number of bars to offset, must be a positive number. Negative offsets are not supported.</p>
<h3 id="rsi"><a href="#rsi" class="headerlink" title="rsi"></a>rsi</h3><p>Relative strength index<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">plot(rsi(close, 7))</div><div class="line"></div><div class="line">// same on pine, but less efficient</div><div class="line">pine_rsi(x, y) =&gt; </div><div class="line">    u = max(x - x[1], 0) // upward change</div><div class="line">    d = max(x[1] - x, 0) // downward change</div><div class="line">    rs = rma(u, y) / rma(d, y)</div><div class="line">    res = 100 - 100 / (1 + rs)</div><div class="line">    res</div><div class="line"></div><div class="line">plot(pine_rsi(close, 7))</div></pre></td></tr></table></figure></p>
<h3 id="crossover"><a href="#crossover" class="headerlink" title="crossover"></a>crossover</h3><p>黄金交叉</p>
<h3 id="crossunder"><a href="#crossunder" class="headerlink" title="crossunder"></a>crossunder</h3><p>死亡交叉</p>
<h3 id="input"><a href="#input" class="headerlink" title="input"></a>input</h3><p>给你的代码指标添加输入。Script input和内置的技术分析指标input完全式样的。</p>
<p>当我们策略写好之后，需要调试一些参数，那么就需要让这些参数暴露出来，这样我们就可以直接在配置里面改参数，然后查看对应的策略回测数据。</p>
<ul>
<li>参数解析<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">defval，类型必须符合type参数定义的类型</div><div class="line">defval决定了在scripts里&quot;settings/inputs&quot;标签里的input变量</div><div class="line"></div><div class="line"></div><div class="line">title 标题，没啥好说的</div><div class="line"></div><div class="line">type (const string) Input type. </div><div class="line">可能选项是</div><div class="line">input.bool, input.integer, input.float, input.string, input.symbol, input.resolution, input.session, input.source, input.color, input.time.</div><div class="line"></div><div class="line">minval (const integer, float) </div><div class="line">input变量最小的可能值。这个参数只有在input 类型为input.integer or input.float.才有效</div><div class="line"></div><div class="line">maxval (const integer, float)</div><div class="line">input变量最大的可能值。这个参数只有在input 类型为input.integer or input.float.才有效</div><div class="line"></div><div class="line">confirm (const bool) </div><div class="line">如果true，用户在指标在添加到图表之前会被询问确认。默认为false。</div><div class="line"></div><div class="line">step (const integer, float) </div><div class="line">Step value to use for incrementing/decrementing input from format dialog. Default value is 1. This argument is used only for input types input.integer and input.float.</div><div class="line">用于增量或减量input，只在input type为input.integer and input.float.时候有效。默认为1</div></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">b = input(title=&quot;On/Off&quot;, type=input.bool, defval=true)</div><div class="line">plot(b ? open : na)</div><div class="line"></div><div class="line">i = input(title=&quot;Offset&quot;, type=input.integer, defval=7, minval=-10, maxval=10)</div><div class="line">plot(offset(close, i))</div><div class="line"></div><div class="line">f = input(title=&quot;Angle&quot;, type=input.float, defval=-0.5, minval=-3.14, maxval=3.14, step=0.02)</div><div class="line">plot(sin(f) &gt; 0 ? close : open)</div><div class="line"></div><div class="line">sym = input(title=&quot;Symbol&quot;, type=input.symbol, defval=&quot;DELL&quot;)</div><div class="line">res = input(title=&quot;Resolution&quot;, type=input.resolution, defval=&quot;60&quot;)</div><div class="line">c = input(title=&quot;Plot Color&quot;, type=input.color, defval=color.red)</div><div class="line">plot(close, color=c)</div><div class="line">plot(security(sym, res, close), color=color.green)</div><div class="line"></div><div class="line">s = input(title=&quot;Session&quot;, defval=&quot;24x7&quot;, options=[&quot;24x7&quot;, &quot;0900-1300&quot;, &quot;1300-1700&quot;, &quot;1700-2100&quot;])</div><div class="line">plot(time(timeframe.period, s))</div><div class="line"></div><div class="line">src = input(title=&quot;Source&quot;, type=input.source, defval=close)</div><div class="line">plot(src)</div><div class="line"></div><div class="line">date1 = input(title=&quot;Date&quot;, type=input.time, defval=timestamp(&quot;20 Feb 2020 00:00 +0300&quot;))</div><div class="line">plot(date1)</div><div class="line"></div><div class="line">date2 = input(title=&quot;Date&quot;, type=input.time, defval=timestamp(&quot;2020-02-20T00:00+03:00&quot;))</div><div class="line">plot(date2)</div></pre></td></tr></table></figure>
<h3 id="security"><a href="#security" class="headerlink" title="security"></a>security</h3><p>Request another symbol/resolution<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">s = security(&quot;MSFT&quot;, &quot;D&quot;, close) // 1 Day</div><div class="line">plot(s)</div><div class="line"></div><div class="line">expr = sma(close, 10)</div><div class="line">s1 = security(&quot;AAPL&quot;, &quot;240&quot;, expr) // 240 Minutes</div><div class="line">plot(s1)</div><div class="line"></div><div class="line">// To avoid difference in calculation on history/realtime you can request not latest values and use merge strategy flags as follows:</div><div class="line">s2=security(syminfo.tickerid, &quot;D&quot;, close[1], barmerge.gaps_off, barmerge.lookahead_on)</div><div class="line">plot(s2)</div><div class="line">f() =&gt; [open, high]</div><div class="line">[o, h] = security(syminfo.tickerid, &quot;D&quot;, f())</div><div class="line">[l, c] = security(syminfo.tickerid, &quot;D&quot;, [low, close])</div><div class="line">plot((o + h + l + c) / 4)</div></pre></td></tr></table></figure></p>
<h3 id="strategy"><a href="#strategy" class="headerlink" title="strategy"></a>strategy</h3><p>strategy是关于策略的方法，这个是非常重要的，一定要学会<br>The function sets a number of strategy properties.</p>
<p>参数解析：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">这个方法参数特多</div><div class="line">strategy(title, shorttitle, overlay, format, precision, scale, pyramiding, calc_on_order_fills, calc_on_every_tick, max_bars_back, backtest_fill_limits_assumption, default_qty_type, default_qty_value, initial_capital, currency, max_lines_count, max_labels_count, slippage, commission_type, commission_value, process_orders_on_close, close_entries_rule, margin_long, margin_short, max_boxes_count) → void</div><div class="line">EXAMPLE</div><div class="line">strategy(title=&apos;MyStrategy&apos;)</div><div class="line">strategy(title=&quot;MyStrategy&quot;, shorttitle=&quot;MS&quot;, pyramiding = 10)</div><div class="line"></div><div class="line">ARGUMENTS</div><div class="line">title (const string) 没啥好说的</div><div class="line">shorttitle (const string) 也没啥好说的，会显示在chart legend上。study short title that would be seen in the chart legend. </div><div class="line">overlay (const bool) 如果true的话，会在主series中(也就就是主K线图）增加一个覆盖(叠在一起）。如果false的话，只会在一个另外独立的chart面板上增加。默认为false</div><div class="line">precision (const integer) 小数点精度</div><div class="line">scale (const integer) 价格范围</div><div class="line">pyramiding (const integer) 最大允许的在同一个方向上的entry数，如果设置为0的话，只有一个entry在同一个方向上下单。默认value为0.</div><div class="line">calc_on_order_fills (const bool) 如果设置为true的话，策略会在下单后重新计算一次intrabar。默认值为false</div><div class="line">calc_on_every_tick (const bool) 额外的intrabar策略计算。如果设置为true的话，策略会计算每一个实时tick，而不是bars的收盘价。参数不会影响策略对历史数据的计算。</div><div class="line">max_bars_back (const integer) 历史引用中，一个策略可用bars的最大数量。</div><div class="line">backtest_fill_limits_assumption (const integer) 限价订单执行假设。仅当市场价格超过限额订单水平指定的ticks数时，限额订单才在栏内填写。</div><div class="line"></div><div class="line"></div><div class="line">default_qty_type (const string) Parameter to determine the number of contracts/shares/lots/units to trade, if the &apos;qty&apos; = &apos;NaN&apos;. The allowed values are: strategy.fixed (fixed quantity by default), strategy.cash (specified in currency of the symbol and the amount is converted into quantity), strategy.percent_of_equity (% of currently available equity).</div><div class="line">default_qty_value (const float) Number of contracts/shares/lots/units if &apos;default_qty_type&apos;=strategy.fixed is used; or amount of cash in currency of the symbol if &apos;default_qty_type&apos;=strategy.cash is used; or number of percents of currently available equity if &apos;default_qty_type&apos;=strategy.percent_of_equity is used.</div><div class="line">currency (const string) Account currency for this strategy. Possible values are: NONE, USD, EUR, AUD, GBP, NZD, CAD, CHF, HKD, JPY, NOK, SEK, SGD, TRY, ZAR</div><div class="line">linktoseries (const bool) if true then the study will be always on the same pane and same price scale as the main series. Should be used only in combination with &apos;overlay=true&apos;. Default is false.</div><div class="line">slippage (const integer) Slippage in ticks to be added to/subtracted from the fill price of buy/sell market or stop orders. If mintick=0.01 and slippage=5, the amount of slippage will be 5*0.01=0.05.</div><div class="line">commission_type (const string) Commission type for an order. The allowed values are: strategy.commission.percent (a percentage of the cash volume of order), strategy.commission.cash_per_contract (money displayed in the account currency per contract), strategy.commission.cash_per_order (money displayed in the account currency per order).</div><div class="line">commission_value (const float) Commission value for an order. Depending on the type selected (commission_type) includes percentage or money.</div><div class="line">process_orders_on_close (const bool) When set to `true`, generates an additional attempt to execute orders after a bar closes and strategy calculations are completed. If the orders are market orders, the broker emulator executes them before the next bar&apos;s open. If the orders are conditional on price, they will only be filled if the price conditions are met. This option is useful if you wish to close positions on the current bar. The default value is &apos;false&apos;.</div><div class="line">close_entries_rule (const string) Determines the order in which orders are closed. Allowed values are: &apos;FIFO&apos; or &apos;ANY&apos;. FIFO (First-In, First-Out) means that when several trades are open, the earliest trades must be closed first. This rule applies to stocks, futures and US forex (NFA Compliance Rule 2-43b). &apos;ANY&apos; means that trades may be closed in any order; this is allowed in non-US forex. The default value is &apos;FIFO&apos;.</div><div class="line">max_lines_count (const integer) The number of last line drawings displayed. The default value is 50 and the maximum allowed is 500.</div><div class="line">max_labels_count (const integer) The number of last label drawings displayed. The default value is 50 and the maximum allowed is 500.</div><div class="line">margin_long (const integer) Margin long is the percentage of the purchase price of a security that must be covered by cash or collateral for long positions. The default value is 100 and must be a non negative integer.</div><div class="line">margin_short (const integer) Margin short is the percentage of the purchase price of a security that must be covered by cash or collateral for short positions. The default value is 100 and must be a non negative integer.</div><div class="line">max_boxes_count (const integer) The number of last box drawings displayed. The default value is 50 and the maximum allowed is 500.</div><div class="line">REMARKS</div><div class="line">Every strategy script must have one strategy call.</div><div class="line">PineScript code that uses argument calc_on_every_tick=true could calculate differently on history and real-time data.</div><div class="line">When using non-standard types of chart as a basis for strategy, you need to realize that the result will be different. The orders will be executed at the prices of this chart (e.g.for Heikin Ashi it’ll take Heikin Ashi prices (the average ones) not the real market prices). Therefore we highly recommend you to use standard chart type for strategies.</div></pre></td></tr></table></figure></p>
<h4 id="strategy-entry"><a href="#strategy-entry" class="headerlink" title="strategy.entry"></a>strategy.entry</h4><p>这是进入市场的命令。<br>如果具有相同ID的订单已经挂起，则可修改订单。 如果没有指定ID的订单，则会发出新的订单。<br>要停用进场指令，应使用命令strategy.cancel或strategy.cancel_all。 与函数strategy.order相比，strategy.entry功能受金字塔影响，可以正确反转市场位置。<br>如果“Limit”和“stop”参数均为“NaN”，则订单类型为市场订单。</p>
<h4 id="strategy-exit"><a href="#strategy-exit" class="headerlink" title="strategy.exit"></a>strategy.exit</h4><p>这是一个退出指定进场或整个市场地位的命令。如果函数strategy.exit被调用一次，则只会退出一次。 如果要退出多次，应该多次调用命令strategy.exit。</p>
<p>参数<br>id (series[string]) 必要参数。 订单标识符。 可以通过引用其标识来取消或修改订单。<br>from_entry (series[string]) 可选参数。以指定进场指令标识符退出。 要退出所有头寸，应使用空字符串。 默认值为空字符串。<br>qty (float) 可选参数。退出交易的合约/股数/手数/单位的数量。默认值为’NaN’。<br>qty_percent (float) 可选参数。定义退出交易的已输入的合约/股数/手数/单位的百分比。当其值不是NaN时，其优先级高于’qty’参数。其值的范围可以从0到100。如果’qty’是NaN，则默认值’qty_percent’是100。<br>profit (float) 可选参数。 利润目标(以点表示)。 如果已指定，当达到指定的利润额(点)时，则以限价订单退出市场头寸。 默认值为“NaN”。<br>limit (float) 可选参数。 利润目标(需指定价格)。 若已指定，则以指定价格(或更好)退出市场头寸。 参数’limit’的优先级高于参数’profit’的优先级(若值非’NaN’，则’limit’取代’profit’)。 默认值为“NaN”。<br>loss (float) 可选参数。 止损(以点表示)。 如果已指定，当达到指定的亏损额(点)时，则以停损单退出市场头寸。 默认值为“NaN”。<br>stop (float) 可选参数。 止损(需指定价格)。 如果已指定，则将以指定价格(或更差)退出市场头寸。 参数’止损’的优先级高于参数’损失’的优先级(若值非’NaN’，则’止损’代替’损失’)。 默认值为“NaN”。<br>trail_price (float) 可选参数。跟踪止损激活水平(需指定价格)。如果已指定，当达到指定价格水平时，将放置跟踪止损单。在“trail_offset”参数中指定用于确定跟踪止损单初始价格的偏移量(以点计)：X 点低于激活水平以退出多头; X点高于激活水平以退出空头。默认值为“NaN”。<br>trail_points (float) 可选参数。跟踪止损激活水平(利润以点表示)。如果已指定，当达到已计算价格水平(指定利润金额)时，将放置跟踪止损单。在“trail_offset”参数中指定用于确定跟踪止损单初始价格的偏移量(以点计)：X 点低于激活水平以退出多头; X点高于激活水平以退出空头。默认值为“NaN”。<br>trail_offset (float) 可选参数。跟踪止损激活水平(以点表示)。以点计的偏移量用于确定跟踪止损单的初始价格：X 点低于’trail_price’ or ‘trail_points’以退出多头; X点高于 ‘trail_price’ or ‘trail_points’以退出空头。默认值为“NaN”。<br>oca_name (string) 可选参数。OCA group的名称 (oca_type = strategy.oca.reduce) 获利目标，止损/跟踪止损。如果未指定名称，将自动生成该名称。<br>comment (string) 可选参数。订单的其他说明。<br>when (bool) 可选参数。订单的状况。若为”true”，订单被放置。若为”false”，则没有任何事情发生(之前放置的相同 ID 的订单没有被撤销)。默认值为”true”。<br>alert_message (string) 当在“创建警报”对话框的“消息”字段中使用占位符时，一个可选参数。</p>
<h4 id="strategy-close"><a href="#strategy-close" class="headerlink" title="strategy.close"></a>strategy.close</h4><p>退出entry<br>如果有多个entry 订单都是同一个ID的话，所有都会一次性退出。<br>如果没有open的带有指定ID的entry触发的话，这个命令将不会生效。</p>
<h4 id="strategy-close-all"><a href="#strategy-close-all" class="headerlink" title="strategy.close_all"></a>strategy.close_all</h4><p>退出你的所有仓位。</p>
<h4 id="strategy-order"><a href="#strategy-order" class="headerlink" title="strategy.order"></a>strategy.order</h4><p>加仓</p>
<h2 id="问题与思考"><a href="#问题与思考" class="headerlink" title="问题与思考"></a>问题与思考</h2><h3 id="strategy-close-与-strategy-exit的区别"><a href="#strategy-close-与-strategy-exit的区别" class="headerlink" title="strategy.close 与 strategy.exit的区别"></a>strategy.close 与 strategy.exit的区别</h3><ul>
<li>close<br>Is a command to exit from the entry with the specified ID.<br>退出某个ID的entry</li>
</ul>
<p>If there were multiple entry orders with the same ID, all of them are exited at once.<br>如果有多个同样ID的订单，所有的订单都会一次性退出。</p>
<ul>
<li>exit<br>It is a command to exit either a specific entry or whole market position.<br>退出某个entry或者退出整个时长仓位</li>
</ul>
<p>If an order with the same ID is already pending, it is possible to modify the order.<br>如果一个相同ID的订单是pending的话，有可能修改这个订单。</p>
]]></content>
      <categories>
        <category>trade</category>
      </categories>
      <tags>
        <tag>trade</tag>
      </tags>
  </entry>
  <entry>
    <title>My working recap</title>
    <url>/2021/my_working_recap.html</url>
    <content><![CDATA[<h2 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h2><p>It is very important for one person to know himself, knowing what he is suitable, what his characteristics is.<br>Therefore, knowing yourself clearly makes you go further and successful. Next, I will summarize some of my working<br>experience about what kind of people are probably become successful in the world of work.</p>
<h2 id="Physical-quality"><a href="#Physical-quality" class="headerlink" title="Physical quality"></a>Physical quality</h2><p>I think the most essential condition of being successful in the world of work is your physical quality. That means<br>you must be physically strong, energetic. In the cases around me, almost all outstanding staff are big body, they are<br>with big build, good endurance and energetic. They don’t need to take nap in the noon, sometimes they are capable of<br>ignoring a meal for continuous working. Once one of my colleague said, I don’t want to have lunch, for loosing weight.<br>Then she continue to sit in front of her laptop and work till afternoon.<br>Another case is one of my colleague, he works till the midnight for an update service of the product. It seems that he<br>never gets tired. He is able to sleep only 3 hours, and can be sober and working till the midnight of next day.<br>He is such a workaholic that you have to admire. Why can he do that? He is full of fat on his body. His fat body helps<br>to provide with the energy, burning the calories.</p>
<p>So, the physical quality decides!</p>
<h2 id="Family-background"><a href="#Family-background" class="headerlink" title="Family background"></a>Family background</h2><p>Whether you are growing up in a rich family or not is a key point. From my observation, most of the hardworking staff<br>are from poor family. You may wonder how I can know their background. It’s simple. You can tell from his/her dress,<br>his/her interest, his/her education, his/her life experience. For example, if your colleague graduated from US college,<br>he/she is very likely growing from a rich family. If your colleague say, I buy a house totally by myself, without<br>anybody’s help. Or I buy a car or a house to my parents, they are probably from a poorer family.</p>
<p>It is a truth that those who from poorer family tend to work under 996 mechanism. Because they are need to work hard<br>to change their situation, change their life.<br>Of course there are few exceptions, people who are eager to work hard to prove themselves or realize their sense of<br>values could also be a workaholic, even though they come from a wealthy family.</p>
<h2 id="Education-background"><a href="#Education-background" class="headerlink" title="Education background"></a>Education background</h2><p>Somehow, education background is an indispensable factor for you career. In the companies I worked for, the management<br>layer have a good education background. They are usually from top University or have higher educational degree. If you<br>want to get promoted in a company, you’d better bear a beautiful educational degree. </p>
<p>I don’t know whether education background decides people or people decide education background. Outstanding people<br>usually have a beautiful education background, and those who from top University are much more likely to be superb.<br>In my opinion, people with good grades and good study ability usually have stronger memory, reaction speed,<br>concentration and brain computing. Anyway, natural intelligence is very important.</p>
]]></content>
      <categories>
        <category>daily</category>
      </categories>
      <tags>
        <tag>daily</tag>
      </tags>
  </entry>
  <entry>
    <title>初学Pytorch</title>
    <url>/2021/learn_pytorch.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>早在2017年的时候就想学深度学习框架TensorFlow。那时候TensorFlow火得不了，经常看公司的博士老哥用TensorFlow做着各种高大上的<br>项目，操着最高端的显卡，做着最炫酷的事情。对神经网络一窍不通的我，只能在旁边站着围观着。那时候我心里想自己一定得抽个时间学学<br>TensorFlow这玩意，虽然当时做的事情用不上算法，更用不上神经网络、深度学习，但这技术太火了，还是想学下，技多不压身麻。</p>
<p>但是，由于自己对机器学习这玩意不是很感冒（实际是数学功底太差了），学了一段时间机器学习那些东西，比如打打kaggle等等。但自己的数学<br>还是太差了，连那种所谓的入门西瓜书的公式都看不懂。玩不下去，学着也得不到啥正反馈，后面还是放弃了。</p>
<p>2021年了，今年偶尔上了一个机器学习的网课，才又把当年的那些东西捡起来。这课程要求使用深度学习框架做项目，要么选TensorFlow，要么选<br>Pytorch。我不知道选哪个，在知乎上搜了下，得出的结论是：Pytorch最近的发展势头要比TensorFlow强劲。<br>工业界目前TensorFlow占主流，不过，未来可能会被Pytorch超越。在学术界上，Pytorch<br>领先TensorFlow的。另外，Pytorch对Python比较友好。所以，有了今天这篇文章。我选择了Pytorch。</p>
<h2 id="Pytorch基础"><a href="#Pytorch基础" class="headerlink" title="Pytorch基础"></a>Pytorch基础</h2><h3 id="Pytorch-Tensor-vs-Numpy-Array"><a href="#Pytorch-Tensor-vs-Numpy-Array" class="headerlink" title="Pytorch Tensor vs Numpy Array"></a>Pytorch Tensor vs Numpy Array</h3><ul>
<li>在Pytorch中，matrix(array)叫做tensors<br>3<em>3 矩阵，就是一个3</em>3tensor<br>其实这和numpy差不多的：</li>
</ul>
<p>运行下一下代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># import numpy library</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># numpy array</div><div class="line">array = [[1,2,3],[4,5,6]]</div><div class="line">first_array = np.array(array) # 2x3 array</div><div class="line">print(&quot;Array Type: &#123;&#125;&quot;.format(type(first_array))) # type</div><div class="line">print(&quot;Array Shape: &#123;&#125;&quot;.format(np.shape(first_array))) # shape</div><div class="line">print(first_array)</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># import pytorch library</div><div class="line">import torch</div><div class="line"></div><div class="line"># pytorch array</div><div class="line">tensor = torch.Tensor(array)</div><div class="line">print(&quot;Array Type: &#123;&#125;&quot;.format(tensor.type)) # type</div><div class="line">print(&quot;Array Shape: &#123;&#125;&quot;.format(tensor.shape)) # shape</div><div class="line">print(tensor)</div></pre></td></tr></table></figure>
<ul>
<li>有些方法numpy 和 pytorch的效果是一样的：<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">np.ones() = torch.ones()</div><div class="line">np.random.rand() = torch.rand()</div></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># numpy ones</div><div class="line">print(&quot;Numpy &#123;&#125;\n&quot;.format(np.ones((2,3))))</div><div class="line"></div><div class="line"># pytorch ones</div><div class="line">print(torch.ones((2,3)))</div></pre></td></tr></table></figure>
<p>对于矩阵的构造和操作，感觉用numpy的话可能会更好些。所以，通常会将神经网络的tensor结果转换为numpy array来查看和显示。<br>可以用如下方法相互转换：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">torch.from_numpy(): from numpy to tensor</div><div class="line">numpy(): from tensor to numpy</div></pre></td></tr></table></figure></p>
<h3 id="下面是一些数学操作"><a href="#下面是一些数学操作" class="headerlink" title="下面是一些数学操作"></a>下面是一些数学操作</h3><p>tensor的加减乘除法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># create tensor </div><div class="line">tensor = torch.ones(3,3)</div><div class="line">print(&quot;\n&quot;,tensor)</div><div class="line"></div><div class="line"># Resize</div><div class="line">print(&quot;&#123;&#125;&#123;&#125;\n&quot;.format(tensor.view(9).shape,tensor.view(9)))</div><div class="line"></div><div class="line"># Addition</div><div class="line">print(&quot;Addition: &#123;&#125;\n&quot;.format(torch.add(tensor,tensor)))</div><div class="line"></div><div class="line"># Subtraction</div><div class="line">print(&quot;Subtraction: &#123;&#125;\n&quot;.format(tensor.sub(tensor)))</div><div class="line"></div><div class="line"># Element wise multiplication</div><div class="line">print(&quot;Element wise multiplication: &#123;&#125;\n&quot;.format(torch.mul(tensor,tensor)))</div><div class="line"></div><div class="line"># Element wise division</div><div class="line">print(&quot;Element wise division: &#123;&#125;\n&quot;.format(torch.div(tensor,tensor)))</div><div class="line"></div><div class="line"># Mean</div><div class="line">tensor = torch.Tensor([1,2,3,4,5])</div><div class="line">print(&quot;Mean: &#123;&#125;&quot;.format(tensor.mean()))</div><div class="line"></div><div class="line"># Standart deviation (std)</div><div class="line">print(&quot;std: &#123;&#125;&quot;.format(tensor.std()))</div></pre></td></tr></table></figure></p>
<h3 id="Variables"><a href="#Variables" class="headerlink" title="Variables"></a>Variables</h3><ul>
<li>Variables 加速 gradient</li>
<li>在神经网络gradient计算时，我们可以用backpropagation, 所以我们需要处理gradients</li>
<li>variables 和 tensor的区别是 variable累积 gradients</li>
<li>我们可以同样用variable做加减乘除运算</li>
<li>为了做backward propagation我们需要 variables</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># import variable from pytorch library</div><div class="line">from torch.autograd import Variable</div><div class="line"></div><div class="line"># define variable</div><div class="line">var = Variable(torch.ones(3), requires_grad = True)</div><div class="line">var</div><div class="line">tensor([1., 1., 1.], requires_grad=True)</div></pre></td></tr></table></figure>
<p>假设我们有等式 y = x^2<br>定义 x = [2,4] variable<br>计算后我们得到 y = [4,16] (y = x^2)<br>概括 o 的等式： o = (1/2)sum(y) = (1/2)sum(x^2)<br>o 的导数是 x<br>结果等于 x, 所以gradients是[2,4]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># lets make basic backward propagation</div><div class="line"># we have an equation that is y = x^2</div><div class="line">array = [2,4]</div><div class="line">tensor = torch.Tensor(array)</div><div class="line">x = Variable(tensor, requires_grad = True)</div><div class="line">y = x**2</div><div class="line">print(&quot; y =  &quot;,y)</div><div class="line"></div><div class="line"># recap o equation o = 1/2*sum(y)</div><div class="line">o = (1/2)*sum(y)</div><div class="line">print(&quot; o =  &quot;,o)</div><div class="line"></div><div class="line"># backward</div><div class="line">o.backward() # calculates gradients</div><div class="line"></div><div class="line"># As I defined, variables accumulates gradients. In this part there is only one variable x.</div><div class="line"># Therefore variable x should be have gradients</div><div class="line"># Lets look at gradients with x.grad</div><div class="line">print(&quot;gradients: &quot;,x.grad)</div></pre></td></tr></table></figure>
<h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p>我们看一个数字识别的案例。</p>
<ul>
<li>步骤如下：</li>
</ul>
<ol>
<li>导入Library</li>
<li>准备数据集</li>
</ol>
<ul>
<li>用MNIST dataset.</li>
<li>28*28 images， 10 labels 从 0 到 9</li>
<li>数据没有正则化，所以我们要将每个数据除以255. 255是图片的基本正则数值。</li>
<li>为了split数据，我们用sklearn的 train_test_split</li>
<li>80% train data; 20% test data</li>
<li>建立feature和目标tensors. 在下部分，我们通过这些tensors创建variable。另外，我们需要为gradients累积定义variable。</li>
<li>batch_size意味着比如：我们有1000sample数据，我们可以同时训练1000个 sample，或者，可以把它分成10个group，每个group 100个sample，按顺序训练10个group。batch_size就是group size。</li>
<li>epoch: 一个epoch意味着一次训练所有的samples</li>
<li>实验中，我们有33600 个sample去训练，batch_size定为100. 我们决定epoch设为29（accuracy到29epoch时候已经很高了）。数据被训练29次。问题是需要多少次迭代呢？<br>训练一次 = 训练33600个sample<br>但是我们split数据336个group（group_size=batch_size=100)，所以1epoch需要336次迭代。我们有29个epoch，所以一共要迭代9744次。</li>
<li>TensorDataset(): 包装tensors的数据集</li>
<li>DataLoader(): 包含dataset和sample</li>
<li>Visualize one of the images in dataset</li>
</ul>
<ol>
<li><p>创建 Logistic Regression模型</p>
</li>
<li><p>实例模型</p>
</li>
</ol>
<ul>
<li>input_dim = 28 * 28 # size of image pxpx</li>
<li>output_dim = 10 # labels 0,1,2,3,4,5,6,7,8,9</li>
<li>create model</li>
</ul>
<ol>
<li>实例 Loss</li>
</ol>
<ul>
<li>Cross entropy loss</li>
</ul>
<ol>
<li>实例 Optimizer</li>
</ol>
<ul>
<li>SGD Optimizer</li>
</ul>
<ol>
<li>Traning the Model</li>
<li>Prediction</li>
</ol>
<p>代码如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># Prepare Dataset</div><div class="line"># load data</div><div class="line">train = pd.read_csv(r&quot;../input/train.csv&quot;,dtype = np.float32)</div><div class="line"></div><div class="line"># split data into features(pixels) and labels(numbers from 0 to 9)</div><div class="line">targets_numpy = train.label.values</div><div class="line">features_numpy = train.loc[:,train.columns != &quot;label&quot;].values/255 # normalization</div><div class="line"></div><div class="line"># train test split. Size of train data is 80% and size of test data is 20%. </div><div class="line">features_train, features_test, targets_train, targets_test = train_test_split(features_numpy,</div><div class="line">                                                                             targets_numpy,</div><div class="line">                                                                             test_size = 0.2,</div><div class="line">                                                                             random_state = 42) </div><div class="line"></div><div class="line"># create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable</div><div class="line">featuresTrain = torch.from_numpy(features_train)</div><div class="line">targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long</div><div class="line"></div><div class="line"># create feature and targets tensor for test set.</div><div class="line">featuresTest = torch.from_numpy(features_test)</div><div class="line">targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long</div><div class="line"></div><div class="line"># batch_size, epoch and iteration</div><div class="line">batch_size = 100</div><div class="line">n_iters = 10000</div><div class="line">num_epochs = n_iters / (len(features_train) / batch_size)</div><div class="line">num_epochs = int(num_epochs)</div><div class="line"></div><div class="line"># Pytorch train and test sets</div><div class="line">train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)</div><div class="line">test = torch.utils.data.TensorDataset(featuresTest,targetsTest)</div><div class="line"></div><div class="line"># data loader</div><div class="line">train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)</div><div class="line">test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)</div><div class="line"></div><div class="line"># visualize one of the images in data set</div><div class="line">plt.imshow(features_numpy[10].reshape(28,28))</div><div class="line">plt.axis(&quot;off&quot;)</div><div class="line">plt.title(str(targets_numpy[10]))</div><div class="line">plt.savefig(&apos;graph.png&apos;)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># Create Logistic Regression Model</div><div class="line">class LogisticRegressionModel(nn.Module):</div><div class="line">    def __init__(self, input_dim, output_dim):</div><div class="line">        super(LogisticRegressionModel, self).__init__()</div><div class="line">        # Linear part</div><div class="line">        self.linear = nn.Linear(input_dim, output_dim)</div><div class="line">        # There should be logistic function right?</div><div class="line">        # However logistic function in pytorch is in loss function</div><div class="line">        # So actually we do not forget to put it, it is only at next parts</div><div class="line">    </div><div class="line">    def forward(self, x):</div><div class="line">        out = self.linear(x)</div><div class="line">        return out</div><div class="line"></div><div class="line"># Instantiate Model Class</div><div class="line">input_dim = 28*28 # size of image px*px</div><div class="line">output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9</div><div class="line"></div><div class="line"># create logistic regression model</div><div class="line">model = LogisticRegressionModel(input_dim, output_dim)</div><div class="line"></div><div class="line"># Cross Entropy Loss  </div><div class="line">error = nn.CrossEntropyLoss()</div><div class="line"></div><div class="line"># SGD Optimizer </div><div class="line">learning_rate = 0.001</div><div class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</div></pre></td></tr></table></figure>
<ul>
<li>Training part<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># Traning the Model</div><div class="line">count = 0</div><div class="line">loss_list = []</div><div class="line">iteration_list = []</div><div class="line">for epoch in range(num_epochs):</div><div class="line">    for i, (images, labels) in enumerate(train_loader):</div><div class="line">        </div><div class="line">        # Define variables</div><div class="line">        train = Variable(images.view(-1, 28*28))</div><div class="line">        labels = Variable(labels)</div><div class="line">        </div><div class="line">        # Clear gradients</div><div class="line">        optimizer.zero_grad()</div><div class="line">        </div><div class="line">        # Forward propagation</div><div class="line">        outputs = model(train)</div><div class="line">        </div><div class="line">        # Calculate softmax and cross entropy loss</div><div class="line">        loss = error(outputs, labels)</div><div class="line">        </div><div class="line">        # Calculate gradients</div><div class="line">        loss.backward()</div><div class="line">        </div><div class="line">        # Update parameters</div><div class="line">        optimizer.step()</div><div class="line">        </div><div class="line">        count += 1</div><div class="line">        </div><div class="line">        # Prediction</div><div class="line">        if count % 50 == 0:</div><div class="line">            # Calculate Accuracy         </div><div class="line">            correct = 0</div><div class="line">            total = 0</div><div class="line">            # Predict test dataset</div><div class="line">            for images, labels in test_loader: </div><div class="line">                test = Variable(images.view(-1, 28*28))</div><div class="line">                </div><div class="line">                # Forward propagation</div><div class="line">                outputs = model(test)</div><div class="line">                </div><div class="line">                # Get predictions from the maximum value</div><div class="line">                predicted = torch.max(outputs.data, 1)[1]</div><div class="line">                </div><div class="line">                # Total number of labels</div><div class="line">                total += len(labels)</div><div class="line">                </div><div class="line">                # Total correct predictions</div><div class="line">                correct += (predicted == labels).sum()</div><div class="line">            </div><div class="line">            accuracy = 100 * correct / float(total)</div><div class="line">            </div><div class="line">            # store loss and iteration</div><div class="line">            loss_list.append(loss.data)</div><div class="line">            iteration_list.append(count)</div><div class="line">        if count % 100 == 0:</div><div class="line">            # Print Loss</div><div class="line">            print(&apos;Iteration: &#123;&#125;  Loss: &#123;&#125;  Accuracy: &#123;&#125;%&apos;.format(count, loss.data, accuracy))</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Artificial-Neural-Network-ANN"><a href="#Artificial-Neural-Network-ANN" class="headerlink" title="Artificial Neural Network (ANN)"></a>Artificial Neural Network (ANN)</h3><p>LR分类可以，但是当复杂度（非线性）增加，准确性就降低了。<br>所以我们需要增加模型的复杂度。为了增加模型复杂性，我们增加更多非线性方法作为hidden layer。</p>
<p>我们所期待的是，当复杂性增加，我用更多的hidden layer，从而我们的模型准确性会更高。</p>
<p>ANN步骤：</p>
<ol>
<li><p>Import Libraries</p>
</li>
<li><p>Prepare Dataset<br>完全和上一部分一样<br>用同样的dataset，只需要train_loader and test_loader.<br>batch size, epoch 和 迭代次数也一样。</p>
</li>
<li><p>创建 ANN Model</p>
</li>
</ol>
<ul>
<li>增加3层hidden layer</li>
<li>用 ReLU, Tanh and ELU 激活函数。</li>
</ul>
<ol>
<li>Instantiate Model Class</li>
</ol>
<ul>
<li>input_dim = 2828 # size of image pxpx</li>
<li>output_dim = 10 # labels 0,1,2,3,4,5,6,7,8,9</li>
<li>hidden layer维度是150，取这个数是随便取的。你可以尝试其他的数值</li>
<li>create model</li>
</ul>
<ol>
<li>Instantiate Loss</li>
</ol>
<ul>
<li>Cross entropy loss</li>
<li>It also has softmax(logistic function) in it.</li>
</ul>
<ol>
<li>Instantiate Optimizer</li>
</ol>
<ul>
<li>SGD Optimizer</li>
</ul>
<ol>
<li><p>Traning the Model</p>
</li>
<li><p>Prediction</p>
</li>
</ol>
<p>有了hidden layers 模型的准确率达到了95%。比原来高了很多。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># Create ANN Model</div><div class="line">class ANNModel(nn.Module):</div><div class="line">    </div><div class="line">    def __init__(self, input_dim, hidden_dim, output_dim):</div><div class="line">        super(ANNModel, self).__init__()</div><div class="line">        </div><div class="line">        # Linear function 1: 784 --&gt; 150</div><div class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim) </div><div class="line">        # Non-linearity 1</div><div class="line">        self.relu1 = nn.ReLU()</div><div class="line">        </div><div class="line">        # Linear function 2: 150 --&gt; 150</div><div class="line">        self.fc2 = nn.Linear(hidden_dim, hidden_dim)</div><div class="line">        # Non-linearity 2</div><div class="line">        self.tanh2 = nn.Tanh()</div><div class="line">        </div><div class="line">        # Linear function 3: 150 --&gt; 150</div><div class="line">        self.fc3 = nn.Linear(hidden_dim, hidden_dim)</div><div class="line">        # Non-linearity 3</div><div class="line">        self.elu3 = nn.ELU()</div><div class="line">        </div><div class="line">        # Linear function 4 (readout): 150 --&gt; 10</div><div class="line">        self.fc4 = nn.Linear(hidden_dim, output_dim)  </div><div class="line">    </div><div class="line">    def forward(self, x):</div><div class="line">        # Linear function 1</div><div class="line">        out = self.fc1(x)</div><div class="line">        # Non-linearity 1</div><div class="line">        out = self.relu1(out)</div><div class="line">        </div><div class="line">        # Linear function 2</div><div class="line">        out = self.fc2(out)</div><div class="line">        # Non-linearity 2</div><div class="line">        out = self.tanh2(out)</div><div class="line">        </div><div class="line">        # Linear function 2</div><div class="line">        out = self.fc3(out)</div><div class="line">        # Non-linearity 2</div><div class="line">        out = self.elu3(out)</div><div class="line">        </div><div class="line">        # Linear function 4 (readout)</div><div class="line">        out = self.fc4(out)</div><div class="line">        return out</div><div class="line"></div><div class="line"># instantiate ANN</div><div class="line">input_dim = 28*28</div><div class="line">hidden_dim = 150 #hidden layer dim is one of the hyper parameter and it should be chosen and tuned. For now I only say 150 there is no reason.</div><div class="line">output_dim = 10</div><div class="line"></div><div class="line"># Create ANN</div><div class="line">model = ANNModel(input_dim, hidden_dim, output_dim)</div><div class="line"></div><div class="line"># Cross Entropy Loss </div><div class="line">error = nn.CrossEntropyLoss()</div><div class="line"></div><div class="line"># SGD Optimizer</div><div class="line">learning_rate = 0.02</div><div class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># ANN model training</div><div class="line">count = 0</div><div class="line">loss_list = []</div><div class="line">iteration_list = []</div><div class="line">accuracy_list = []</div><div class="line">for epoch in range(num_epochs):</div><div class="line">    for i, (images, labels) in enumerate(train_loader):</div><div class="line"></div><div class="line">        train = Variable(images.view(-1, 28*28))</div><div class="line">        labels = Variable(labels)</div><div class="line">        </div><div class="line">        # Clear gradients</div><div class="line">        optimizer.zero_grad()</div><div class="line">        </div><div class="line">        # Forward propagation</div><div class="line">        outputs = model(train)</div><div class="line">        </div><div class="line">        # Calculate softmax and ross entropy loss</div><div class="line">        loss = error(outputs, labels)</div><div class="line">        </div><div class="line">        # Calculating gradients</div><div class="line">        loss.backward()</div><div class="line">        </div><div class="line">        # Update parameters</div><div class="line">        optimizer.step()</div><div class="line">        </div><div class="line">        count += 1</div><div class="line">        </div><div class="line">        if count % 50 == 0:</div><div class="line">            # Calculate Accuracy         </div><div class="line">            correct = 0</div><div class="line">            total = 0</div><div class="line">            # Predict test dataset</div><div class="line">            for images, labels in test_loader:</div><div class="line"></div><div class="line">                test = Variable(images.view(-1, 28*28))</div><div class="line">                </div><div class="line">                # Forward propagation</div><div class="line">                outputs = model(test)</div><div class="line">                </div><div class="line">                # Get predictions from the maximum value</div><div class="line">                predicted = torch.max(outputs.data, 1)[1]</div><div class="line">                </div><div class="line">                # Total number of labels</div><div class="line">                total += len(labels)</div><div class="line"></div><div class="line">                # Total correct predictions</div><div class="line">                correct += (predicted == labels).sum()</div><div class="line">            </div><div class="line">            accuracy = 100 * correct / float(total)</div><div class="line">            </div><div class="line">            # store loss and iteration</div><div class="line">            loss_list.append(loss.data)</div><div class="line">            iteration_list.append(count)</div><div class="line">            accuracy_list.append(accuracy)</div><div class="line">        if count % 500 == 0:</div><div class="line">            # Print Loss</div><div class="line">            print(&apos;Iteration: &#123;&#125;  Loss: &#123;&#125;  Accuracy: &#123;&#125; %&apos;.format(count, loss.data, accuracy))</div></pre></td></tr></table></figure>
<h3 id="Convolutional-Neural-Network-CNN"><a href="#Convolutional-Neural-Network-CNN" class="headerlink" title="Convolutional Neural Network (CNN)"></a>Convolutional Neural Network (CNN)</h3><p>CNN非常适合图像分类</p>
<p>CNN步骤：</p>
<ol>
<li>Import Libraries</li>
<li>Prepare Dataset</li>
</ol>
<ul>
<li>和前面部分完全相同</li>
<li>We use same dataset so we only need train_loader and test_loader.</li>
</ul>
<ol>
<li>Convolutional layer:</li>
</ol>
<ul>
<li>根据filters（kernels)创建feature</li>
<li>Padding: 在实施filter后，原始图片的维度减少。然而，我们要保留尽可能多的原始图片信息，我们可以在convolutional layer 后，运用padding去增加dimension of feature map。</li>
<li>我们用2层convolutional的</li>
<li>out_channels是16</li>
<li>Filter(kernel) size is 5*5</li>
</ul>
<ol>
<li>Pooling layer:</li>
</ol>
<ul>
<li>为convolutional layer(feature map) output准备一个精简的feature map</li>
<li>使用2个pooling layer</li>
<li>Pooling size是： 2*2</li>
</ul>
<ol>
<li><p>Flattening: Flats the features map </p>
</li>
<li><p>Fully Connected Layer:</p>
</li>
</ol>
<ul>
<li>可以是logistic regression，但要以softmax function结尾。</li>
<li>我们不会在FC层使用激活函数</li>
<li>我们结合convolutional part 和 logistic regression去创建CNN模型</li>
</ul>
<ol>
<li>Instantiate Model Class</li>
</ol>
<ul>
<li>create model</li>
</ul>
<ol>
<li>Instantiate Loss</li>
</ol>
<ul>
<li>Cross entropy loss</li>
<li>It also has softmax(logistic function) in it.</li>
</ul>
<ol>
<li>Instantiate Optimizer</li>
</ol>
<ul>
<li>SGD Optimizer</li>
</ul>
<ol>
<li><p>Traning the Model</p>
</li>
<li><p>Prediction<br>convolutional layer准确率高达98%</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># Create CNN Model</div><div class="line">class CNNModel(nn.Module):</div><div class="line">    def __init__(self):</div><div class="line">        super(CNNModel, self).__init__()</div><div class="line">        </div><div class="line">        # Convolution 1</div><div class="line">        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)</div><div class="line">        self.relu1 = nn.ReLU()</div><div class="line">        </div><div class="line">        # Max pool 1</div><div class="line">        self.maxpool1 = nn.MaxPool2d(kernel_size=2)</div><div class="line">     </div><div class="line">        # Convolution 2</div><div class="line">        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)</div><div class="line">        self.relu2 = nn.ReLU()</div><div class="line">        </div><div class="line">        # Max pool 2</div><div class="line">        self.maxpool2 = nn.MaxPool2d(kernel_size=2)</div><div class="line">        </div><div class="line">        # Fully connected 1</div><div class="line">        self.fc1 = nn.Linear(32 * 4 * 4, 10) </div><div class="line">    </div><div class="line">    def forward(self, x):</div><div class="line">        # Convolution 1</div><div class="line">        out = self.cnn1(x)</div><div class="line">        out = self.relu1(out)</div><div class="line">        </div><div class="line">        # Max pool 1</div><div class="line">        out = self.maxpool1(out)</div><div class="line">        </div><div class="line">        # Convolution 2 </div><div class="line">        out = self.cnn2(out)</div><div class="line">        out = self.relu2(out)</div><div class="line">        </div><div class="line">        # Max pool 2 </div><div class="line">        out = self.maxpool2(out)</div><div class="line">        </div><div class="line">        # flatten</div><div class="line">        out = out.view(out.size(0), -1)</div><div class="line"></div><div class="line">        # Linear function (readout)</div><div class="line">        out = self.fc1(out)</div><div class="line">        </div><div class="line">        return out</div><div class="line"></div><div class="line"># batch_size, epoch and iteration</div><div class="line">batch_size = 100</div><div class="line">n_iters = 2500</div><div class="line">num_epochs = n_iters / (len(features_train) / batch_size)</div><div class="line">num_epochs = int(num_epochs)</div><div class="line"></div><div class="line"># Pytorch train and test sets</div><div class="line">train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)</div><div class="line">test = torch.utils.data.TensorDataset(featuresTest,targetsTest)</div><div class="line"></div><div class="line"># data loader</div><div class="line">train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)</div><div class="line">test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)</div><div class="line">    </div><div class="line"># Create CNN</div><div class="line">model = CNNModel()</div><div class="line"></div><div class="line"># Cross Entropy Loss </div><div class="line">error = nn.CrossEntropyLoss()</div><div class="line"></div><div class="line"># SGD Optimizer</div><div class="line">learning_rate = 0.1</div><div class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</div></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers" target="_blank" rel="external">https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers</a></p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>股票交易名词术语的一些笔记</title>
    <url>/2021/trading_term.html</url>
    <content><![CDATA[<h3 id="大宗交易-block-trading"><a href="#大宗交易-block-trading" class="headerlink" title="大宗交易 (block trading)"></a>大宗交易 (block trading)</h3><p>大宗交易(block trading),又称为大宗买卖，是指达到规定的最低限额的证券单笔买卖申报，买卖双方经过协议达成一致并经交易所确定成交的证券交易。</p>
<p>其中上交所规定（2015年最新修订版）：<br>1、A股单笔买卖申报数量在30万股（含）以上，或交易金额在200万元（含）人民币以上；<br>B股( 上海)单笔买卖申报数量在30万股（含）以上，或交易金额在20万美元（含）以上；</p>
<h3 id="洗盘"><a href="#洗盘" class="headerlink" title="洗盘"></a>洗盘</h3><p>股市里，所谓洗盘，是主力操纵股市，压低股价的一种手段，是庄家为拉高股价获利出货，先有意制造假象，迫使低价买进者卖出股票，以减轻拉升压力的一种手法。</p>
<ul>
<li>庄家洗盘的信号<br>一般来说，底部吸货过程中的洗盘和K线洗盘一般为二三天的时间，K线组合为一周左右，形式洗盘短的为一个月左右，长的则三个月甚至可到半年，拉升过程中的洗盘一般为一周左右，<br>快速洗盘一般只要二三天，以形式洗盘则在三周左右。此时便可以从成交量变化上看清庄家震仓洗盘的痕迹。</li>
</ul>
<p>1、双连阴震仓法——两连阴组合式的成交量</p>
<p>5日成交量均线与10日成交量均线交叉当日，K线为阴阳线与之相对应，当日成交量为阴量。且5日成交量线与10日成交量均线交叉前一日的K线为阴成交量也为阴，但成交量比交叉日成交量大。<br>当双连阴式的成交量组合出现时，庄家常故意做出一些极为不利的技术指标，给人以破位下行的感觉，此种形态通常是庄家洗盘震仓的拿手好戏。而一根阴量显示庄家开始洗盘，此时一些持股者还能保持平静，坚持持仓。但第二根阴量出现时，在庄家的刻意打压下，心态不稳的持股者终于被震出局。</p>
<p>这样，平均成交持仓量趋于一致，当大部分浮筹被清洗之后，庄家这才确信拉升的阻力减轻，于是，大幅拉高就开始了。</p>
<p>2、阴阳交错震仓法——四根阴阳结合的成交量</p>
<p>一般说来，二根阳量表示庄家试盘，庄家小幅拉升股价以此来试探市场的反应。二根阴量表示震仓。一旦庄家发现跟风者甚多，就会连拉阴线以吓走跟庄者。<br>通过四根阴阳量，庄家就可以知道跟庄者筹码的多少及其持仓成本。这种组合是由连续二根阳量、连续二根阴量组合而成。二根阳量均呈递增趋势，二根阴量均呈递减趋势，线与日成交量均线只要在上述日内任何一天交叉，且此时线的均线呈多头排列，那么，逢低介入应有厚报。</p>
<p>3、量减震仓法——递减型双阳成交量组合</p>
<p>当5日成交量均线与10日成交量均线交叉当日，K线为阳线。与此相对应的，当日成交量为阳量，且5日成交量均线与10日成交量均线交叉前一日，K线为阳线，且当日成交量为阳量。<br>双阳成交量的这种递减型组合为强势整理式庄家震仓洗盘行为。其显著的特点是持续只有一天，但力度不小。经过一段时间的拉抬，庄家要震出短线获利盘，以便进行筹码的充分换手，提高跟庄者的平均持仓成本。如此一来，庄家拉高出货就比较轻松。</p>
<h3 id="震荡指标"><a href="#震荡指标" class="headerlink" title="震荡指标"></a>震荡指标</h3><p>就是只在一个区间范围内变化的技术指标。比如RSI，它只会在0-100之间变化。震荡指标三兄弟：RSI、KD、MACD</p>
<h3 id="股本"><a href="#股本" class="headerlink" title="股本"></a>股本</h3><p>股本就是总共发行的股票数量。 其中又分为限售股和流通股，限售股就是没有在市场上流通，不允许买卖的股票，比如说他们发给公司高层的奖励性质的股票。 流通股就是在市场上也就是股市中可进行买卖的股票。</p>
<h3 id="金字塔加码-pyramidding"><a href="#金字塔加码-pyramidding" class="headerlink" title="金字塔加码 pyramidding"></a>金字塔加码 pyramidding</h3><p>获利加仓事必须的；市场大多数时候是概率均匀分布，而你真正能賺到钱是靠那些大多数人想象不到的波动。获利加仓的意义在于不要辜负好运气。<br>加仓，通常可以在获得止损同幅度的获利时加等数底仓，同时上移止损到底仓价格。当然你可以分几次加。还有一个关键在于：一定要设定一个合理的持仓时间，在这个时间内，仓位必须要产生预定的利润；</p>
<h3 id="背离-Divergence"><a href="#背离-Divergence" class="headerlink" title="背离 Divergence"></a>背离 Divergence</h3><h3 id="损益比"><a href="#损益比" class="headerlink" title="损益比"></a>损益比</h3><p>损益是一种对交易者在一定时期内持续创造利润大于亏损的能力的监测方法。损益的计算方法是用获利交易的平均利润除以亏损交易的平均损失。结果是两个数字组成了损益比。通常一个成功策略的基准是它应该产生至少2:1的损益比。</p>
<h3 id="股票增发"><a href="#股票增发" class="headerlink" title="股票增发"></a>股票增发</h3><p>今天（20210607）伊利大跌，是因为定增所导致的。因此学习了下股票增发的影响。<br>公开增发，就是面向全体股民发售。<br>这种增发很缺德，因为增发本身就已经稀释了所有股东的利益。</p>
<p>定向增发，就是不面向散户发售。<br>定向增发通常是公司要募集资金去收购新的资产。通常股民们会把定向增发，当成是利好。</p>
<h3 id="双均线策略"><a href="#双均线策略" class="headerlink" title="双均线策略"></a>双均线策略</h3><p>双均线策略，指的是运用两条不同周期的移动平均线，即短周期移动平均线和长周期移动平均线的相对大小，研判买进与卖出时机的策略。<br>由短周期均线自下向上穿越长周期均线，所形成的交点，称为金叉。当短周期均线自上而下穿越长周期均线，所形成的交点，称为死叉。<br>这样我们可以构建一个双均线策略：双均线金叉的时候，表明该币很强势，市场属于多头市场；反之，当出现死叉点时，市场属于空头市场。</p>
<h3 id="抵扣价"><a href="#抵扣价" class="headerlink" title="抵扣价"></a>抵扣价</h3><p>蕭明道常運用「扣抵」概念，抓到個股主升段行情。「所謂扣抵，是使用均線操作的一個重要觀念。說穿了，就是未卜先知，提早掌握未來趨勢。」他解釋，移動平均線是較長期的價格發展線，相對K線，更具有趨勢意義。且平均日期愈長，所代表的行情走勢就愈大。</p>
<p>抵扣价，簡單講就是「喜新厭舊」：「新」指的是今日的收盤價；「舊」則是指均線計算時第1日的收盤價、也就是均線今日收盤價的扣抵值。</p>
<p>比方說，計算5日移動平均線時，必須先算出第1日到第5日的價格平均，當第6日的新價格出現後，就剔除第1日的價格，從第2日開始重新計算。由於扣抵過程中，可知道「舊」的收盤價位置高低，及「新」的收盤價相對位置，因此事前就會知道移動平均線將往上揚，或向下走。</p>
<h3 id="除息-amp-除权"><a href="#除息-amp-除权" class="headerlink" title="除息&amp;除权"></a>除息&amp;除权</h3><p>上市公司给我们的分红有两种形式：向股东派发现金股利和股票股利，上市公司可根据情况选择其中一种形式进行分红，也可以两种形式同时用。</p>
<p>现金股利是指以现金形式向股东发放股利，称为派股息或派息。</p>
<p>现金分红是公司通过可分配利润，以货币资金的形式支付给股东权利，不会改变股本大小，但股东权益减少，每股代表的实际价格也会减少，因此价格相应降低，这一过程叫除息。</p>
<p>除息价=股息登记日的收盘价－每股所分红利现金额<br>例如：某股票股息登记日的收盘价是5元，每股送红利现金0.5元，则其次日股价为5-0.5=4.5(元)</p>
<p>除权是由于公司股本增加，每股股票所代表的企业实际价值(每股净资产)有所减少，需要在发生该事实之后从股票市场价格中剔除这部分因素，而形成的剔除行为。</p>
<p>要想了解除权，我们先来了解下投资者的所有者权益构成。</p>
<p>上市公司发行股票拿到募集资金后，会分为两部分：股本和资本公积。</p>
<p>股票发行价格中，实际价值1元是股票面值，记作股本；超出1元的部分计作资本公积。</p>
<p>上市公司有了钱，开始投资赚钱，将赚取的收益再分成两部分：盈余公积和未分配利润。</p>
<p>因此，作为投资者我们享有以上四部分的所有权：</p>
<p>所有者权益=股本+资本公积+盈余公积+未分配利润</p>
<p>上市公司给投资者分红的第二种形式是：股票股利，是指上市公司向股东分发股票，红利以股票的形式出现，又称为送红股或送股；另外，投资者还经常会遇到上市公司转增股本的情况，转增股本与分红有所区别。</p>
<p>送股：是采取将盈余公积或未分配利润转化为股本的分红方式，上市公司一般只将未分配利润部分送股。<br>转股：是采取将资本公积转化为股本的分红方式。</p>
<p>这两种形式都会使股本数增加，但同时也会让股价下跌！这一过程就是除权。</p>
<p>送股/转股仅仅是稀释股本，降低股价，可以看作是单纯降低股价便于买卖，增加流通性，没有其它实质意义。是上市公司向股东传递公司管理当局预期盈利将继续增长的信息，可以增加股票市场流动性。</p>
<p>除权价=股权登记日的收盘价÷(1+每股送红股数)<br>例如：某股票股权登记日的收盘价是25.8元，每10股送2股，即每股送红股数为0.2，则次日股价为18.8÷(1+0.2)=21.5(元)</p>
]]></content>
      <categories>
        <category>trade</category>
      </categories>
      <tags>
        <tag>trade</tag>
      </tags>
  </entry>
  <entry>
    <title>RSI 计算</title>
    <url>/2021/RSI.html</url>
    <content><![CDATA[<h2 id="RSI计算公式"><a href="#RSI计算公式" class="headerlink" title="RSI计算公式"></a>RSI计算公式</h2><p>RSI = 100 – 100 / ( 1 + RS )<br>RS = Relative Strength = AvgU / AvgD<br>AvgU = 在最后N个bar里所有向上移动（涨了多少）的平均值<br>AvgD = 在最后N个bar里所有向下移动（跌了多少）的平均值<br>N = RSI周期</p>
<ul>
<li>有3中种常用的计算AvgU 、 AvgD的方法</li>
</ul>
<h2 id="RSI手把手计算"><a href="#RSI手把手计算" class="headerlink" title="RSI手把手计算"></a>RSI手把手计算</h2><p>计算 up moves（涨了多少 和 down moves（跌了多少） (get U and D)</p>
<p>计算 Relative Strength (get RS)<br>计算 Relative Strength Index (get RSI)</p>
<h3 id="Step-1-计算-Up-Moves-和-Down-Moves"><a href="#Step-1-计算-Up-Moves-和-Down-Moves" class="headerlink" title="Step 1: 计算 Up Moves 和 Down Moves"></a>Step 1: 计算 Up Moves 和 Down Moves</h3><p>我们会以14天为一个period来做参考，14天是最常用的参数。<br>要计算RSI，你需要最近15天的收盘价(closing prices)。（如果是10天为一个周期的话，你需要最近11天的收盘价）。</p>
<p>咱开始计算最近14天的up moves 和 down moves。<br>首先，计算每个bar之间的差值：Chng = Closet – Closet-1<br>也就是当天的收盘价 - 前一天的收盘价</p>
<p>每个bar中，up move (U) 等于:<br>Closet – Closet-1，如果价格变动是正数的话<br>0，如果价格变动是负数或0的话。</p>
<p>Down move (D) 等于:<br>Closet – Closet-1 的绝对值，如果价格变动是负数的话<br>0，如果价格变动是正数或0的话。</p>
<p>现在，你已经有了RSI公式的输入，也就是最近N天的增长或下降（N是RSI周期）。</p>
<p>下一步就是求平均</p>
<h3 id="Step2-平均-上涨和下跌"><a href="#Step2-平均-上涨和下跌" class="headerlink" title="Step2: 平均 上涨和下跌"></a>Step2: 平均 上涨和下跌</h3><p>有3个常用的方法。每个计算up 、 down moves的方式都不同：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Simple Moving Average</div><div class="line">Exponential Moving Average</div><div class="line">Wilder’s Smoothing Method</div></pre></td></tr></table></figure></p>
<h4 id="Simple-Moving-Average"><a href="#Simple-Moving-Average" class="headerlink" title="Simple Moving Average"></a>Simple Moving Average</h4><p>这是最直接的方法：<br>AvgU = 把最近N个bars里的所有的up move(U)相加，然后除以N</p>
<p>AvgD = 把最近N个bars里的所有的down move(D)相加，然后除以N</p>
<h4 id="Exponential-Moving-Average"><a href="#Exponential-Moving-Average" class="headerlink" title="Exponential Moving Average"></a>Exponential Moving Average</h4><p>这方法跟上面的方法是一样的，只是用EMA来算。EMA周期就是RSI周期。公式如下：<br>AvgUt = α <em> Ut + ( 1 – α ) </em> AvgUt-1</p>
<p>AvgDt = α <em> Dt + ( 1 – α ) </em> AvgDt-1</p>
<p>α = 2 / ( N + 1 )</p>
<h4 id="Wilder’s-Smoothing-Method"><a href="#Wilder’s-Smoothing-Method" class="headerlink" title="Wilder’s Smoothing Method"></a>Wilder’s Smoothing Method</h4><p>J. Welles Wilder是RSI的发明者。这方法跟Exponential Moving Average的逻辑是一样的，只是 α 不同：</p>
<p>α = 1 / N</p>
<p>所以，1 – α = ( N – 1 ) / N</p>
<p>比如，计算RSI 14的 average up move：<br>AvgUt = 1/14 <em> Ut + 13/14 </em> AvgUt-1</p>
<h3 id="Step-3-计算Relative-Strength"><a href="#Step-3-计算Relative-Strength" class="headerlink" title="Step 3: 计算Relative Strength"></a>Step 3: 计算Relative Strength</h3><p>现在最近14天的AvgU，AvgD都有了，下一步就是要计算Relative Strength，也就是计算AvgU 和 AvgD的比</p>
<p>RS = AvgU / AvgD</p>
<h3 id="Step-4-计算-Relative-Strength-Index-RSI"><a href="#Step-4-计算-Relative-Strength-Index-RSI" class="headerlink" title="Step 4: 计算 Relative Strength Index (RSI)"></a>Step 4: 计算 Relative Strength Index (RSI)</h3><p>最后，我们知道Relative Strength，就可以计算RSI了。<br>Step 4: Calculating the Relative Strength Index (RSI)</p>
<p>RSI = 100 – 100 / ( 1 + RS)</p>
<ul>
<li>最低的 RSI<br>什么情况下RSI的值最低呢？完全的熊市~。<br>想象下，每天的收盘价都会低于前一天的收盘价。这将没有up的天数，最近N天的所有的U会是0.<br>AvgU会是0。<br>另一方面，AvgD会是某个正数<br>Relative Strength会是0，因为是 0除以某个正数，所以是0</li>
</ul>
<p>RSI也是0：<br>RSI = 100 – 100 / ( 1 + 0 ) = 100 – 100 = 0</p>
<ul>
<li>最高RSI<br>和上面的特殊情况相反咯，就是每天都涨涨涨，超级牛市<br>AvgD 会是0, AvgU 是某个正数. RS会是某个正数除以0。但是数学上，这是非法的。所以这种特殊情况会定义RSI为100。<br>如果 AvgD不是0，是一个非常小的数。RS会接近正无穷，RSI会非常靠近0。</li>
</ul>
<p>RSI = 100 – 100 / ( 1 + a big number ) = 100 – 0 = 100</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>3种方法会给我们3种不同的结果。交易员看你自己的喜好而定，但是一般选用了某种方法，就统一用那种方法好了。</p>
]]></content>
      <categories>
        <category>finance</category>
      </categories>
      <tags>
        <tag>finance</tag>
        <tag>RSI</tag>
      </tags>
  </entry>
  <entry>
    <title>PySpark, 利用蒙特卡洛测算圆柱体体积</title>
    <url>/2021/pyspark_monte_carlo.html</url>
    <content><![CDATA[<h2 id="蒙特卡洛算法"><a href="#蒙特卡洛算法" class="headerlink" title="蒙特卡洛算法"></a>蒙特卡洛算法</h2><p>刚开始我以为蒙特卡洛是一个人的名字，后来才发现是一个城市的名字，而且是一个赌城的名字。有趣的是还有个<br>算法叫做拉斯维加斯算法。另外，不知道有没有澳门算法。</p>
<p>先简单说说蒙特算法的原理吧。就比如说你要在一堆苹果中找出一个最大的苹果。一个一个地拿过来，第一个拿出来<br>然后它就是当前最大的，然后拿第二个，和刚拿的第一个比较，看看谁大，如果比第一个大的话，就讲第二个苹果作为<br>最大的苹果，否则第一个依然视为最大的。</p>
<p>就这样一个一个苹果不断地拿出来比较、迭代。拿出的苹果越多就越接近所有苹果中最大的那个苹果。</p>
<h2 id="蒙特卡洛计算Pi"><a href="#蒙特卡洛计算Pi" class="headerlink" title="蒙特卡洛计算Pi"></a>蒙特卡洛计算Pi</h2><p>先拿Spark官方的一个例子，用pyspark来计算Pi的值<br><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> random <span class="keyword">import</span> random</div><div class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add</div><div class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    <span class="string">"""</span></div><div class="line">        Usage: pi [partitions]</div><div class="line">    """</div><div class="line">    spark = SparkSession\</div><div class="line">        .builder\</div><div class="line">        .appName(<span class="string">"PythonPi"</span>)\</div><div class="line">        .getOrCreate()</div><div class="line"></div><div class="line">    partitions = int(sys.argv[<span class="number">1</span>]) <span class="keyword">if</span> len(sys.argv) &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">2</span></div><div class="line">    n = <span class="number">100000</span> * partitions</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(_)</span>:</span></div><div class="line">        x = random() * <span class="number">2</span> - <span class="number">1</span></div><div class="line">        y = random() * <span class="number">2</span> - <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">if</span> x ** <span class="number">2</span> + y ** <span class="number">2</span> &lt;= <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span></div><div class="line"></div><div class="line">    count = spark.sparkContext.parallelize(range(<span class="number">1</span>, n + <span class="number">1</span>), partitions).map(f).reduce(add)</div><div class="line">    print(<span class="string">"Pi is roughly %f"</span> % (<span class="number">4.0</span> * count / n))</div><div class="line"></div><div class="line">    spark.stop()</div></pre></td></tr></table></figure></p>
<p>解析下这段代码。关键算法是在<code>f(_)</code>。就是在一定的数值范围内，随机生成x,y 两个数。如果x,y 符合<code>x ** 2 + y ** 2 &lt;= 1</code><br>这公式的话就返回1，否则返回0</p>
<p>通过pyspark迭代100000次，执行100000次<code>f(_)</code>这个方法，这样就会生成一个以<code>1,0</code>组合成，length为100000<br>的数组。然后sum这个大数组，把数组里面的1全部累加起来。然后，看累加起来的数与100000相除，也就是算出1在<br>数组中出现的比例，再用这个比例乘以4就可以近似算出Pi的值。执行次数越多，越接近Pi的准确值。</p>
<h2 id="蒙特卡洛算体积"><a href="#蒙特卡洛算体积" class="headerlink" title="蒙特卡洛算体积"></a>蒙特卡洛算体积</h2><p>算体积的原理也是类似。比如我们算球的体积，可以假设球被一个立方体包住，然后随机生成很多个点（x,y,z) 遍布在<br>立方体的方位内，如果满足球的体积公式则返回1，否则返回0。然后看看遍历后占所有随机点的比例有多大。再将这个比例乘以<br>立方体的体积，就能得出球的体积</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><div class="line">radius = <span class="number">4</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">f1</span><span class="params">(_)</span>:</span></div><div class="line">    x = uniform(<span class="number">0</span> - radius, <span class="number">0</span> + radius)</div><div class="line">    y = uniform(<span class="number">1</span> - radius, <span class="number">1</span> + radius)</div><div class="line">    z = uniform(<span class="number">-2</span> - radius, <span class="number">-2</span> + radius)</div><div class="line">    <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">if</span> x ** <span class="number">2</span> + (y - <span class="number">1</span>) ** <span class="number">2</span> + (z + <span class="number">2</span>) ** <span class="number">2</span> &lt;= <span class="number">16</span> <span class="keyword">else</span> <span class="number">0</span></div><div class="line"></div><div class="line"></div><div class="line">N = <span class="number">10000000</span></div><div class="line">partitions = <span class="number">1</span></div><div class="line">cube_volume = (radius * <span class="number">2</span>) ** <span class="number">3</span></div><div class="line">count = spark.sparkContext.parallelize(range(<span class="number">1</span>, N + <span class="number">1</span>), partitions).map(f1).reduce(add)</div><div class="line"></div><div class="line">print(count)</div><div class="line"></div><div class="line">print(<span class="string">"Volume is roughly %f"</span> % (count / N * cube_volume))</div></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>只要掌握蒙特卡洛原理，实现起来并不难。有问题欢迎留言，或联系我。</p>
]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>pyspark</tag>
      </tags>
  </entry>
  <entry>
    <title>Kivy 有史以来我接触最不友好的一个Python框架</title>
    <url>/2021/kivy.html</url>
    <content><![CDATA[<h2 id="开头"><a href="#开头" class="headerlink" title="开头"></a>开头</h2><p>最近有个朋友找我帮他看看一个构建Kivy的问题。结果弄了整整一天，连编译环境都有问题，最终也没能编译成功。虽然最后里成功已经很接近了，但<br>考虑朋友说换成用HTML做了，就没有继续折腾Kivy了。</p>
<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>刚开始，我说想快点帮他解决，就打算直接在Kivy官网找官方的Docker镜像下载了。后来，下了好几个镜像，官方的，非官方的<br>结果都有各种各样的问题，跑不通。</p>
<p>后面就放弃Docker了，直接在本地环境搭建环境。在运行<code>buildozer -v android debug</code> 的时候，报错：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&apos;sdkmanager not installed&apos;</div></pre></td></tr></table></figure></p>
<p>后面才发现需装一个Android的SDK （Android Studio ），这我在官方文档上都看到啊。。真是无语哎</p>
<ul>
<li>相关报错信息：<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Original exception:</div><div class="line">===================</div><div class="line"></div><div class="line">    Traceback (most recent call last):</div><div class="line">      File &quot;/Users/xifeng.li/.local/lib/python3.7/site-packages/sh.py&quot;, line 2076, in __init__</div><div class="line">        os.execve(cmd[0], cmd, ca[&quot;env&quot;])</div><div class="line">    OSError: [Errno 8] Exec format error: b&apos;/Users/xifeng.li/opt/test_kivy/app/.buildozer/android/platform/build-armeabi-v7a/build/other_builds/hostpython3/desktop/hostpython3/native-build/python3&apos;</div><div class="line"></div><div class="line"></div><div class="line"># Command failed: /Users/xifeng.li/opt/anaconda3/bin/python -m pythonforandroid.toolchain create --dist_name=myapp --bootstrap=sdl2 --requirements=python3,kivy --arch armeabi-v7a --copy-libs --color=always --storage-dir=&quot;/Users/xifeng.li/opt/test_kivy/app/.buildozer/android/platform/build-armeabi-v7a&quot; --ndk-api=21</div><div class="line"># ENVIRONMENT:</div><div class="line">#     TERM_PROGRAM = &apos;iTerm.app&apos;</div><div class="line">#     PYENV_ROOT = &apos;/Users/xifeng.li/.pyenv&apos;</div><div class="line">#     TERM = &apos;xterm-256color&apos;</div><div class="line">#     SHELL = &apos;/bin/bash&apos;</div><div class="line">#     CLICOLOR = &apos;Yes&apos;</div><div class="line">#     TMPDIR = &apos;/var/folders/5_/r_bbpkqj2t126yjr97zgbvshnb19df/T/&apos;</div><div class="line">#     CONDA_SHLVL = &apos;1&apos;</div><div class="line">#     CONDA_PROMPT_MODIFIER = &apos;(base) &apos;</div><div class="line">#     TERM_PROGRAM_VERSION = &apos;3.3.9&apos;</div><div class="line">#     OLDPWD = &apos;/Users/xifeng.li/Library/Android/sdk/tools/bin&apos;</div><div class="line">#     TERM_SESSION_ID = &apos;w0t3p0:A097B1EA-1572-46C1-847B-DC60F03F9BCF&apos;</div><div class="line">#     USER = &apos;xifeng.li&apos;</div><div class="line">#     COMMAND_MODE = &apos;unix2003&apos;</div><div class="line">#     CONDA_EXE = &apos;/Users/xifeng.li/opt/anaconda3/bin/conda&apos;</div><div class="line">#     SSH_AUTH_SOCK = &apos;/private/tmp/com.apple.launchd.xatnkbYTCt/Listeners&apos;</div><div class="line">#     __CF_USER_TEXT_ENCODING = &apos;0x2AB0A5AE:0x0:0x0&apos;</div><div class="line">#     _CE_CONDA = &apos;&apos;</div><div class="line">#     PATH = &apos;/Users/xifeng.li/.buildozer/android/platform/apache-ant-1.9.4/bin:/Users/xifeng.li/Library/Android/sdk/sdkmanager:/Users/xifeng.li/Library/Android/sdk/tools:/Users/xifeng.li/.pyenv/bin:/Users/xifeng.li/opt/anaconda3/bin:/Users/xifeng.li/opt/anaconda3/condabin:/Users/xifeng.li/.pyenv/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/xifeng.li/Library/Python/2.7/bin:/Users/xifeng.li/.local/bin&apos;</div><div class="line">#     LaunchInstanceID = &apos;D7D5D23B-AE28-49F3-80C1-63F519A6608F&apos;</div><div class="line">#     CONDA_PREFIX = &apos;/Users/xifeng.li/opt/anaconda3&apos;</div><div class="line">#     PWD = &apos;/Users/xifeng.li/opt/test_kivy/app&apos;</div><div class="line">#     JAVA_HOME = &apos;/Library/Java/JavaVirtualMachines/jdk1.8.0_231.jdk/Contents/Home&apos;</div><div class="line">#     ANDROID_SDK = &apos;/Users/xifeng.li/Library/Android/sdk&apos;</div><div class="line">#     ITERM_PROFILE = &apos;Default&apos;</div><div class="line">#     XPC_FLAGS = &apos;0x0&apos;</div><div class="line">#     _CE_M = &apos;&apos;</div><div class="line">#     XPC_SERVICE_NAME = &apos;0&apos;</div><div class="line">#     SHLVL = &apos;1&apos;</div><div class="line">#     HOME = &apos;/Users/xifeng.li&apos;</div><div class="line">#     COLORFGBG = &apos;15;0&apos;</div><div class="line">#     LC_TERMINAL_VERSION = &apos;3.3.9&apos;</div><div class="line">#     LS_OPTIONS = &apos;--color=auto&apos;</div><div class="line">#     ITERM_SESSION_ID = &apos;w0t3p0:A097B1EA-1572-46C1-847B-DC60F03F9BCF&apos;</div><div class="line">#     CONDA_PYTHON_EXE = &apos;/Users/xifeng.li/opt/anaconda3/bin/python&apos;</div><div class="line">#     LOGNAME = &apos;xifeng.li&apos;</div><div class="line">#     LC_CTYPE = &apos;UTF-8&apos;</div><div class="line">#     CONDA_DEFAULT_ENV = &apos;base&apos;</div><div class="line">#     LC_TERMINAL = &apos;iTerm2&apos;</div><div class="line">#     SECURITYSESSIONID = &apos;186a6&apos;</div><div class="line">#     COLORTERM = &apos;truecolor&apos;</div><div class="line">#     _ = &apos;/Users/xifeng.li/.local/bin/buildozer&apos;</div><div class="line">#     PACKAGES_PATH = &apos;/Users/xifeng.li/.buildozer/android/packages&apos;</div><div class="line">#     ANDROIDSDK = &apos;/Users/xifeng.li/.buildozer/android/platform/android-sdk&apos;</div><div class="line">#     ANDROIDNDK = &apos;/Users/xifeng.li/.buildozer/android/platform/android-ndk-r19c&apos;</div><div class="line">#     ANDROIDAPI = &apos;27&apos;</div><div class="line">#     ANDROIDMINAPI = &apos;21&apos;</div><div class="line">#</div><div class="line"># Buildozer failed to execute the last command</div><div class="line"># The error might be hidden in the log above this error</div><div class="line"># Please read the full log, and search for it before</div><div class="line"># raising an issue with buildozer itself.</div><div class="line"># In case of a bug report, please add a full log with log_level = 2</div><div class="line">(base) XifengLideMacBook-Pro:app xifeng.li$ /Users/xifeng.li/opt/anaconda3/bin/python -m pythonforandroid.toolchain create --dist_name=myapp --bootstrap=sdl2 --requirements=python3,kivy --arch armeabi-v7a --copy-libs --color=always --storage-dir=&quot;/Users/xifeng.li/opt/test_kivy/app/.buildozer/android/platform/build-armeabi-v7a&quot; --ndk-api=21</div><div class="line">/Users/xifeng.li/opt/anaconda3/lib/python3.7/site-packages/pythonforandroid/toolchain.py:84: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&apos;s documentation for alternative uses</div><div class="line">  import imp</div><div class="line">[INFO]:    Will compile for the following archs: armeabi-v7a</div><div class="line">[ERROR]:   Build failed: Android SDK dir was not specified, exiting.</div></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://github.com/diegodukao/docker-python3-kivy-buildozer" target="_blank" rel="external">https://github.com/diegodukao/docker-python3-kivy-buildozer</a><br><a href="https://stackoverflow.com/questions/65724724/while-building-kivy-app-sdkmanager-not-installed" target="_blank" rel="external">https://stackoverflow.com/questions/65724724/while-building-kivy-app-sdkmanager-not-installed</a><br><a href="https://developer.android.com/studio#downloads" target="_blank" rel="external">https://developer.android.com/studio#downloads</a><br><a href="https://hub.docker.com/r/kivy/buildozer" target="_blank" rel="external">https://hub.docker.com/r/kivy/buildozer</a></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>kivy</tag>
      </tags>
  </entry>
  <entry>
    <title>Web SSH服务搭建</title>
    <url>/2020/web_ssh.html</url>
    <content><![CDATA[<h2 id="前言背景"><a href="#前言背景" class="headerlink" title="前言背景"></a>前言背景</h2><p>最近买了个域名，然后绑定了一个Vultr的IP，结果第二天这个IP就被trash GFW给封了，呵呵。以前也是<br>这样，但不知道为啥被封，现在知道了。如果国外IP绑定域名的话，就会被封。我域名还是实名制的呢。这让我不能SSH到<br>虚拟机，不翻墙也不能访问我那注册的域名。要想SSH到虚拟机服务器只能通过代理服务器或跳板机，或者还有个好方法，<br>那就是用Web SSH。</p>
<p>用Web SSH可以通过Vultr自带提供的Console工具登陆到虚拟机。但是自带的Web SSH界面很难看，而且比较难用。<br>还不如自己搭建一个Web SSH服务。</p>
<h2 id="正题"><a href="#正题" class="headerlink" title="正题"></a>正题</h2><p>我在Github搜了下，还挺多Web SSH相关的项目。最后我选这个 <a href="https://github.com/huashengdun/webssh。" target="_blank" rel="external">https://github.com/huashengdun/webssh。</a><br>这个我觉得还是挺简单易用的。</p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>这个安装非常非常简单，前提是你用python环境，然后执行：<br><code>pip install webssh</code></p>
<h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><p><code>wssh --address=&#39;0.0.0.0&#39; --port=8000  --fbidhttp=False --wpintvl=30</code></p>
<p>然后通过浏览器直接访问 <code>IP:port</code></p>
<p><img src="/images/2020/web_ssh/af96aa57.png" alt=""></p>
<p>然后就可以通过网页来操作虚拟机服务器了，从而跳过GFW对我IP的封锁。</p>
<p>就这么简单。</p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title>关于羽毛球私教</title>
    <url>/2020/badminton_coach.html</url>
    <content><![CDATA[<p>要想打好羽毛球，首先要用正确的动作去打，然后就是不断的练习。经常打野球打来打去不总结的话很难知道自己存在什么问题，<br>改怎么改进。打了这么多年还是很菜，于是总是想找个教练指导指导，快速进步。于是我自己在网上找了不少教练，也上了不少课。<br>我来谈谈找羽毛球私教的个人看法。</p>
<h2 id="私教一"><a href="#私教一" class="headerlink" title="私教一"></a>私教一</h2><p>前几天在中羽找了个私教。头像是和谌龙合影的，自称是前厦门队的，然后拿过福建省运会男单第四，当时看了下，好像看起来还不错。就加了微信。</p>
<p>微信一聊，他给出的价格是一对一400/小时不包场费（这价格足够可以找省队退役的职业球员了），加上场费大概就是500+了。我觉得有点贵，但想想万一教的好的话也值得。就跟他说能不能找个人一起拼课，他给出的拼课1小时价格是220/人。</p>
<p>后来他回复说可以，还说另外一个拼课的人水平算是业余高手了。</p>
<p>周六下午我去到现场，先是跟他的下面的一个助理教练（小喽啰）打了打，也就打得比我好一点吧，打了21球。</p>
<p>打完后，他调出一些毛病了。我都挺认同，然后问我想练什么。我说练高远球和杀球。然后他跟我讲要从步伐练起，结果说了一大堆，都是网上大把的内容，我早就听过不知道多少遍了。<br>然后练了15分钟步伐。再说挥拍，他丫的挥拍时候说我动作不规范还过来用脚踢我的脚，态度还很不耐烦和生气的样子。我当时就纳闷了（TMD我交了这么多钱来第一次试个课，有不好的习惯肯定不是一下能纠正过来的，你指出来就好了。<br>另外，又不是培养国家队的，我是自己掏钱来学的，我是你客户，是你上帝啊，你态度能好点不，搞得好像他要培养职业球员一样的，把你当小孩）。后来，他说挥拍要屏住气，他丫的还打我胃部一拳。</p>
<p>还有个很无语的是，400块钱一个小时了，他那些训练球，一半以上的烂球，打不了的那种。<br>最让我鄙视的是他那种教球的态度，好表面上看是很严格，实际上是个LJ，价格收的贵，还装逼。要你叠好球，收球，他站在那动都不动。</p>
<p>他说的几点改进的地方是：</p>
<ol>
<li>挥拍后，左腿脚尖落地。</li>
<li>挥拍轨迹是一条直线，一个C，往右腿平面上挥。然后收拍再收到左边</li>
</ol>
<h2 id="私教二"><a href="#私教二" class="headerlink" title="私教二"></a>私教二</h2><p>今天又约了一个私教。这个教练是在一个俱乐部打球认识的。球打得不错，经常参加业余比赛，拿了不少业余比赛的奖项。<br>我问他练一次要多少钱，他说在包场地400元一个小时，那个场地大概是100多点一个小时。我抱着尝试一次的心态答应了，其实内心觉得还是挺贵的。<br>总觉得越贵的教练可能对自己的指导会有着完全不一样的感受，指导后可能会有飞跃的进步吧。</p>
<p>来到球场后，教练拿着一箱练习球。开始是打打高远球，打了几个有点让我不爽的是，和上次那个教练一样，练习球一大堆的烂球，球都扁了的，根本就打不了。不过这个在态度方面要比上次那个好很多。他说<br>他从小就练羽毛球没读书，然后最近因为打羽毛球混了个MBA。学校包你学费的，只要你帮他们打比赛就好。看他目前还混得不错吧。他说现在学生主要是教学生，小孩子。成人的比较少。深圳这个地方<br>做家教教小朋友真的很赚钱啊，太多有钱人家的孩子了。各种培训班兴趣班，各种人傻钱多的家长。</p>
<p>打了没几个球，教练开始给我指导，说我肩膀很僵硬，这要会导致下身的力传上来。要我放松肩膀，先不抓拍来摆动下大臂。先放松大臂，再进一步考虑小臂手腕。然后建议我在平时打高远球的时候可以考虑不要收拍，手臂一直举着。</p>
<p>然后，后面再练了练加上步伐的高远球。最后练了练杀球。</p>
<p>讲解+打了没几个球，很快1个小时就过去了。收获其实很小很小，性价比非常非常低。其实这些讲解的内容，网上李宇轩讲得比他的要详细很多，好很多。<br>虽然私教可以现场可以指出你的动作存在的问题，但是最重要的还是通过练习来让你找感觉，去纠正。如果指了没能纠正领悟过来，那也是没用。</p>
<p>我觉得90%还是靠练习。10%是知道自己的问题。如果私教不能现场给你喂球找感觉，那画这么多钱去上一个小时还不如自己网上去买李宇轩视频，然后把自己打球的过程录下视频发给他，让他指出你的不足，再自己找个水平好点的人一起练练，一起打打。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我看北京的私教300块能找到很专业的，至少会给你喂球练练，不会出现一堆的烂球。深圳这边400块一个小时的教练一堆的烂球，额呵呵。态度还很差，依然还有多学生找他呢。<br>总得来说，我觉得找私教对我来说是性价比非常低的，因为私教所说的这些内容网上都有，而且网上很多讲得比私教还好，好详尽，比如李宇轩的课程。什么人适合上私教的课呢，是那些根本就没有网上找资料，懒得自己去找资料的人。或者是刚打羽毛球的纯粹小白。<br>如果你对羽毛球有一定的了解，并且有能力、有动力去网上找学习资料的话，完全没有必要花钱找私教了。<br>这是妥妥的浪费钱，你更需要的是找一个好的陪练，通过打的过程中慢慢自己领悟，纠正自己的错误动作。</p>
]]></content>
      <categories>
        <category>羽毛球</category>
      </categories>
      <tags>
        <tag>badminton</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx如何配置</title>
    <url>/2020/howto_config_nginx.html</url>
    <content><![CDATA[<p>Nginx 推送动态内容给CGI, FastCGI, 和其它web服务器，比如Apache。返回内容再由Nginx传送给客户端。<br>本文将带你了解熟悉Nginx的配置</p>
<h2 id="Directives-Blocks-and-Contexts"><a href="#Directives-Blocks-and-Contexts" class="headerlink" title="Directives, Blocks, and Contexts"></a>Directives, Blocks, and Contexts</h2><p>所有的Nginx配置文件都在/etc/nginx/ 目录下。主要的配置文件是/etc/nginx/nginx.conf。</p>
<p>在Nginx内的配置选项称为指示<br>以组编排的指令被称作为 blocks 或 context。这两个术语是同等的。</p>
<p>‘#’开头的行会被注释，不会被Nginx解析。<br>如果行内包含指令的话必须以 ‘;’ 结尾，否则Nginx将不能加载配置，并报错。</p>
<p>下面是一个缩略版本的nginx.conf<br>文件以4个指令开头：user, worker_process, error_log, 和 pid。<br>这些指令都在block或context外面，所以他们被认为在 main context上。<br>Event和 http blocks是额外指令的区域，他们也同样存在于main context</p>
<blockquote>
<p>/etc/nginx/nginx.conf<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">user  nginx;</div><div class="line">worker_processes  1;</div><div class="line"></div><div class="line">error_log  /var/log/nginx/error.log warn;</div><div class="line">pid        /var/run/nginx.pid;</div><div class="line"></div><div class="line">events &#123;</div><div class="line">       . . .</div><div class="line">&#125;</div><div class="line"></div><div class="line">http &#123;</div><div class="line">       . . .</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
</blockquote>
<h2 id="http-Block"><a href="#http-Block" class="headerlink" title="http Block"></a>http Block</h2><p>http Block 包含处理web流量的指令。这些指令通常被定义为统一的.</p>
<p>下面是Nginx文档给出的可用的http block 列表：</p>
<blockquote>
<p>/etc/nginx/nginx.conf<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">http &#123;</div><div class="line">    include       /etc/nginx/mime.types;</div><div class="line">    default_type  application/octet-stream;</div><div class="line"></div><div class="line">    log_format  main  &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos;</div><div class="line">                      &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos;</div><div class="line">                      &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;;</div><div class="line"></div><div class="line">    access_log  /var/log/nginx/access.log  main;</div><div class="line"></div><div class="line">    sendfile        on;</div><div class="line">    #tcp_nopush     on;</div><div class="line"></div><div class="line">    keepalive_timeout  65;</div><div class="line"></div><div class="line">    #gzip  on;</div><div class="line"></div><div class="line">    include /etc/nginx/conf.d/*.conf;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
</blockquote>
<h2 id="Server-Blocks"><a href="#Server-Blocks" class="headerlink" title="Server Blocks"></a>Server Blocks</h2><p>上面的http block包含 include指令，告诉Nginx网站配置文件放置的地方。</p>
<p>如果你从官方仓库安装Nginx，会有这么一行 /etc/nginx/conf.d/*.conf; 像上面说的http block的那个一样<br>每个用Nginx部署的网站应该在这目录 /etc/nginx/conf.d/ 有自己的配置文件；命名一般为 example.com.conf<br>如果想disable它的话，可以改名为example.com.conf.disabled.</p>
<p>如果你从Debian或Ubuntu仓库安装的话，会有这么一行/etc/nginx/sites-enabled/*;<br>../sites-enabled/文件夹里包含 从 /etc/nginx/sites-available/ 链接过来的配置文件。<br>如果要disable网站的话，一般是删掉在sites-enabled 上的链接。</p>
<p>一般你都能在 /etc/nginx/conf.d/default.conf 或 etc/nginx/sites-enabled/default找到一个配置的例子供你参考。<br>不管你从哪个渠道安装，对一个网站，服务器配置文件总会包含一个或多个block，比如：</p>
<blockquote>
<p>/etc/nginx/conf.d/example.com.conf<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">server &#123;</div><div class="line">    listen         80 default_server;</div><div class="line">    listen         [::]:80 default_server;</div><div class="line">    server_name    example.com www.example.com;</div><div class="line">    root           /var/www/example.com;</div><div class="line">    index          index.html;</div><div class="line">    try_files $uri /index.html;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
</blockquote>
<h2 id="监听端口"><a href="#监听端口" class="headerlink" title="监听端口"></a>监听端口</h2><p>listen指令 告诉Nginx监听的 hostname/IP 和 TCP端口。<br>default_server参数是说 这个虚拟的host会回答在80端口上的请求，这个端口将不会回答其它的虚拟host。<br>第二个statement监听的是IP v6，其它没有什么差别。</p>
<p>Name-Based Virtual Hosting</p>
<h2 id="基于name的虚拟主机"><a href="#基于name的虚拟主机" class="headerlink" title="基于name的虚拟主机"></a>基于name的虚拟主机</h2><p>server_name指令允许在一个IP地址上存在多个域名。服务器根据收到的请求header决定域名。</p>
<p>你应该按每个域名来创建一个配置文件，或者每个网站一个配置。这里有些案例：</p>
<h3 id="同时处理example-com-和-www-example-com请求："><a href="#同时处理example-com-和-www-example-com请求：" class="headerlink" title="同时处理example.com 和 www.example.com请求："></a>同时处理example.com 和 www.example.com请求：</h3><blockquote>
<p>/etc/nginx/conf.d/example.com.conf<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">server_name   example.com www.example.com;</div></pre></td></tr></table></figure></p>
</blockquote>
<h3 id="server-name-指令也可以用-通配符-example-com-和-example-com，两个都会指使服务器处理example-com下的所有子域名："><a href="#server-name-指令也可以用-通配符-example-com-和-example-com，两个都会指使服务器处理example-com下的所有子域名：" class="headerlink" title="server_name 指令也可以用 通配符 *.example.com 和 .example.com，两个都会指使服务器处理example.com下的所有子域名："></a>server_name 指令也可以用 通配符 *.example.com 和 .example.com，两个都会指使服务器处理example.com下的所有子域名：</h3><blockquote>
<p>/etc/nginx/conf.d/example.com.conf<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">server_name   *.example.com;</div><div class="line">server_name   .example.com;</div></pre></td></tr></table></figure></p>
</blockquote>
<h3 id="处理所有example-开头的域名："><a href="#处理所有example-开头的域名：" class="headerlink" title="处理所有example.开头的域名："></a>处理所有example.开头的域名：</h3><blockquote>
<p>/etc/nginx/conf.d/example.com.conf<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">server_name   example.*;</div></pre></td></tr></table></figure></p>
</blockquote>
<p>Nginx允许你指定一个无效的域名。Nginx使用HTTP header里的name 来作为域名回答请求，不管domain name是否有效。<br>如果你用你是用内网的话，使用非域名 hostname是非常有效的。这包括在/etc/hosts配的域名。</p>
<h2 id="Location-Blocks"><a href="#Location-Blocks" class="headerlink" title="Location Blocks"></a>Location Blocks</h2><p>location 设置让Nginx返回服务期内的资源请求。和server_name指令一样，告诉Nginx怎么处理域名的请求。<br>location指令覆盖指定的文件和文件夹，比如<a href="http://example.com/blog/：" target="_blank" rel="external">http://example.com/blog/：</a></p>
<blockquote>
<p>/etc/nginx/sites-available/example.com<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">location / &#123; &#125;</div><div class="line">location /images/ &#123; &#125;</div><div class="line">location /blog/ &#123; &#125;</div><div class="line">location /planet/ &#123; &#125;</div><div class="line">location /planet/blog/ &#123; &#125;</div></pre></td></tr></table></figure></p>
</blockquote>
<p>上面的location是一字不差地匹配，它匹配主机segment之后的HTTP请求的任何部分：<br>Request: <a href="http://example.com/" target="_blank" rel="external">http://example.com/</a></p>
<p>Returns：假设server_name 是example.com, location / 指令会决定这个请求怎么处理<br>Nginx总是履行最匹配的请求：</p>
<p>Request: <a href="http://example.com/planet/blog/" target="_blank" rel="external">http://example.com/planet/blog/</a> 或 <a href="http://example.com/planet/blog/about/" target="_blank" rel="external">http://example.com/planet/blog/about/</a><br>Returns: 这个会履行 location /planet/blog/ ，因为 它最匹配这个location，即使/planet/ 同样匹配这个请求。</p>
<p>具体的location匹配可以参考下如下例子：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">location  = / &#123;</div><div class="line">  # 精确匹配 / ，主机名后面不能带任何字符串</div><div class="line">  [ configuration A ]</div><div class="line">&#125;</div><div class="line"></div><div class="line">location  / &#123;</div><div class="line">  # 因为所有的地址都以 / 开头，所以这条规则将匹配到所有请求</div><div class="line">  # 但是正则和最长字符串会优先匹配</div><div class="line">  [ configuration B ]</div><div class="line">&#125;</div><div class="line"></div><div class="line">location /documents/ &#123;</div><div class="line">  # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索</div><div class="line">  # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条</div><div class="line">  [ configuration C ]</div><div class="line">&#125;</div><div class="line"></div><div class="line">location ~ /documents/Abc &#123;</div><div class="line">  # 匹配任何以 /documents/Abc 开头的地址，匹配符合以后，还要继续往下搜索</div><div class="line">  # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条</div><div class="line">  [ configuration CC ]</div><div class="line">&#125;</div><div class="line"></div><div class="line">location ^~ /images/ &#123;</div><div class="line">  # 匹配任何以 /images/ 开头的地址，匹配符合以后，停止往下搜索正则，采用这一条。</div><div class="line">  [ configuration D ]</div><div class="line">&#125;</div><div class="line"></div><div class="line">location ~* \.(gif|jpg|jpeg)$ &#123;</div><div class="line">  # 匹配所有以 gif,jpg或jpeg 结尾的请求</div><div class="line">  # 然而，所有请求 /images/ 下的图片会被 config D 处理，因为 ^~ 到达不了这一条正则</div><div class="line">  [ configuration E ]</div><div class="line">&#125;</div><div class="line"></div><div class="line">location /images/ &#123;</div><div class="line">  # 字符匹配到 /images/，继续往下，会发现 ^~ 存在</div><div class="line">  [ configuration F ]</div><div class="line">&#125;</div><div class="line"></div><div class="line">location /images/abc &#123;</div><div class="line">  # 最长字符匹配到 /images/abc，继续往下，会发现 ^~ 存在</div><div class="line">  # F与G的放置顺序是没有关系的</div><div class="line">  [ configuration G ]</div><div class="line">&#125;</div><div class="line"></div><div class="line">location ~ /images/abc/ &#123;</div><div class="line">  # 只有去掉 config D 才有效：先最长匹配 config G 开头的地址，继续往下搜索，匹配到这一条正则，采用</div><div class="line">    [ configuration H ]</div><div class="line">&#125;</div><div class="line"></div><div class="line">location ~* /js/.*/\.js</div><div class="line"></div><div class="line"></div><div class="line">http://seanlook.com/2015/05/17/nginx-location-rewrite/</div><div class="line"></div><div class="line">location ~ ^/(trucks|cars|planes|system|tools)&#123;</div><div class="line"># 以trucks開頭或cars。。。</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>顺序 no优先级：<br>(location =) &gt; (location 完整路径) &gt; (location ^~ 路径) &gt; (location ~,~* 正则顺序) &gt; (location 部分起始路径) &gt; (/)</p>
<h2 id="Location-Root-和-Index"><a href="#Location-Root-和-Index" class="headerlink" title="Location Root 和 Index"></a>Location Root 和 Index</h2><p>location设置是另一个具有自己的参数block的变量。</p>
<p>一旦Nginx确定了哪个location指令最匹配给定请求，对这个请求的响应就由相关的location指令block的内容确定。<br>一个例子：</p>
<blockquote>
<p>/etc/nginx/sites-available/example.com<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">location / &#123;</div><div class="line">    root html;</div><div class="line">    index index.html index.htm;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
</blockquote>
<p>在这个例子中，root 是 在默认Nginx安装路径下的 html 目录。绝对路径是 /etc/nginx/html/.</p>
<p>Request: <a href="http://example.com/blog/includes/style.css" target="_blank" rel="external">http://example.com/blog/includes/style.css</a></p>
<p>Returns: Nginx会尝试寻找放置于/etc/nginx/html/blog/includes/style.css 内的文件</p>
<blockquote>
<p>注意，如果需要的话root 你的直接设为绝对路径</p>
</blockquote>
<p>index这个参数告诉Nginx如果没有设置路由的话，将返回哪个文件。比如：<br>Request: <a href="http://example.com" target="_blank" rel="external">http://example.com</a><br>Returns: Nginx将尝试返回/etc/nginx/html/index.html 这个文件。<br>如果index指令设置了多个值得话，Nginx将会按顺序来返回第一个存在的文件。比如如果index.html不存在相关目录的话，然后index.htm将会被返回。如果都不存在则会报404.<br>这有个更复杂的案例，展示了一组响应域example.com的服务器的location：</p>
<blockquote>
<p>/etc/nginx/sites-available/example.com<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">location / &#123;</div><div class="line">    root   /srv/www/example.com/public_html;</div><div class="line">    index  index.html index.htm;</div><div class="line">&#125;</div><div class="line"></div><div class="line">location ~ \.pl$ &#123;</div><div class="line">    gzip off;</div><div class="line">    include /etc/nginx/fastcgi_params;</div><div class="line">    fastcgi_pass unix:/var/run/fcgiwrap.socket;</div><div class="line">    fastcgi_index index.pl;</div><div class="line">    fastcgi_param SCRIPT_FILENAME /srv/www/example.com/public_html$fastcgi_script_name;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
</blockquote>
<p>在此示例中，所有以.pl扩展名结尾的资源请求都由第二个location block处理，第二个location block为这些请求指定一个fastcgi处理程序。<br>否则，Nginx使用第一个location 指令。 资源位于文件系统上的/srv/www/example.com/public_html/。 如果请求中未指定文件名，<br>Nginx将查找并提供index.html或index.htm文件。 如果找不到索引文件，则服务器将返回404错误。</p>
<p>我们分析下下面几个请求：</p>
<p>Request: <a href="http://example.com/" target="_blank" rel="external">http://example.com/</a></p>
<p>Returns：/srv/www/example.com/public_html/index.html（如果存在）。 如果该文件不存在，它将返回/srv/www/example.com/public_html/index.htm。 如果都不存在，Nginx将返回404错误。</p>
<p>Request: <a href="http://example.com/blog/" target="_blank" rel="external">http://example.com/blog/</a></p>
<p>Returns: /srv/www/example.com/public_html/blog/index.html（如果存在）。 如果该文件不存在，它将返回/srv/www/example.com/public_html/blog/index.htm。 如果都不存在，Nginx将返回404错误。</p>
<p>Request: <a href="http://example.com/tasks.pl" target="_blank" rel="external">http://example.com/tasks.pl</a></p>
<p>Returns: Nginx将使用FastCGI处理程序执行位于/srv/www/example.com/public_html/tasks.pl的文件并返回结果。tasks.pl and return the result.</p>
<p>Request: <a href="http://example.com/username/roster.pl" target="_blank" rel="external">http://example.com/username/roster.pl</a></p>
<p>Returns: Nginx将使用FastCGI处理程序执行位于/srv/www/example.com/public_html/username/roster.pl的文件并返回结果。</p>
<p>有问题可以留言，并关注我。谢谢大家。</p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>知乎TOP20热门评论</title>
    <url>/2020/zhihu_comment.html</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>之前爬取了知乎几乎所有的问题回答，有一亿多个回答吧。然后又爬取了所有大部分的评论comment，然后<br>放到Hive里做做统计，看看有没有什么有趣的发现。哈哈，也算是做做NLP研究吧。</p>
<h2 id="正题"><a href="#正题" class="headerlink" title="正题"></a>正题</h2><p>这里就直接列出点赞数TOP20的评论吧：</p>
<h3 id="TOP-20"><a href="#TOP-20" class="headerlink" title="TOP 20"></a>TOP 20</h3><ul>
<li>评论：<br>徐茂森！我看你是想死了！！跨年你自己跨去吧，我要回娘家</li>
<li>点赞数：<br>43898</li>
<li>问题：<br>你有哪些匿名才敢发出来的照片？</li>
</ul>
<h3 id="TOP19"><a href="#TOP19" class="headerlink" title="TOP19"></a>TOP19</h3><ul>
<li>评论：<br>你这种人就是欠赞[捂脸][捂脸]</li>
<li>点赞数<br>43906</li>
<li>问题:<br>你讨厌蔡徐坤吗？为什么？</li>
</ul>
<h3 id="TOP18"><a href="#TOP18" class="headerlink" title="TOP18"></a>TOP18</h3><ul>
<li>评论：<br>当代人无法挽回地越来越低俗</li>
<li>点赞数<br>43963</li>
<li>问题:<br>你最讨厌什么梗？</li>
</ul>
<h3 id="TOP17"><a href="#TOP17" class="headerlink" title="TOP17"></a>TOP17</h3><ul>
<li>评论：<br>“我是我男神的女神的男神的女神”</li>
<li>点赞数：<br>44414   </li>
<li>问题:<br>你有什么不愿意告诉另一半的秘密？</li>
</ul>
<h3 id="TOP16"><a href="#TOP16" class="headerlink" title="TOP16"></a>TOP16</h3><ul>
<li>评论：<br>我觉得不是电网牛逼。。。是你牛逼，如果这个社会上多一些你这样有责任心的人，电网得笑出声来，毕竟真出了问题会受到严厉考核，变压器那么多，电网工作人员有可能有巡视和监控不到位的地方，你这好心人给指出来了，他们避免了一次考核，巴不得好好感谢你。。。一个被各种考核的电网员工扎心留言。。</li>
<li>点赞数：<br>45347</li>
<li>问题:<br>国家电网到底有多强大？</li>
</ul>
<h3 id="TOP15"><a href="#TOP15" class="headerlink" title="TOP15"></a>TOP15</h3><ul>
<li>评论：<br>我的赞不是谁都能收的，就怕有命收没命享</li>
<li>点赞数：<br>45466</li>
<li>问题:<br>室友的哪些不该看的东西被你看到了？<h3 id="TOP14"><a href="#TOP14" class="headerlink" title="TOP14"></a>TOP14</h3></li>
<li>评论：<br>主持人周立波，是我最讨厌的公众人物</li>
<li>点赞数：<br>47372</li>
<li>问题:<br>综艺节目有哪一幕你看了最生气？<h3 id="TOP13"><a href="#TOP13" class="headerlink" title="TOP13"></a>TOP13</h3></li>
<li>评论：<br>看了你的文，太气了，那些记者也是间接杀人凶手╰（‵□′）╯</li>
<li>点赞数：<br>49618</li>
<li>问题:<br>你见过的最虐心的案子是什么？</li>
</ul>
<h3 id="TOP12"><a href="#TOP12" class="headerlink" title="TOP12"></a>TOP12</h3><ul>
<li>评论：<br>哈哈哈，你就秀吧，这么点字把自己全家夸了个遍。</li>
<li>点赞数：<br>50016</li>
<li>问题:<br>你听过小孩说的最可怕的话是什么？</li>
</ul>
<h3 id="TOP11"><a href="#TOP11" class="headerlink" title="TOP11"></a>TOP11</h3><ul>
<li>评论：<br>你说的第一点我同意！但是第二点我表示反对</li>
<li>点赞数：<br>52046</li>
<li>问题:<br>外星人是怎么看待地球人的？</li>
</ul>
<h3 id="TOP10"><a href="#TOP10" class="headerlink" title="TOP10"></a>TOP10</h3><ul>
<li>评论：<br>黄磊的教育真的很优秀了</li>
<li>点赞数：<br>53049</li>
<li>问题:<br>综艺节目有哪一幕让你看了最生气？</li>
</ul>
<h3 id="TOP9"><a href="#TOP9" class="headerlink" title="TOP9"></a>TOP9</h3><ul>
<li>评论：<br>刚跟媳妇说这名字不错 咱以后有孩子可以用  我媳妇说你傻啊 你知不知道自己姓钱。。。。</li>
<li>点赞数：<br>53108</li>
<li>问题:<br>有哪些让你惊艳到的名字？<h3 id="TOP8"><a href="#TOP8" class="headerlink" title="TOP8"></a>TOP8</h3></li>
<li>评论：<br>当代人无法挽回地越来越低俗</li>
<li>点赞数<br>53525</li>
<li>问题:<br>你最讨厌什么梗？<h3 id="TOP7"><a href="#TOP7" class="headerlink" title="TOP7"></a>TOP7</h3></li>
<li>评论：<br>于是又变成了前任?以后的女朋友再问你有没有给前任洗过，千万别答成：“洗过，还不止一个呢”😄</li>
<li>点赞数：<br>55969</li>
<li>问题:<br>男生的「求生欲」很强是一种什么样的表现？</li>
</ul>
<h3 id="TOP6"><a href="#TOP6" class="headerlink" title="TOP6"></a>TOP6</h3><ul>
<li>评论：<br>鹿晗：糟了，是心动的感觉！</li>
<li>点赞数<br>58822</li>
<li>问题:<br>有哪些让你觉得恶心的恶俗歌词？</li>
</ul>
<h3 id="TOP5"><a href="#TOP5" class="headerlink" title="TOP5"></a>TOP5</h3><ul>
<li>评论：<br>我要宰了你</li>
<li>点赞数<br>62974</li>
<li>问题:<br>人有没有前世？</li>
</ul>
<h3 id="TOP4"><a href="#TOP4" class="headerlink" title="TOP4"></a>TOP4</h3><ul>
<li>评论：<br>当时看到多多就觉得我将来有个女儿如果像多多这样会又聪明又懂得保护自己，我一定做梦都会偷偷笑醒的</li>
<li>点赞数：<br>63803</li>
<li>问题:<br>综艺节目有哪一幕让你看了最生气？</li>
</ul>
<h3 id="TOP3"><a href="#TOP3" class="headerlink" title="TOP3"></a>TOP3</h3><ul>
<li>评论：<br>出狱瘦了十几斤 我觉得这句话有诱导犯罪的嫌疑</li>
<li>点赞数：<br>64247</li>
<li>问题:<br>综艺节目有哪一幕让你看了最生气？<h3 id="TOP2"><a href="#TOP2" class="headerlink" title="TOP2"></a>TOP2</h3></li>
<li>评论：<br>知乎应该允许人点两次赞！</li>
<li>点赞数：<br>64360</li>
<li>问题:<br>综艺节目有哪一幕让你看了最生气？</li>
</ul>
<h3 id="TOP1"><a href="#TOP1" class="headerlink" title="TOP1"></a>TOP1</h3><ul>
<li>评论：<br>你怕是捡来的吧熟人只认识你姐不认识你（滑稽）</li>
<li>点赞数<br>75113</li>
<li>问题:<br>亲姐弟之间要不要避嫌？</li>
</ul>
]]></content>
      <categories>
        <category>daily</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>做空美股跟谁学，我的收获</title>
    <url>/2020/stock_short.html</url>
    <content><![CDATA[<h2 id="初入美股"><a href="#初入美股" class="headerlink" title="初入美股"></a>初入美股</h2><p>玩美股也断断续续有两三年了，以前都是正儿八经地买入正股，从来没有试过做空。今年年初阶段，因为瑞幸<br>的暴雷，见识到了中概股的诚信问题，后来从朋友那才得知其实很多中概股水分都挺大的，其中有一只叫<br>跟谁学更是如此。我一听跟谁学，这是啥公司呀，从未听过有这么一家公司啊。一查得知原来是做网上辅导的，<br>我想，这领域不是已经被新东方，学而思之类的机构所占领了么？哪里轮得到跟谁学这个等闲之辈啊。</p>
<h2 id="决定做空垃圾股"><a href="#决定做空垃圾股" class="headerlink" title="决定做空垃圾股"></a>决定做空垃圾股</h2><p>瑞幸的下场我们都知道了弄虚作假的后果。作为一个价值投资信仰者，我在想做空一个垃圾股也是价值投资<br>的一种体现啊。于是在40块钱的时候卖空了GSX，可万万没想到的是卖空的没几天，GSX就像吃了药似的不断<br>暴涨，到7月份一直涨到了60几块钱。账面上我直接亏了50%。</p>
<p>最近一两天（0708）有暴涨了20%，我真是服了，有点被侮辱智商的感觉，这股也太妖了吧。有点撑不住了，买了<br>点PUT，然后再平仓掉了部分正股。这玩意涨得毫无逻辑性可言，没有任何利好，纯粹是投机性的疯涨。呵呵。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>巴菲特曾经说：查理和我对做空都不陌生，我们两个都失败了。<br>芒格：做空并看着推动者把股价拉升是非常刺激的，但不值得去体验这种刺激</p>
<p>另外，即使做空了，一定一定要设置止损。设置止损，设置止损！</p>
<p>有个网友说的好：<br>做空是一个高技术的活，从跟谁学这里也学到一些：<br>1、做空只能顺势，不能逆势，所谓急涨不做空，你觉得他涨高了快了去空它，跟趋势作对不如等他下来的时围上去踩一脚；<br>2、做空要设置严格的止损，一旦看错了要认栽跑路，否则亏损可能远超预期，直至爆仓；</p>
<p>经过这次的经验和教训，希望以后自己能谨记这些经验</p>
]]></content>
      <categories>
        <category>security</category>
      </categories>
      <tags>
        <tag>security</tag>
      </tags>
  </entry>
  <entry>
    <title>内卷的数据挖掘何去何从</title>
    <url>/2020/dataminner.html</url>
    <content><![CDATA[<p>作为一个在大数据领域多年的从业者，接触过很多优秀的数据挖掘的同事。在和他们的交流中我也了解到了很多数据挖掘的从业之道。<br>要想成为一成功的数据挖掘专家职业的四个步骤：</p>
<p>步骤1：获得研究生以上的学历<br>在我接触过大厂的优秀的数据挖掘者中，几乎没有研究生一下学历的。<br>数据挖掘专家需要在数据科学以及业务管理方面有扎实的背景。相关的master学位包括计算机科学，数据科学，信息系统，统计和商业管理或任何相关领域。<br>你需要了解如何使用统计方法来分析数据，并且希望能够开发出预测模型。<br>数据挖掘专家必须能够将数据分析应用于实际的业务问题，因此开发商务智能的课程是出色的准备。</p>
<p>步骤2：先从简单的岗位开始<br>从简单的开始开始。如果毕业后没能找到一份理想的数据挖掘工作的话，你完全可以从数据分析师做起。先进入这个行业再说。<br>大学毕业后，寻找担任数据分析师的职位。这将使你进一步磨练技术技能，并对数据提取，转换和加载的过程有一个全面的了解。<br>你还需要牢牢掌握数据库设计等其他知识。</p>
<p>步骤3：熟练工具和修炼技能<br>熟悉数据分析工具，尤其是SQL，NoSQL，SAS和Hadoop,Spark<br>Java，Python\IPython<br>有Linux操作系统方面的经验。这些在IT领域都少不了的</p>
<p>熟练各种算法<br>真正了解一些算法比了解一些算法要好。 如果你非常了解线性回归，k均值聚类和逻辑回归，<br>可以解释和解释它们的结果，并且实际上可以从头到尾完成一个数据项目，那么你将比如果你知道每一个都更受雇 单一算法，但不能使用它们。<br>大多数时候，当你使用算法时，它将是库中的一个版本（你很少会编写自己的SVM实现-花费的时间太长）。</p>
<p>步骤4：多去实践<br>比如做一个股市预测的项目。看起来很困难，但仍然可以分解为一些小的可实施步骤。<br>我首先连接到yahoo finance API，并提取了每日价格数据。<br>然后，我创建了一些指标，例如过去几天的平均价格，并用它们来预测未来（请注意，这里没有真正的算法，只是技术分析）。<br>效果不是很好，所以我学习了一些统计数据，然后使用了线性回归。然后，我连接到另一个API，逐分钟收集数据，并将其存储在SQL数据库中。<br>依此类推，直到算法运行良好。</p>
<p>不仅可以让你学习SQL语法，还用它来存储价格数据，<br>没有应用程序的学习是不会很好地保留下来，也不会使你做好进行实际数据科学工作的准备。</p>
<p>最后，多投简历，多面试，多总结吧<br>以前有个同事刚开始做数据分析岗位，研究生读的是化学，所以基础都不好。那些算法啥的都是在做数据分析<br>后自学的。那时他想跳槽做数据挖掘之类的工作，毕竟工资会比数据分析高一些。看他每天都不停地投简历，<br>约面试。好长一段时间都没找到合适的工作，我都替他捏把汗。<br>最终还是有志者事竟成的找到一份满意的数据挖掘工作了。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title>关于LDAP和Kerberos</title>
    <url>/2020/ldap_kerberos.html</url>
    <content><![CDATA[<h3 id="两个对比"><a href="#两个对比" class="headerlink" title="两个对比"></a>两个对比</h3><p>首先，Kerberos是一个认证的方法。LDAP是一个为了沟通兼容LDAP数据库的协议。</p>
<p>LDAP比较不方便(刚开始上手感觉很难)，但是简单。相反，Kerberos比较方便但更复杂。</p>
<p>Kerberos比LDAP更安全。</p>
<p>它们是完全不同用途的完全不同协议。<br>我认为人们倾向于将两者放在一起是因为通常一个项目会把它们会放到同一个package下</p>
<p>简单地说：<br>LDAP是一个访问目录的协议（比如：OpenLDAP或者目录）。<br>它的部分功能是，它有能力去通过账号密码认证一个连接。</p>
<p>Kerberos是一个认证和一个单登入协议。<br>它让一个进程去认证一个提供登陆后并且加密的ticket（进程用来访问文件，程序等资源）的服务器<br>因为进程可以保存ticket然后用它来访问不同的程序，同时避免了用户再次认证。</p>
<h3 id="LDAP是干嘛的"><a href="#LDAP是干嘛的" class="headerlink" title="LDAP是干嘛的"></a>LDAP是干嘛的</h3><p>LDAP就是花名册，LDAP服务器就是门卫处的老大爷。<br>对着花名册看你是不是里面的人，决定放不放你进来。</p>
<p>没人喜欢记忆一大堆混乱的账号和密码，员工不喜欢，企业更不喜欢。<br>这样基于LDAP的统一账号体系就搭建完成了，员工可以用一个账号和密码来访问和对接全部软件和研发工具，<br>公司的众多软件基础设施也开始能从账户层面进行简单明确的管理了。</p>
<h3 id="Kerberos在Ubuntu18上的安装"><a href="#Kerberos在Ubuntu18上的安装" class="headerlink" title="Kerberos在Ubuntu18上的安装"></a>Kerberos在Ubuntu18上的安装</h3><h4 id="安装server端"><a href="#安装server端" class="headerlink" title="安装server端"></a>安装server端</h4><p>sudo apt-get install krb5-admin-server krb5-kdc<br>然后会弹出一个输入框：输入realm（这里我设为DENNIS.COM)<br>然后输入kerberos server for your realm：我输入为我当前的hostname device2<br>然后输入administrative server for kerberos realm：我输入为我当前的hostname device2</p>
<p>下一步是：<br>sudo kdb5_util create -r DENNIS.COM -s</p>
<p>下一步启动service：<br>sudo service krb5-kdc restart<br>sudo service krb5-admin-server restart</p>
<p>测试新用户：<br>kinit dennis/admin<br>klist 列出缓存的Kerberos票据<br>klist</p>
<h4 id="安装client"><a href="#安装client" class="headerlink" title="安装client"></a>安装client</h4><p>一旦用户成功登陆到操作系统，他即可访问任何kerberized服务。<br>sudo apt-get install krb5-user</p>
<p>创建新的管理员用户（实体）<br>sudo kadmin.local<br>进入shell交互<br>输入 addprinc dennis/admin<br>然后输入两次密码<br>再quit退出</p>
<p>验证用户：<br>kinit dennis/admin<br>然后klist查看</p>
<p>为kdc-client添加秘钥 keytab<br>先进入kdc-server， kadmin.local 进入交互模式<br>addprinc hdfs/agent@DENNIS.COM</p>
<p>xst -norandkey -k hdfs.keytab hdfs/agent@DENNIS.COM</p>
<p>然后回到client机器上, 新增hdfs的Linux账号，输入命令kinit加载秘钥。<br>执行：kinit -k -t hdfs.keytab hdfs/agent@DENNIS.COM</p>
<blockquote>
<p>如下是principals的介绍<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Kerberos Principals</div><div class="line">kerberos中最重要的概念是principal，在这里就再带一遍。</div><div class="line"></div><div class="line">Principal可以理解为用户或服务的名字，全集群唯一，由三部分组成：username(or servicename)/instance@realm，</div><div class="line">例如：nn/host001@TEST.COM，host001为集群中的一台机器；或admin/admin@TEST.COM，管理员账户。</div><div class="line"></div><div class="line">username or servicename：在本文里为服务，HDFS的2个服务分别取名为nn和dn，即namenode和datanode</div><div class="line">instance：在本文里为具体的FQDN机器名，用来保证全局唯一（比如多个datanode节点，各节点需要各自独立认证）</div><div class="line">realm：域，我这里为http://TEST.COM（全大写哟）</div></pre></td></tr></table></figure></p>
</blockquote>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://juejin.im/entry/5aec6ac46fb9a07ac3635884" target="_blank" rel="external">https://juejin.im/entry/5aec6ac46fb9a07ac3635884</a><br><a href="https://zhuanlan.zhihu.com/p/32732045" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/32732045</a><br><a href="https://help.ubuntu.com/lts/serverguide/openldap-server.html" target="_blank" rel="external">https://help.ubuntu.com/lts/serverguide/openldap-server.html</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/34556597" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/34556597</a><br><a href="https://www.cnblogs.com/yinzhengjie/p/10765503.html" target="_blank" rel="external">https://www.cnblogs.com/yinzhengjie/p/10765503.html</a><br><a href="https://blog.gmem.cc/kerberos-under-ubuntu" target="_blank" rel="external">https://blog.gmem.cc/kerberos-under-ubuntu</a><br><a href="http://www.out1kiss.me/?p=538" target="_blank" rel="external">http://www.out1kiss.me/?p=538</a><br><a href="https://blog.csdn.net/bluishglc/article/details/84892155" target="_blank" rel="external">https://blog.csdn.net/bluishglc/article/details/84892155</a></p>
]]></content>
      <categories>
        <category>security</category>
      </categories>
      <tags>
        <tag>security</tag>
      </tags>
  </entry>
  <entry>
    <title>Databricks Delta Lake 简介</title>
    <url>/2020/delta_lake_intro.html</url>
    <content><![CDATA[<h2 id="百花齐放的大数据生态"><a href="#百花齐放的大数据生态" class="headerlink" title="百花齐放的大数据生态"></a>百花齐放的大数据生态</h2><p>17，18是计算引擎火热的两年，19年已然是红海了。计算引擎中的王者是Spark，综合指标最好，生态也好，当其他引擎还在ETL,交互查询，流上厮杀时，Spark已经在AI领域越走越远。</p>
<p>对比明显的是，计算层的上层和下层在17,18年却乏善可陈。但是到19年整个局势开发生变化，向下走是存储层Delta Lake耀眼夺目，解决了原先数仓的诸多痛点，让数仓进化到数据湖。<br>向上走是交互应用层的Linkis一马当先，形成交互标准，粘合周边生态，很好的衔接了应用交互层和计算引擎层的衔接。</p>
<p>问题重重的数据存储层<br>前面我们提到，早先基于Hive的数仓或者传统的文件存储形式（比如Parquet/ORC）,都存在一些长期难以解决的问题：</p>
<p>小文件的问题<br>并发读写问题<br>有限的更新支持<br>海量元数据（例如分区）导致Hive metastore不堪重负<br>细节展开的话，你会发现每一个问题又会引发众多应用场景层面的问题。</p>
<p>比如并发读写还有更新问题让实时数仓的实现变得很困难。小文件问题需要我们自己写合并代码，并且在合并过程中还会造成数据不可读的问题。如此种种不一而足。</p>
<p>为了解决这些先天不足的问题，我们只能通过复杂的架构设计来解决这些问题（美其名曰lambda架构）。<br>比如为了解决先天不足的更新问题，我们可能需要先将数据写入一个其他的系统（如HBase）,然后再将HBase导出成Parquet文件/Hive表供下游使用。<br>在复杂的流程（超长的Pipeline）运行的过程中，还会不断涉及到Schema的变换以及磁盘的读取，所以架构复杂了不仅仅会导致运维成本高企，CPU/IO浪费也就变得异常严重。</p>
<p>其实上面这些问题的根源，都是因为存储层不给力，为了曲线救国，我们无奈通过架构来弥补。<br>Delta Lake单刀直入，直接解决存储层的问题，带来的益处就是极大的简化我们的架构设计，简化运维成本，降低服务器成本。</p>
<h2 id="Delta-Lake-生之逢时"><a href="#Delta-Lake-生之逢时" class="headerlink" title="Delta Lake 生之逢时"></a>Delta Lake 生之逢时</h2><p>天下苦传统数仓久已，Delta Lake 横空出世，那么它是如何解决上面的存储层问题呢？我列举了如下几个重要的特性：</p>
<p>以元数据也是大数据思想武装自己，设计了基于HDFS存储的元数据系统，解决metastore不堪重负的问题。<br>支持更多种类的更新模式，比如Merge/Update/Delete等操作，配合流式写入或者读取的支持，让实时数据湖变得水到渠成。<br>流批操作可以共享同一张表<br>版本概念，可以随时回溯，避免一次误操作或者代码逻辑而无法恢复的灾难性后果。<br>Delta Lake 其实只是一个Lib库</p>
<p>Delta Lake 是一个lib 而不是一个service,不同于HBase,他不需要单独部署，而是直接依附于计算引擎的。<br>目前只支持Spark引擎。这意味什么呢？<br>Delta Lake 和普通的parquet文件使用方式没有任何差异，你只要在你的Spark代码项目里引入delta包，按标准的Spark datasource操作即可，可谓部署和使用成本极低。</p>
<h2 id="Delta-Lake到底是什么"><a href="#Delta-Lake到底是什么" class="headerlink" title="Delta Lake到底是什么"></a>Delta Lake到底是什么</h2><p>Parquet文件 + Meta 文件 + 一组操作的API = Delta Lake.</p>
<p>所以Delta没啥神秘的，和parquet没有任何区别。但是他通过meta文件以及相应的API,提供众多特性功能的支持。<br>在Spark中使用它和使用parquet的唯一区别就是把format parquet换成detla。</p>
<p>和Hive如何整合</p>
<p>因为惯性以及历史的积累，大家还是希望能像使用hive那样使用delta,而不是去使用spark的datasource API。<br>截止到笔者写这些文字之前，官方还没有支持。不过官方透露阿里的同学已经在做这块的整合。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>delta lake</tag>
      </tags>
  </entry>
  <entry>
    <title>2020大数据未来就业方向</title>
    <url>/2020/bigdata_jobs.html</url>
    <content><![CDATA[<h2 id="大数据未来"><a href="#大数据未来" class="headerlink" title="大数据未来"></a>大数据未来</h2><p>2020年及以后的大数据的未来<br>大数据不仅是一个术语，而且还与许多新兴技术捆绑在一起，例如人工智能，机器学习，区块链，增强现实，物联网等。我之所以列出上述技术的原因是，许多报告预测，这些技术将大行其道，并将在2020年及之后实现革命性的增长。对于所有这些技术，大数据将成为关键来源，甚至我们可以说它是“新兴技术的生命”。</p>
<p>我们生活在数字时代，在世界的每个角落，都有人不断寻找新技术或某种东西来改造这个世界。结果，每天都会引入许多技术或趋势。一些在流行和接受度很高的游戏中占据上风，但是某些技术在特定时期内一直保持病毒式传播，在此之后它就消失了。</p>
<p>但是，大数据并不是消失的技术之一。许多行业专家预测，大数据前景广阔，在任何情况下都不会黯淡。</p>
<p>随着云技术的爆炸式增长，处理不断增长的数据海的需求已成为设计数字架构的底层考虑。 在一个交易，库存乃至IT基础架构可以完全以虚拟状态存在的世界中，良好的大数据方法可通过从许多来源获取数据来创建整体概览，包括：</p>
<p>虚拟网络日志<br>安全事件和模式<br>全球网络流量模式<br>异常检测与解决<br>合规信息<br>客户行为和偏好跟踪<br>地理位置数据<br>社交渠道数据，用于品牌情感跟踪<br>库存水平和货运跟踪</p>
<p>下面是根据PayScale统计的大数据相关岗位TOP7：</p>
<ol>
<li>数据科学家<br>能够为大型企业挖掘和解释复杂数据的人才信息技术（IT）数据科学家有很多机会。 他们与跨职能的IT团队合作，编译并创建各种统计数据模型，以为他们制定与系统相关的建议和行动计划提供信息。</li>
</ol>
<p>首选资格-IT数据科学家应该对不同的数据挖掘技术（例如聚类，回归分析，决策树和支持向量机）有深入的了解。 这种职位通常需要计算机科学的高级学位（例如，硕士或博士学位），此外还需要相关领域的前几年工作经验。</p>
<ol>
<li>数据工程师<br>数据工程师利用其计算机科学和工程技术的优势来汇总，分析和处理海量数据集。常见任务包括创建计算机算法并将其转换为原型代码，开发技术流程以改善数据可访问性，以及为最终用户设计报告，仪表板和工具。</li>
</ol>
<p>首选资格-雇主通常要求数据工程职位的应聘者必须成功完成计算机科学，工程学或相关领域的大学学位。他们还更喜欢在该领域拥有三到五年经验的申请人。<br>所需的技术能力包括Linux系统知识，SQL数据库设计能力以及对诸如Java，Python，Kafka，Hive或Storm之类的编码语言的熟练掌握。<br>软技能包括出色的书面和口头交流能力以及独立工作和与团队合作的能力。</p>
<ol>
<li>数据分析<br>PayScale描述了数据分析人员如何通过设计和实施大规模调查来收集有关各种主题的可行信息。 他们的工作是招募调查参与者，汇编和解释提交的数据，并以传统图表和报告以及数字格式传达他们的发现。</li>
</ol>
<p>优先任职资格-寻找数据分析师工作的人员必须对计算机程序（例如Microsoft Excel，PPT和SQL数据库）有所了解。 数据分析师还需要良好的沟通和表达能力，并且需要能够将经常复杂的信息有效地翻译给公司利益相关者。</p>
<ol>
<li>安全工程师<br>安全工程师在IT灾难计划，规避和缓解中起着至关重要的作用。它们通过设置计算机防火墙，检测和响应入侵来减少企业风险。他们还为新的或更新的软件和硬件创建和实施测试计划，并为计算机网络建立多层防御协议。</li>
</ol>
<p>首选资格-该职位需要工程，计算机科学或相关领域的学士学位，以及几年的相关工作经验以及理想的行业安全认证。除了对计算机语言和操作系统的技术了解之外，安全工程师还应具备扎实的解决问题和数学能力以及独立工作的能力。</p>
<ol>
<li><p>数据库管理员<br>经过培训且在项目管理和多任务处理方面非常熟练的数据库管理员可以对复杂的数据库进行诊断和修复。 他们还审查对数据和数据使用情况的业务请求，评估数据源以改善数据馈送并帮助设计和安装存储硬件。<br>首选资格–数据库经理的招聘广告通常会列出信息技术学士学位，并且至少要在数据库领导职位中担任五年以上才能作为首选资格。 数据库管理角色的求职者也应该精通不同的数据库软件，例如MySQL和Oracle。</p>
</li>
<li><p>数据架构师<br>数据架构师使用他们对面向数据的计算机语言的知识来组织和维护关系数据库和公司存储库中的数据，从而为企业数据模型的每个主题领域开发数据架构策略。</p>
</li>
</ol>
<p>优先任职资格–雇主在数据架构师中寻求的常见工作技能和关键字属性包括高级技术水平（尤其是SQL等语言），出色的分析敏锐度，出色的数据平台架构能力(比如Hadoop相关的大数据平那台）、<br>创造性的可视化能力和解决问题的能力，以及注重细节的能力。 大多数数据架构师至少在计算机科学相关领域获得了学士学位（并且通常是高级学位）。</p>
<ol>
<li>技术招聘人员<br>技术招聘人员专门为人才库寻找和筛选有才华的大数据，IT和其他技术专业人员。 他们与公司紧密合作以评估其招聘需求，然后在市场上寻找最适合特定职位空缺的候选人。<br>他们还支持他们在整个工作申请，面试，雇用和入职过程中招募的专业候选人。 </li>
</ol>
<p>最后打个小广告，本人从事大数据行业6年，曾在多家世界500强企业数据岗位待过.关注我可以免费送Hadoop、Hive、Spark、Python相关大数据学习资料。</p>
<h2 id="大数据就业方向"><a href="#大数据就业方向" class="headerlink" title="大数据就业方向"></a>大数据就业方向</h2><p>其实大数据行业是个很大的范围，里面分很多个方向。我来列几个工业界最长见的方向吧：</p>
<ol>
<li>大数据开发。<br>这是招聘的坑位相对较多的一种岗位。<br>主要需要的技术栈有：Java/Scala，Hadoop，Spark/Flink，Hive，HBase，Kafka，Zookeeper.</li>
</ol>
<p>还有其它Hadoop衍生出来的技术框架：Impala/Presto，Kylin，Druid等等。</p>
<p>这个岗位是一个开发岗位，所以需要Java基础比较好，一般都是要有一定的开发经验。然后需要学的技术栈<br>主要是围绕着Hadoop体例的框架。这都需要有比较大的付出。</p>
<p>工作内容一般是开发一些数据处理平台的后端，比如ETL平台、数据报表平台、机器学习自动化平台等等</p>
<ol>
<li>大数据运维/架构<br>这个岗位招的人比较少。</li>
</ol>
<p>主要技能：Linux/shell脚本, Hadoop原理，调度工具Oozie/Azkaban/Airflow<br>工作内容一般是搭建和维护Hadoop集群，比如CDH集群；搭建和维护Hadoop下的各种组件，<br>Hive、HBase、Kafka、Zookeeper等等。<br>这种工作比起大数据开发更不费脑力，但是工作比较繁杂。对不太喜欢做开发的人，是个不错的选择。</p>
<ol>
<li>ETL工程师<br>这种也是招人数量比较多的岗位，但要求一般不高，有专科学历就够了。<br>因为很多公司尤其是大公司，有大量数据需要人力去倒腾处理。</li>
</ol>
<p>主要需要的技能有：各种数据库的SQL，PLSQL。各种关系型数据库如MySQL/PostgresSQL, Hive</p>
<p>会一点编程语言比如Python更好，如虎添翼。</p>
<p>工作内容一般是做数据清洗，比如从爬虫出来的数据一般比较脏比较乱，需要过滤出有用的信息保存到数据库中，<br>然后将数据从一个地方转移到另一个地方。比如从MySQL到Hive</p>
<ol>
<li>大数据分析师<br>这个岗位属于大数据比较上层的岗位，比较适合女生，因为比起大数据开发，大数据分析所学的东西少了很多。</li>
</ol>
<p>主要的技能有：SQL/Hive-SQL, Spark, Python/R<br>其它的技能视自己想发展的小方向而定。强烈推荐学精通一些Python，因为Python有很多数据分析各个方向<br>的第三方模块，比如pandas,matplotlib,seaborn,superset,tensorflow,sklearn等等<br>工作内容就是在大数据前提下的数据分析，以前用Excel就足够了，但现在数据量大了就得用新的数据分析技能</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title>Kyligence面试经历</title>
    <url>/2019/kyligence_interview.html</url>
    <content><![CDATA[<h2 id="投简历"><a href="#投简历" class="headerlink" title="投简历"></a>投简历</h2><p>最近偶尔在Boss直聘上看工作机会，看到Kyligence这家公司在招Hadoop实施工程师，看了下，<br>好像还挺适合我的，公司的也挺牛的，这是Apache Kylin背后的公司。于是我就在上面询问面试官。<br>然后我发了份简历给他，本来也没报多大希望，毕竟自己的条件并不突出。</p>
<h2 id="通知面试"><a href="#通知面试" class="headerlink" title="通知面试"></a>通知面试</h2><p>过了一天，我随口问了下面试官，我的条件符合吗？没想到的是，面试官居然说我挺合适的！这让我<br>感到非常惊喜，因为之前投了太多公司都没有回复，甚至投了很多很low的公司的没有任何下文。</p>
<h2 id="第一次面试"><a href="#第一次面试" class="headerlink" title="第一次面试"></a>第一次面试</h2><p>我满怀着期待去参加面试。第一次面试地点是约在一个咖啡店（后来才知道这是因为面试官刚好要在<br>那边拜访客户）。面试官还请我喝了一杯咖啡。</p>
<p>面试细节以后补充。。</p>
<h2 id="第二次面试"><a href="#第二次面试" class="headerlink" title="第二次面试"></a>第二次面试</h2><p>回去不就，我就接到了上海Kyligence HR的电话。她说面试官对我的表现反馈还不错，我顿时又很惊喜，<br>我还以为我没机会了。然我约了我进行第二次面试的时间，第二次面试是一个电话面试。</p>
<p>我觉得这个公司各方面都非常不错。首先，很尊重面试者，面试官都很nice；然后技术也非常强，可以说是<br>国内大数据行业的领头羊，我要是能加入是一种荣幸呀。</p>
<p>面试细节以后补充。。</p>
<p>整个通话时间在50分钟左右。</p>
<p>面试完后，面试官说有消息会进一步联系我。我感觉自己有几个问题答得很含糊，不知道面试官会不会<br>把我pass掉。等了一天没有回复，内心很失落，以为自己被太太了。又过了一天，我鼓起勇气在Boss上问面试官我这轮面试过了吗？<br>很快便得到了回复，面试官的回复让我有点意外，他说会有下轮面试，让我耐心等候。</p>
<h2 id="第三轮面试"><a href="#第三轮面试" class="headerlink" title="第三轮面试"></a>第三轮面试</h2><p>等了1天，果然，上海的HR小姐姐给我来电了，她说这两轮对我的表现比较满意，想邀请我到深圳这边的<br>驻点办公室进行第三轮面试，面试者是南区这边的负责人，也就是深圳这边的老大吧。<br>面试细节以后补充。。</p>
<p>大概聊了半个多小时，就快到中午12点了。让我回去，有消息再进一步联系。。</p>
<h2 id="第四轮面试"><a href="#第四轮面试" class="headerlink" title="第四轮面试"></a>第四轮面试</h2><p>过了两天，下午突然收到了上海总部HR打来的电话，说跟我聊个20分钟。首先，她说经过这么<br>多轮的面试，面试官们对我的表面都挺满意的。问我对公司的感觉怎么样，我说我对公司也非常<br>满意，公司的人都很Nice，很受尊重。</p>
<h2 id="收到Offer"><a href="#收到Offer" class="headerlink" title="收到Offer"></a>收到Offer</h2><p>再过了一个工作日，上海的HR电话我，说公司领导对我都表示满意，正式给我发Offer。非常开心，能够<br>加入一个自己理想的公司，好好努力！</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>《Spark内核设计的艺术架构设计与实现》的笔记</title>
    <url>/2019/spark_internal_reading_notes.html</url>
    <content><![CDATA[<h2 id="为什么什么买这本书"><a href="#为什么什么买这本书" class="headerlink" title="为什么什么买这本书"></a>为什么什么买这本书</h2><p>最近想为Spark社区做点贡献吧，又不知道从何下手，就想先买本Spark原理的书来研究下先。<br>于是在淘宝上花了90多买了这本 《Spark内核设计的艺术架构设计与实现》</p>
<h3 id="Akka-到-Netty"><a href="#Akka-到-Netty" class="headerlink" title="Akka 到 Netty"></a>Akka 到 Netty</h3><p>了解到Spark1.x使用了Akka做内部的通信架构。比如Spark各个组件之间的通信，用户文件与Jar包的上传，<br>节点之间的shuffle过程，Block数据的复制与备份等等。<br>然而，在Spark2.0 Akka被移除，而是写了自己基于Netty的RPC框架。</p>
<h4 id="那么Netty为什么可以取代Akka？"><a href="#那么Netty为什么可以取代Akka？" class="headerlink" title="那么Netty为什么可以取代Akka？"></a>那么Netty为什么可以取代Akka？</h4><p>首先不容置疑的是Akka可以做到的，Netty也可以做到，但是Netty可以做到，Akka却无法做到，原因是啥？<br>在软件栈中，Akka相比Netty要Higher一点，它专门针对RPC做了很多事情，而Netty相比更加基础一点，可以为不同的应用层通信协议（RPC，FTP，HTTP等）提供支持，<br>在早期的Akka版本，底层的NIO通信就是用的Netty；其次一个优雅的工程师是不会允许一个系统中容纳两套通信框架，恶心！<br>最后，虽然Netty没有Akka协程级的性能优势，但是Netty内部高效的Reactor线程模型，无锁化的串行设计，<br>高效的序列化，零拷贝，内存池等特性也保证了Netty不会存在性能问题。</p>
<h4 id="TransportClient"><a href="#TransportClient" class="headerlink" title="TransportClient"></a>TransportClient</h4><p>有了TransportClientFactory，Spark的各个模块就可以使用它创建RPC客户端TransportClient了。<br>每个TransportClient实例只能和一个远端的RPC服务通信，所以Spark中的<br>组件如果想要和多个RPC服务通信，就需要持有多个TransportClient实例。</p>
<h3 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h3><p>了解到了软件开发有个度量系统这么个东西，哈哈长见识了。Spark的度量系统是用的Metrics(<a href="https://github.com/dropwizard/metrics" target="_blank" rel="external">https://github.com/dropwizard/metrics</a>)</p>
<h3 id="SparkContext"><a href="#SparkContext" class="headerlink" title="SparkContext"></a>SparkContext</h3><p>SparkContext 是Spark中的元老级API，从0.x.x版本就已经存在了。SparkContext相当于Spark应用程序的<br>发动机引擎，而SparkContext的参数则有SparkConf负责，SparkConf就是操作面板。</p>
<h3 id="第五章Spark的执行环境"><a href="#第五章Spark的执行环境" class="headerlink" title="第五章Spark的执行环境"></a>第五章Spark的执行环境</h3><p>前几篇幅，主要讲了Spark的RPC，这玩意的作用比如说：从worker是怎么从Driver端去下载jar包之类的依赖文件。<br>看了十几页，完全没有头绪，细节代码也看不懂。总结一下：对于看不懂的篇章，并且不是很感兴趣的内容还是跳过吧。<br>不要在上面耗费太多的时间。我觉得看这种书还是要提前看看目录了解下再看内容，不要一章一章地阅读全书。</p>
<p>另外，这里我想说说看书的状态。看书要在一个好的状态下才能有效率地阅读。我认为好的状态有两个方面：一个是<br>外界环境，另一个是自己的身心状态。<br>外界状态有这么几个方面吧：安静程度，受打扰程度，氛围环境等等。所以通常我在图书馆里面的阅读效率是比较高的。<br>身心状态有：身体状况，心情状态等等。比如最近几天我的坐骨神经就很不舒服，非常很勉强地去看书，效率非常低。<br>还是要先把身体状态调整好再去阅读吧。</p>
<h3 id="第六章-Spark-Block"><a href="#第六章-Spark-Block" class="headerlink" title="第六章 Spark Block"></a>第六章 Spark Block</h3><p>文件系统的文件在存储到磁盘上时，都是以块为单位写入的。操作系统的块是以固定的大小读写的，例如：512字节、<br>1024字节、2048字节等。</p>
<p>在Spark的存储体系中，数据的读写也是以块为单位，只不过这个块并非操作系统的块，而是涉及用于Spark存储体系的块。<br>每个Block都有唯一的标识，Spark把这个标识抽象为BlockId。</p>
<p>Spark与Hadoop的重要区别之一就是对于内存的使用。Hadoop只讲内存作为计算资源，Spark除将内存作为计算资源外，<br>还将内存的一部分纳入到存储体系中。Spark使用MemoryManager对存储体系和内存计算所使用的内存进行管理。</p>
<p>StorageMemoryPool是存储体系用到的内存池，ExecutionMemoryPool是计算引擎用到的内存池</p>
<p><img src="/images/2019/spark_internal_reading_notes/5fa6c7da.png" alt=""></p>
<p>MemoryStore负责将Block存储到内存。Spark通过将广播数据、RDD、Shuffle数据存储到内存，减少了对磁盘IO<br>的依赖，提高了程序的读写效率。</p>
<h3 id="第七章-调度系统"><a href="#第七章-调度系统" class="headerlink" title="第七章 调度系统"></a>第七章 调度系统</h3><p>Scheduler 分为DAGScheduler 和 TaskScheduler</p>
<p>容错处理：当某个worker节点上的Task失败时，可以利用DAG重新调度计算这些失败的Task(执行已成功的Task可以从CheckPoint中读取<br>，而不用重新计算)。在流失计算的场景中，Spark需要记录日志和CheckPoint，以便利用CheckPoint和日志对数据恢复。</p>
<p>Spark中的Job可能包含一个到多个Stage，这些Stage的划分是从ResultStage开始，从后往前边划分边创建的。（执行的时候应该不是）</p>
<p><img src="/images/2019/spark_internal_reading_notes/88070957.png" alt=""></p>
<p>调度池对TaskSet的调度取决于调度算法，有两种可选的算法：FIFO；<br>Spark的调度池中的调度队列与YARN中调度队列的设计非常相似，也采用了层级队列的管理方式。</p>
<p><img src="/images/2019/spark_internal_reading_notes/4602d935.png" alt=""></p>
<p>我看了一下官网的文档：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">## FIFO</div><div class="line">By default, Spark’s scheduler runs jobs in FIFO fashion. Each job is divided into “stages” (e.g. map and reduce phases), </div><div class="line">and the first job gets priority on all available resources while its stages have tasks to launch, then the second job gets priority, etc. </div><div class="line">If the jobs at the head of the queue don’t need to use the whole cluster, </div><div class="line">later jobs can start to run right away, but if the jobs at the head of the queue are large, </div><div class="line">then later jobs may be delayed significantly.</div><div class="line"></div><div class="line">## FAIR</div><div class="line">The fair scheduler also supports grouping jobs into pools, and setting different scheduling options (e.g. weight) for each pool. </div><div class="line">This can be useful to create a “high-priority” pool for more important jobs, </div><div class="line">for example, or to group the jobs of each user together and give users equal shares regardless of how many concurrent jobs they have </div><div class="line">instead of giving jobs equal shares. </div><div class="line">This approach is modeled after the Hadoop Fair Scheduler.</div><div class="line"></div><div class="line">Without any intervention, newly submitted jobs go into a default pool, </div><div class="line">but jobs’ pools can be set by adding the spark.scheduler.pool “local property” to the SparkContext in the thread that’s submitting them.</div></pre></td></tr></table></figure></p>
<blockquote>
<p>想法20200130<br>要看懂Spark的源码，设计模式得比较熟练呀。里面的这些类是怎么依赖的，是怎么设计的，我完全不懂，这是一拍脑袋决定的呢？还是有<br>这种设计相关的设计模式套路。所以，我觉得设计模式应该要掌握好才能让读代码更有意义。</p>
</blockquote>
<p>Spark的资源调度分为两层：<br>第一层是Cluster Manager（Yarn模式下是RM，Standalone模式为Master）将资源分配给Application<br>第二层是Application进一步将资源分配给Application的各个Task。</p>
<p>ExecutorLossReason 有四个子类，分别代表四种不同的原因：<br>SlaveLost：Worker丢失<br>LossReasonPending：位置的原因导致的Executor退出。<br>ExecutorKilled：Executor被杀死了。<br>ExecutorExited：Executor退出了。</p>
<p>OutputCommitCoordinator 是DAGScheduler中的重要组件，其实现方式简洁明了，有很高的借鉴意义。<br>TaskScheduler依赖于LaucherBackend和SchedulerBackend。通过对TaskSchedulerImpl的初始化、<br>启动、提交Task、资源分配等功能的分析，更深入地了解TaskSchedulerImpl的调度流程。</p>
<h2 id="第八章计算引擎"><a href="#第八章计算引擎" class="headerlink" title="第八章计算引擎"></a>第八章计算引擎</h2><p>Tungsten最早是由Databricks公司提出的对Spark的内存和CPU使用进行优化的计划。Tungsten使用<br>sun.misc.Unsafe的API直接操作系统内存。</p>
<p>在Tungsten中实现了一种与操作系统的内存Page非常相似的数据结构，这个对象就是MemoryBlock。</p>
<p>Task的定义：<br>Task主要包括ShuffleMapTask和ResultTask两种。每次任务尝试都会申请单独的连续内存，以执行计算。</p>
<p>Task中提供的抽象方法如下：</p>
<ol>
<li>runTask： 运行Task的接口</li>
<li>preferredLocations：获取当前Task编好的位置信息。</li>
</ol>
<p>ShuffleMapTask类似于Hadoop中的MapTask，它对输入数据计算后，将输出的数据在Shuffle之前映射到<br>不同的分区，那么下游处理各个分区的Task将知道处理哪些数据。</p>
<p>ResultTask类似于Hadoop中的ResultTask，它读取上游ShuffleMapTask输出的数据并计算得到最终的结果。</p>
<h4 id="AppendOnlyMap"><a href="#AppendOnlyMap" class="headerlink" title="AppendOnlyMap"></a>AppendOnlyMap</h4><p>Spark 提供了AppendOnlyMap来对null值进行缓存。AppendOnlyMap还是在内存中对任务执行结果进行聚合运算的利器。</p>
<p>AppendOnlyMap内置排序使用的TimSort，也就是优化版的归并排序。</p>
<p><img src="/images/2019/spark_internal_reading_notes/5d9fbd3b.png" alt=""></p>
<p><img src="/images/2019/spark_internal_reading_notes/7c8dce24.png" alt=""></p>
<h4 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h4><p>为了提升程序执行效率，Spark有时需要在map端对数据进行缓存、聚合、内置排序等操作。reduce端为了提升<br>效率，也可能需要对数据进行缓存、聚合、排序。<br>Shuffle过程中，map任务通过将多个分区的数据写入同一个文件，减轻了读写大量小文件给磁盘IO效率带来的压力。<br>reduce任务通过对存储在同一个远端节点上的Shuffle Block进行积累，批量下载远端的Block，节省了网络IO。</p>
<blockquote>
<p>想法20200203<br>学习的本质是社交。<br>最近几天一个人在家看这本书，感觉非常枯燥乏味，很难坚持下去。我认为最好还是能加入<br>一个社区和其他志同道合的人一同分享并学习。或者想办法靠近比你厉害的人。<br>但是，要找到这样的环境很难！</p>
</blockquote>
<h3 id="第九章部署环境"><a href="#第九章部署环境" class="headerlink" title="第九章部署环境"></a>第九章部署环境</h3><p>Driver：应用驱动程序，有了Driver，Application才会被提交到Spark集群运行。Driver可以选择在客户端运行，<br>也可以选择项Master注册，然后由Master命令Worker启动Driver。</p>
<p>Master：Spark的主控节点。在实际的生产环境中会有多个Master，只有一个Master处于active状态，<br>其余的Master处于slave状态</p>
<p>Worker：Spark的工作节点，向Master汇报自身的资源、Executor执行状态的改变，并接受Master的命令<br>启动Executor或Driver。</p>
<h4 id="HeartbeatReceiver"><a href="#HeartbeatReceiver" class="headerlink" title="HeartbeatReceiver"></a>HeartbeatReceiver</h4><p>HeartbeatReceiver 运行在Driver上，用以接收各个Executor的心跳（HeartBeat)消息，对各个Executor<br>的”生死”进行监控。</p>
<h4 id="Master-详解"><a href="#Master-详解" class="headerlink" title="Master 详解"></a>Master 详解</h4><p>Master负责对整个集群中所有的资源的统一管理和分配，它接收各个Worker的注册、更新状态、心跳等信息，也接收Driver<br>和Application的注册。</p>
<p>Worker向Master注册时会携带自身的身份和资源信息（ID、host、port、内核数、内存大小等），这些资源<br>将按照一定的资源调度策略分配给Driver或Application。Master给Driver分配了资源后，<br>将向Worker发送启动Driver的命令。</p>
<p>Master给Application分配了资源后，将向Worker发送启动Executor的命令，后者在接收到启动Executor的<br>命令后启动Executor。</p>
<p>启动Master有作为JVM进程内的对象启动和作为单独的进程启动的两种方式。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Tmux简明教程</title>
    <url>/2019/tmux_guide.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>你经常可能会遇到这样的情况：你在vim编辑你在远程服务器上面的代码，然后你要新开一个窗口再次ssh到<br>这个远程服务器来测试运行你的代码。此外，如果你的WIFI断线了，你的所有session都会挂掉，GG。<br>然后你又要麻烦地重新开两个session。</p>
<p>其实，上述问题都可以用tmux解决，一个可以提供如下功能的命令行工具：</p>
<ol>
<li>在一个terminal window内开启多个windows，panes。</li>
<li>在一个session内(这个session会一直保持着，即使断网的时候也会保持着)保持windows和panes。</li>
<li>能够共享session(这功能对结对编程来说真是太妙了！)，就是说，你在session上做得所有操作，<br>在另一台电脑上连接到同一个session，会看到同样的输入和输出。</li>
</ol>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>Ubuntu用户只需要执行<code>sudo apt-get install tmux</code><br>Mac 用户只需要执行<code>brew install tmux</code></p>
<p>安装完后，执行如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">tmux</div></pre></td></tr></table></figure></p>
<p><img src="/images/2019/tmux_guide/1dec3830.png" alt=""><br>这看起来和普通的终端差不多，除了底下有条绿色的状态bar。在这个bar上，我们可以运行tmux命令去控制管理终端windows和sessions</p>
<h3 id="Tmux基本命令："><a href="#Tmux基本命令：" class="headerlink" title="Tmux基本命令："></a>Tmux基本命令：</h3><p>当你进入到tmux后，你可以通过先运行一个prefix key来执行各种命令。默认来说，tmux的prefix key是 ctrl + b。也就是说在执行<br>其他任何命令之前你都要先同时按下ctrl + b。 </p>
<p>tmux的命令有很多很多，但首先我们了解下基本的就好了，如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;prefix&gt; c: 创建一个新的window（在status bar中显示）</div><div class="line">&lt;prefix&gt; 0: 切换到 window 0</div><div class="line">&lt;prefix&gt; 1: 切换到 window 1</div><div class="line">&lt;prefix&gt; 2: 切换到 window 2 (etc.)</div><div class="line">&lt;prefix&gt; x: Kill 当前的window </div><div class="line">&lt;prefix&gt; w: 切换 window</div><div class="line">&lt;prefix&gt; d: detach 到tmux（也就是说当你离开后，再次回到session中）</div></pre></td></tr></table></figure></p>
<p>如果你把在tmux session的所有的window都kill后，它将会kill整个session并且回到普通的终端上。<br>如果你用<prefix> d 去 detach到tmux，你将会回到你的tmux session，而且你可以看到你之前运行的东西已然在那里。<br>要查看所有的tmux sessions，你可以使用：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">tmux ls</div></pre></td></tr></table></figure></prefix></p>
<p>要attach到最后一个使用的session，你可以用：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">tmux a</div></pre></td></tr></table></figure></p>
<p>要attach到指定的某个session，你可以用：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">tmux a -t &lt;session-name&gt;</div></pre></td></tr></table></figure></p>
<p>这里补充说明下 session 和 window之间的关系，我们看看这个图，<br>一个window相当于你的显示器能看到的所有东西，然后一个window上可以分成一块块的拼图，也就是各个panes：<br><img src="/images/2019/tmux_guide/5a861515.png" alt=""></p>
<p>这就这么一些。现在你可以通过多个终端window和永久的sessions来在你的远程电脑上工作了。<br>你甚至可以通过让两个人attach到同一个session来进行结对编程！</p>
<h3 id="下一步"><a href="#下一步" class="headerlink" title="下一步"></a>下一步</h3><p>上面的命令对掌握tmux真的非常有用。通常，我尽量少开窗口，一般是一个window用于vim，另一个window做开发，第3个窗口去运行命令。<br>大多数人倾向于用panes来在同一个屏幕上展现多个内容。</p>
<p>Panes是个特别炫酷的东西而且很值得学习，但是推荐你们先放一放，不要急着学习太多panes的东西，因为它会增加你的tmux学习曲线。<br>当你用panes时候，你要记住水平和垂直创建panes的命令，还有切换不同panes的命令，还有改变panes大小，关闭panes的命令，等等等等。<br>所以我建议先把其它的玩熟练再去用panes这些花哨的东西。</p>
<p>你也可以通过在home目录下添加<code>.tmux.conf</code>文件来指定tmux的一些配置。<br>比如通常可以在里面改变 prefix的快捷键（很多人会使用ctrl+a 作为prefix）。<br>初始window数设置为1，而不是0;然后还可以设置下颜色。<br>如果你打算重新绑定prefix，很有可能这将会为你以后形成肌肉记忆了，哈哈。</p>
<p>下面是一个精简版的.tmux.conf, 只改变了window数和 prefix:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># remap prefix from &apos;C-b&apos; to &apos;C-a&apos;</div><div class="line">unbind C-b</div><div class="line">set-option -g prefix C-a</div><div class="line">bind-key C-a send-prefix</div><div class="line"># Start window numbering at 1</div><div class="line">set -g base-index 1</div><div class="line"></div><div class="line"># 建议加上这个，用鼠标会有很大惊喜，呵呵。在多个panes的时候切换用鼠标点一下就好了！</div><div class="line"># 同时也解决了scroll的问题，可以轻松地用鼠标在不同的panes内滚动！</div><div class="line">set -g mouse on</div></pre></td></tr></table></figure></p>
<p>tmux有非常多的可选配置，但是你得由浅到深，刚开始尽可能精简。当你想尝试一些更高级的功能时，不要在网上复制粘贴。<br>你必须清楚<code>.tmux.conf</code>的每一个配置的作用是什么。</p>
<p>一旦你感觉你掌握了基础，将会有很多好玩的高级tmux游戏等着你。</p>
]]></content>
      <categories>
        <category>tmux</category>
      </categories>
      <tags>
        <tag>tmux</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo博客SEO优化，添加robots.txt</title>
    <url>/2019/hexo_seo.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近一时兴起，想提高自己博客的点击率，就尝试做了一些SEO优化，并且加入了Google Adsense广告。呵呵，<br>写博客这么辛苦，赚点钱是应该的嘛。</p>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>废话不说了，直接进入主题。都知道要想在百度搜索，Google搜索结果页中排得靠前，你得让人家的爬虫搜你嘛，<br>所以我们得通过一个叫<code>robots.txt</code>的文件放在根目录上。这文件的目的，就是告诉搜索引擎应该搜索我这网站的那些内容。<br>我们当然希望是搜索我们文章内容本身，不要去搜那些JavaScript和CSS代码。</p>
<h3 id="配置-robots-txt"><a href="#配置-robots-txt" class="headerlink" title="配置 robots.txt"></a>配置 robots.txt</h3><p>我们在hexo 根目录下的 <code>public</code> 目录下新建一个<code>robots.txt</code>文件，内容如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">User-agent: *</div><div class="line">Allow: /</div><div class="line">Allow: /archives/</div><div class="line">Allow: /categories/</div><div class="line">Allow: /tags/</div><div class="line">Allow: /about/</div><div class="line"></div><div class="line">Disallow: /vendors/</div><div class="line">Disallow: /js/</div><div class="line">Disallow: /css/</div><div class="line">Disallow: /fonts/</div><div class="line">Disallow: /fancybox/</div><div class="line"></div><div class="line">Sitemap: https://mikolaje.github.io/sitemap.xml</div><div class="line">Sitemap: https://mikolaje.github.io/baidu_sitemap.xml</div></pre></td></tr></table></figure></p>
<p>最后面两行是site-map</p>
<p>这里要注意的是如果js和fonts这些加了disallow的话，会出现谷歌抓取问题。<br><img src="/images/2019/hexo_seo/google_spyder.png" alt=""><br>因为现在（2019-09以后）Google Search默认是用智能手机引擎来抓取，<br>所以如果js和css这样被disallow的话会有问题，建议还是把上面的disallow去掉。</p>
<h3 id="Sitemap即网站地图"><a href="#Sitemap即网站地图" class="headerlink" title="Sitemap即网站地图"></a>Sitemap即网站地图</h3><p>它的作用在于便于搜索引擎更加智能地抓取网站。<br>最简单和常见的sitemap形式，是XML文件，在其中列出网站中的网址以及关于每个网址的其他元数据（上次更新时间、更新的频率及相对其他网址重要程度等）。</p>
<p>要使用<code>sitemap</code>我们需要安装两个hexo的插件：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">npm install hexo-generator-sitemap --save</div><div class="line">npm install hexo-generator-baidu-sitemap --save</div></pre></td></tr></table></figure></p>
<p>然后，我们要在根目录下的<code>_config.yml</code> 的最后面添加如下内容：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><div class="line"><span class="attr">sitemap:</span></div><div class="line"><span class="attr">  path:</span> sitemap.xml</div><div class="line"><span class="attr">baidusitemap:</span></div><div class="line"><span class="attr">  path:</span> baidusitemap.xml</div></pre></td></tr></table></figure>
<h3 id="配置-google-analytics"><a href="#配置-google-analytics" class="headerlink" title="配置 google analytics"></a>配置 google analytics</h3><p>在theme/next/_config.yml文件下添加如下配置：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">google_analytics: UA-146421499-1</div></pre></td></tr></table></figure></p>
<p>Track ID要到你自己的GA的页面里找</p>
<h3 id="配置ads-txt"><a href="#配置ads-txt" class="headerlink" title="配置ads.txt"></a>配置ads.txt</h3><blockquote>
<p>ads.txt是干什么用的？</p>
<p>授权数字卖方 (ads.txt) 是一项 IAB 计划，可帮助确保您的数字广告资源只通过您认定为已获得授权的卖家（如 AdSense）进行销售。创建自己的 ads.txt 文件后，您可以更好地掌控允许谁在您的网站上销售广告，并可防止向广告客户展示仿冒广告资源。</p>
</blockquote>
<p>在Google Adsense找到相应的页面下载 ads.txt，然后同样放在根目录的<code>public</code>目录下面。<br><img src="/images/2019/hexo_seo/adsense.png" alt=""></p>
<h3 id="修改博文链接"><a href="#修改博文链接" class="headerlink" title="修改博文链接"></a>修改博文链接</h3><p>HEXO默认的文章链接形式为<code>domain/year/month/day/postname</code>，默认就是四级url，并且可能造成url过长，对搜索引擎是不太不友好，<br>我们可以改成domain/postname 的形式。编辑站点的_config.yml文件，<br>修改其中的permalink字段改为<code>permalink: :title.html</code>即可。</p>
<blockquote>
<p>配置完成后，重新部署hexo：<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code></p>
</blockquote>
<h2 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h2><h3 id="备用网页（有适当的规范标记）"><a href="#备用网页（有适当的规范标记）" class="headerlink" title="备用网页（有适当的规范标记）"></a>备用网页（有适当的规范标记）</h3><p>最近发现有77个页面处于这状态，为什么自己辛辛苦苦写的Blog，Google不给搜索？<br>我查了下文档：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">备用网页（有适当的规范标记）：</div><div class="line">相应网页与 Google 所识别出的规范网页重复。该网页正确地指向了这个规范网页，因此您无需执行任何操作。</div></pre></td></tr></table></figure></p>
<blockquote>
<p>什么是规范网页呢？<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">规范网址是 Google 认为在您网站上的一组重复网页中最具代表性的网页的网址。</div><div class="line">举例来说，如果同一个网页有多个网址（例如：example.com?dress=1234 和 example.com/dresses/1234），Google 便会从中选择一个网址作为规范网址。请注意，不完全相同的网页也可能被视为重复网页；通过对列表式页面的排序或过滤方式（例如，按价格排序或按服装颜色过滤）略做更改而生成的网页并不具有唯一性。</div><div class="line">规范网页所在的网域可以与相应重复网页的网域不同。</div></pre></td></tr></table></figure></p>
</blockquote>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>SEO</tag>
      </tags>
  </entry>
  <entry>
    <title>PySpark 会比Scala或Java慢吗（译）？</title>
    <url>/2019/pyspark_slower.html</url>
    <content><![CDATA[<p>首先，你必须知道不同类型的API（RDD API，MLlib 等），有它们不同的性能考虑。</p>
<h2 id="RDD-API"><a href="#RDD-API" class="headerlink" title="RDD API"></a>RDD API</h2><p>（带JVM编排的Python结构）</p>
<p>这是一个会被Python代码性能和PySpark实施影响最大的组件。虽然Python性能很可能不会是个问题，至少有几个因素你要考虑下：</p>
<ul>
<li><p>JVM 通信的额外开销。所有进出Python executor的数据必须通过一个socket和一个JVM worker. 尽管这过程相当高效，以为走的都是本地通信，<br>但多少依然还是要付出点代价。</p>
</li>
<li><p>基于进程的Python executor 对比基于线程（单JVM多线程）的 Scala executors。每个Python executor在它独自的进程里运行。<br>它的副作用是，虽然它有着比JVM更强的隔离性，并且对executor生命周期的一些控制也比JVM更强，但是潜在地会比JVM的executor消耗更多的内存。<br>比如：</p>
<ul>
<li>解析器内存footprint</li>
<li>加载模块的footprint</li>
<li>更低效的广播（因为每个进程需要独自的广播复制）</li>
</ul>
</li>
<li><p>Python本身的性能。总的来说Scala会比Python更快，但不同的task有有所不同。此外，你有其它的选项包括JITs<br>比如Numba，C扩展Cython或者其它专业的lib比如Theano。最后，可以考虑用PyPy作为解析器。</p>
</li>
<li><p>PySpark configuration提供<code>spark.python.worker.reuse</code>参数， 这可以用来对每个task在 forking Python进程和复用已有的进程中作出选择。<br>后者似乎在避免昂贵的垃圾回收方面上更有用（这更多的是一个印象而不是系统测试的结果）</p>
</li>
<li><p>在CPython里首选的垃圾回收方法，引用计数法，和典型的Spark 作业（比如流式处理，没有引用循环）结合得挺好，并且减少了长时间垃圾回收等待的风险。</p>
</li>
</ul>
<h2 id="MLlib"><a href="#MLlib" class="headerlink" title="MLlib"></a>MLlib</h2><p>（结合Python和JVM执行）</p>
<p>基本上要考虑的和前面说的那些差不多，这里再补充一些。尽管MLlib所用的基础架构是Python RDD，所有的算法都是直接用Scala来执行的。这意味着需要额外的开销来将Python 对象转为Scala对象，<br>增长的内存使用率和一些其它的限制我们将来再说。</p>
<p>现在的Spark2.x，基于RDD的API是以一个维护模式存在，Spark3.0计划会移除RDD API。</p>
<h2 id="DataFrame-API-和-Spark-ML"><a href="#DataFrame-API-和-Spark-ML" class="headerlink" title="DataFrame API 和 Spark ML"></a>DataFrame API 和 Spark ML</h2><p>（限制在driver的用Python代码的JVM执行）<br>这些可能是对标准数据处理task最好的选择。因为Python代码在driver端大多被限制在高层次的逻辑操作，在这方面上Scala和Python基本上没有什么区别。<br>有个例外是，按行的Python UDF相对来说会比Scala慢很多。尽管有很多改进的机会（在Spark2.0有着大量的改进），最大的限制还是JVM和Python解析器之间数据传送。</p>
<p>尽量习惯于用Spark内置的一些函数比如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">from pyspark.sql.functions import col, lower, trim</div><div class="line"></div><div class="line">exprs = [</div><div class="line">    lower(trim(col(c))).alias(c) if t == &quot;string&quot; else col(c) </div><div class="line">    for (c, t) in df.dtypes</div><div class="line">]</div><div class="line"></div><div class="line">df.select(*exprs)</div></pre></td></tr></table></figure></p>
<p>应该用Spark的lower而不是Python String的lower，这样做有几个好处：</p>
<ul>
<li>这操作直接将数据到JVM而不用到Python解析器</li>
<li>只需要投影一次，而不用对字段的每个字符串进行投影</li>
</ul>
<p>对了，避免在DataFrame和RDD之间的转换，因为这需要耗费很大的序列化和反序列化工作，更别说JVM和Python之间的数据传输了。</p>
<p>值得注意的是，调用Py4J会有非常高的延迟。这包括这样的调用：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">from pyspark.sql.functions import col</div><div class="line"></div><div class="line">col(&quot;foo&quot;)</div></pre></td></tr></table></figure></p>
<p>通常，这不应该是个问题（overhead是固定的，不取决于数据量，但假如是实时程序，你可能考虑对 Java wrapper进行 缓存/复用 。</p>
<h2 id="GraphX-和-Spark-DataSets"><a href="#GraphX-和-Spark-DataSets" class="headerlink" title="GraphX 和 Spark DataSets"></a>GraphX 和 Spark DataSets</h2><p>对于 Spark 1.6 和 2.1，GraphX和Spark DataSets都不提供Python接口，所以你可以说PySpark比Scala差多了。</p>
<h3 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h3><p>实践里，GraphX开发几乎完全停滞了，项目目前在维护模式，在JIRA上一些tickets都已经关掉了，不再fix。GraphFrames库提供了Python结合，你可以选它作为一个 graph处理的办法。</p>
<h3 id="DataSets"><a href="#DataSets" class="headerlink" title="DataSets"></a>DataSets</h3><p>主观来说，Python在统计类型的DataSets没有什么空间，即使现有的Scala实施过于简单，并且不提供和DataFrame一样的性能优势。</p>
<h2 id="Streaming"><a href="#Streaming" class="headerlink" title="Streaming"></a>Streaming</h2><p>从我之前说来看，我都会强烈推荐Scala，而不是Python。未来如果PySpark在structured streams上得到支持的话，可能会改变，但是现在来说，还是为时过早。再者，基于RDD的API<br>在Databricks文档里（ 2017-03-03）已经被定为“streaming遗产”，所以，可以期许下在未来进行统一。</p>
<h2 id="非性能考虑"><a href="#非性能考虑" class="headerlink" title="非性能考虑"></a>非性能考虑</h2><h3 id="功能平等"><a href="#功能平等" class="headerlink" title="功能平等"></a>功能平等</h3><p>不是所有的Spark特性、功能在PySpark上都有。需要确保下你需要的那部分已经实现了，并且尝试了解可能的限制。</p>
<p>有点特别重要的是，当你使用MLlib，和其它类似的混合Context（比如在task里调用Java/Scala 方法)。公平来讲，一些PySpark API，比如mllib.linalg，提供比Scala更加复杂的方法。</p>
<h3 id="API设计"><a href="#API设计" class="headerlink" title="API设计"></a>API设计</h3><p>PySpark API的设计和Scala类似，并不那么Pythonic。 这意味着很容易地可以在两种语言之间切换，但同时，Python可能会变得难以理解</p>
<h3 id="架构的复杂性"><a href="#架构的复杂性" class="headerlink" title="架构的复杂性"></a>架构的复杂性</h3><p>PySpark数据处理流程相当复杂比起纯粹的JVM执行来说。PySpark程序非常难去debug或找出出错原因。此外，至少在基本对Scala和JVM总体的理解上是必须要有的。</p>
<h3 id="Spark2-0-及以后"><a href="#Spark2-0-及以后" class="headerlink" title="Spark2.0 及以后"></a>Spark2.0 及以后</h3><p>随着RDD API被冻结，正在进行迁移到DataSet API对Python用户同时带来机会和挑战。尽管高级层次部分的API用Python包装会容易很多，但更高级的直接被使用的可能性很低。</p>
<p>此外，在SQL的世界里，原生Python function依然是二等公民。但值得期待的是，在将来伴随着Apache Arrow序列化，Python的地位会提高（目前侧重仍然是数据收集，UDF序列化以及反序列化仍然是个长远的目标）。<br>对于那些Python代码依赖性很强的项目，还可以选择纯Python的框架，比如Dask或Ray等等，也挺有意思的。</p>
<h2 id="不必和其它比较"><a href="#不必和其它比较" class="headerlink" title="不必和其它比较"></a>不必和其它比较</h2><p>Spark DataFrame（SQL，DataSets）API提供了一个在PySpark程序里整合Java/Scala代码优雅的方式。<br>你可以用<code>DataFrames</code> 去输送数据给原生JVM代码，然后返回结果。<br>我已经在其它地方解释了我的看法 <a href="https://stackoverflow.com/q/31684842" target="_blank" rel="external">这里</a> ，你可以在这 <a href="https://stackoverflow.com/q/36023860/1560062" target="_blank" rel="external">https://stackoverflow.com/q/36023860/1560062</a> 找到一个Python-Scala的工作案例 。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>pyspark</tag>
      </tags>
  </entry>
  <entry>
    <title>自建家用服务器集群，打造一个私有云</title>
    <url>/2019/private_IDC.html</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="我的战神"><a href="#我的战神" class="headerlink" title="我的战神"></a>我的战神</h3><p>还记得17年的时候，那时深度学习刚火起来，幼稚的我居然打算往这个领域去研究（根本没有意识到这领域需要的数学功底要多深）。俗话说，工欲善其事，必先利其器，于是乎，<br>一股脑地跑到广州的岗顶那买了台神舟战神 超级本（GTX-1060）。当时，经常都听到东哥在提到GPU，CUDA，GTX1080提到这些关键词。哈，为什当时会选择买一台笔记本，而不是台式呢？</p>
<p>其实，当时页考虑过买台式的，但是，考虑到当时在外面租房子，台式机搬家不方便。于是就想搞一台显卡性能好的笔记本，但是，完全不知道台式机的1060和笔记本的1060完全不是一个概念，<br>虽然都要同一个型号。至于为什么要买神舟，而不是华硕，或者其它，很简单，—-因为它性价比高（其实不高，买回来不久就感觉上当了，买回来玩FIFA OL会经常卡卡的，小毛病很多，而且<br>偶尔还会蓝屏。。）</p>
<p>那时候7400左右的价格在岗顶那买回来，真正用它就用了一年的时间吧。后来，在今年5月份，我以3000的价格在闲鱼上低价卖了。<br><img src="/images/2019/private_IDC/IMG_3434.jpg" alt="Sample Image Added via Markdown"></p>
<h3 id="第一台组装台式机"><a href="#第一台组装台式机" class="headerlink" title="第一台组装台式机"></a>第一台组装台式机</h3><p>前面算是有点扯远了，不过也算是记录一下我早期的一些想法吧，正是有了曾经的各种因素，导致了我今天打算搭建私有集群的IDEA。在尝到了神舟笔记本的坑后，我意识到，其实买一台台式机的必要性。<br>台式机在散热，硬件稳定性上都明显强于笔记本，而且，性价比也挺高的。对了，对于购买台式机的一个很重要的因素是，17年到18年这段时间里，我在云服务上花了不少钱，刚开始在阿里云，<br>然后在Google Cloud上。陆陆续续大概花了有六七千块钱吧，也将近买一台机器的钱了。<br>18年年初，我跑去深圳华强北，组装了自己第一台组装机（华硕主板，16G内存，1T硬盘），不加显示器，一共5000多人民币。背回来立马装了ubuntu系统，顿时感到有了自己的服务器，那感觉就像<br>在外面租房久了，终于买了属于自己的房子，再也不要担心服务器没续费，登陆不上；再也不用担心资源不够用，要去计算云服务的成本和开支了。自己的机子想怎么扩展都行。</p>
<p>其实刚开始，由于技术原因，我并没有把它当做一台服务器在用，只是把它当成一台PC安装了一些自己熟悉的开发环境和软件。在配置了Ngrok后，基本上把它当成一台云服务器了，24小时运行着。<br>关于ngrok的配置我前面有一篇专门的文章有讲得很详细。</p>
<p>最近发现我的台式机风扇声音有点大（比起Dell T30来说），从长远来看，毕竟是要7*24 running的，考虑到耗电量和稳定性，还是服务器好啊。我打算将再购置<br>2台服务器。其实，服务器完全可以当PC来用，但PC不能当服务器来用。</p>
<h3 id="第一台真正的服务器"><a href="#第一台真正的服务器" class="headerlink" title="第一台真正的服务器"></a>第一台真正的服务器</h3><p>虽然我的台式机现在配置有32G内存，在安装了一些软件（Cloudera Manager， Elasticsearch）后，内存就不太够用了。我是做数据这一块的，平时也要用分布式的框架，像Hadoop生态圈的<br>所以组件。我非常需要有一个真正的分布式平台去练手，于是就有了我的第一台服务器————戴尔T30<br><img src="/images/2019/private_IDC/t30.jpg" alt="Sample Image Added via Markdown"></p>
<p>也是在华强北的赛格广场，3200买的。配置是E3-1225，16G DDR4，2T*2 带阵列。T30 我没理解错的话应该是戴尔服务器的入门版。我对服务器不太理解，刚入门，挺好奇的，只知道一般PC的CPU<br>是分I7，I5等等，服务器的CPU是E开头的。然后，服务器用的内存和PC用的也不一样，是不能共用的。</p>
<p>购机的时候和那些卖电脑的老板闲聊也学到了好多专业知识。别看他们穿着没有写字楼的白领干净，没有程序员工资高，<br>但他们对计算机硬件知识还是很有经验的。比如，老板跟我说，你知道磁盘是怎么运行的吗？他说并不是说高转速的磁盘就一定好，如果突然断电的话，<br>转速高的磁盘容易被划伤。并介绍了下SAS，SATA，SSD之间的一些区别。他说SAS这种算是比较老的格式了。现在基本都用SATA。</p>
<p>那天我看了两种服务器，一种是机架式服务器，另一种是踏实。机架式的风扇声比较大（虽然单路的会比双路的小一些，但比起塔式服务器会大挺多的。）</p>
<h4 id="坑人的阵列卡"><a href="#坑人的阵列卡" class="headerlink" title="坑人的阵列卡"></a>坑人的阵列卡</h4><p>作为一个服务器装机菜鸟，我从网上下好的ubuntu服务器放在U盘里，然后改好U盘启动引导，开始安装。前几步没有啥问题，但到了选择安装磁盘路径的时候，找不到任何磁盘，<br>从而无法点击下一步继续。后来在老板的指导下，我才得知，原来是因为有阵列卡，BIOS没有设置对还是啥的，所以才读不到磁盘。于是，我在老板的远程指导下成功地拆卸掉阵列卡，<br>并重新接线好两块磁盘。之后，便正常地顺利正常地安装好ubuntu server了。</p>
<p><img src="/images/2019/private_IDC/raid.jpg" alt="Sample Image Added via Markdown"></p>
<h3 id="第一次在咸鱼买的服务器"><a href="#第一次在咸鱼买的服务器" class="headerlink" title="第一次在咸鱼买的服务器"></a>第一次在咸鱼买的服务器</h3><p>在咸鱼闲逛了下，看中了一台T20，看样子跟之前的DELL T30差不多：E3-1225，4G，1T。内存，硬盘这些我都可以自己另外再买零件组装进去。所以，这台配置虽然比之前的低了点，<br>但看在T20 只卖1200（包邮）我觉得还是挺划算的。不过买来后才发现，原来T20本来就是这价格，甚至1000都能买到。我当时买来还以为自己赚大了，毕竟T30我买的是3200。<br>收到货打开发现，比我想象中差些，因为这机子里面到处都是灰尘（从中学到了一个经验：以后网购服务器，一定要让卖家拍几个机子内部的照片或视频，确保没有太多灰尘）。</p>
<p>我自己又另外花了几个小时来清理机器，主要是拿棉签一点点地把灰尘差掉。</p>
<h4 id="交换机"><a href="#交换机" class="headerlink" title="交换机"></a>交换机</h4><p>哈哈，没钱买千兆网卡，千兆交换机，只能用路由器充当了。</p>
<h3 id="机器的container–机柜"><a href="#机器的container–机柜" class="headerlink" title="机器的container–机柜"></a>机器的container–机柜</h3><p>2020年初买了个小机柜，1。2米的。机柜挺好用的，如果你设备多的话（主机，电源排插，路由器），可以都放进去。其实，机柜对我最大的好处就是防尘。我方面里的灰尘太大了，<br>几个礼拜就可以把主机风扇上的灰尘粘的满满的。<br><img src="/images/2019/private_IDC/92e826b2.png" alt=""></p>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>玩硬件组装也是个超级无底洞，各个部件都可以扩展起来，内存，硬盘，CPU，可以水平扩展，也可以横向扩展。以后准备好好赚钱升级设备吧。如果比较缺钱的话可以上淘宝或闲鱼买点二手的配件，<br>比如内存条，硬盘这些。最好是可以和个人卖家交易，因为个人的话经常可以买到一些物美价廉的二手物品。另外，V2EX网站也有专门二手转让的版块，可以去看看。</p>
<h2 id="20200225-Update"><a href="#20200225-Update" class="headerlink" title="20200225 Update"></a>20200225 Update</h2><h3 id="购买内存发现好多坑"><a href="#购买内存发现好多坑" class="headerlink" title="购买内存发现好多坑"></a>购买内存发现好多坑</h3><p>服务器用了一段时间了，想升级下内存配置。于是去咸鱼上搜服务器内存，挨家挨户地去问，这内存我的服务器可以用吗？当我说出是DELL 塔式服务器<br>的时候，很多商家就直接不搭理了，或者直接说不行。感觉很是失落，买个东西还被这么多人拒绝。</p>
<p>也是怪自己在内存这方面完全没什么经验，也不知道自己的服务器到底需要买什么样子的内存。之前我对服务器的知识范围知识在DDR 代数，还设有<br>它的频率 2133、2400等等。却不知道内存还有很多规格和知识。比如 纯ECC，RECC，非ECC；2Rx8，2Rx4；这些到底是神马意思啊。然后最主要<br>的是这些不同规则的内存之间能否共存兼容，和我的机器是否兼容。</p>
<p>这两天咸鱼上几乎搜了遍，然后在网上也搜了很多相关资料，然后在牙医的Q群上咨询，学到了好多。比如，2Rx8，2Rx4有的机器是可以混用的<br>有的不能。内存是玄学，不实际通电啥也不知道，不过不建议混用。</p>
<p>纯ECC的内存比较少，所以比较贵。一些初级服务器才会用，比如说DELL T30 T130 等等都是只能用纯ECC内存。这种内存8G的DDR4都要300块。<br>在淘宝搜关键词 纯ECC+DDR4 即可找到。ERCC的会便宜一些，但是需要比较高端的机型，比如一些E5 CPU的机器。</p>
<p>对于这些概念可以在这里学学：<a href="https://www.bilibili.com/read/cv2879006" target="_blank" rel="external">https://www.bilibili.com/read/cv2879006</a></p>
<p>ECC 的全程是 Error Checking and Correcting。ECC是一种校验，ECC的R表示register，即寄存器，所以RECC就是在ECC的基础上加了一个寄存器。<br>ECC是控制器直接读内存颗粒，REG ECC是控制器读寄存器，寄存器读颗粒。</p>
<p>2Rx8： 表示2个rank组成，每个rank八个内存颗粒</p>
<h3 id="购买的第一台交换机"><a href="#购买的第一台交换机" class="headerlink" title="购买的第一台交换机"></a>购买的第一台交换机</h3><p>在咸鱼上63块钱买了个TP-LINK 5口千兆交换机。交换机还挺便宜的。买回来完全不知道怎么接，然后我试着先把主机跟交换机相连，然后交换机和<br>路由器的LAN口相连。交换机的好处是主机之间可以直接通过交换机连接，不用通过路由器了。AP路由器我可以放到机柜外边，这样信号会更好。<br>这也是我买交换机的原因之一。<br><img src="/images/2019/private_IDC/3036ea94.png" alt=""></p>
<h3 id="购买的一台梅林路由器"><a href="#购买的一台梅林路由器" class="headerlink" title="购买的一台梅林路由器"></a>购买的一台梅林路由器</h3><p>除了买交换机外，我另外买了个华硕的二手路由器。型号是Ac66U。价格是190。 第一次买这么贵的路由器，因为这路由器新的话，大概要600元。<br>卖家直接帮我装好了梅林系统。</p>
<h3 id="配置路由器和交换机"><a href="#配置路由器和交换机" class="headerlink" title="配置路由器和交换机"></a>配置路由器和交换机</h3><p>路由器默认的网关是192.168.50.1 。我把网关改为了0.1。不知道为什么，我插上去后路由器识别不到交换机中的所有主机。但是改回50.1<br>网段后，又可以识别到里面的所有主机。想了很久感觉是DHCP的问题。可能是第一次50网段时候，DHCP把里面所有的主机都自动分配了50网段<br>的IP，但是改为0网段后，所有主机的网段都变为50网段的了，然后换到0网段的时候就出现问题的了。我猜应该是这样的。</p>
<p>解决方法就是：先把网线插到路由器上，然后通过如下方法固定主机IP:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1. sudo vim /etc/netplan/01-network-manager-all.yaml</div><div class="line">2. 改为如下配置</div><div class="line">network:</div><div class="line">  version: 2</div><div class="line">  ethernets:</div><div class="line">      enp0s31f6:</div><div class="line">          dhcp4: no</div><div class="line">          dhcp6: no</div><div class="line">          addresses: [192.168.0.101/24,]</div><div class="line">          gateway4: 192.168.0.1</div><div class="line">          nameservers:</div><div class="line">             addresses: [192.168.0.1, 114.114.114.114]</div><div class="line"></div><div class="line">  #renderer: NetworkManager</div><div class="line">3. sudo netplan apply  # 生效配置</div></pre></td></tr></table></figure></p>
<p>这样只要主机和路由器网段一致就可以识别到主机了。</p>
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title>SQLAchemy的多进程实践</title>
    <url>/2019/sqlalchemy_with_multiprocess.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近上头说我写的ETL工具从MySQL导出为CSV的速度太慢了，需要性能优化。的确，有部分数据因为在MySQL里面做了<br>分表分库，而我目前的导出实现是一个一个对小表进行导出。其实，这一步完全是可以并发多个表同时导出的。<br>理论上如果网络IO没有瓶颈的话，多个表同时从MySQL里dump可以大大提升效率。</p>
<h2 id="正题"><a href="#正题" class="headerlink" title="正题"></a>正题</h2><h3 id="查阅文档"><a href="#查阅文档" class="headerlink" title="查阅文档"></a>查阅文档</h3><p>为了实现并发地使用sqlalchemy我花了不少时间在网上找资料，也在StackOverflow寻求帮助。<br>其实刚开始我是想用threading结合SQLAlchemy来实现多线程导出的。去SQLAlchemy官网一看，没有找到相关的实现文档，<br>却找到了multiprocessing与SQLAlchemy的实践:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Using Connection Pools with Multiprocessing</div><div class="line">It’s critical that when using a connection pool, and by extension when using an Engine created via create_engine(), that the pooled connections are not shared to a forked process. TCP connections are represented as file descriptors, which usually work across process boundaries, meaning this will cause concurrent access to the file descriptor on behalf of two or more entirely independent Python interpreter states.</div><div class="line"></div><div class="line">There are two approaches to dealing with this.</div><div class="line"></div><div class="line">The first is, either create a new Engine within the child process, or upon an existing Engine, call Engine.dispose() before the child process uses any connections. This will remove all existing connections from the pool so that it makes all new ones. Below is a simple version using multiprocessing.Process, but this idea should be adapted to the style of forking in use:</div></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line">engine = create_engine(<span class="string">"..."</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_in_process</span><span class="params">()</span>:</span></div><div class="line">  engine.dispose()</div><div class="line"></div><div class="line">  <span class="keyword">with</span> engine.connect() <span class="keyword">as</span> conn:</div><div class="line">      conn.execute(<span class="string">"..."</span>)</div><div class="line"></div><div class="line">p = Process(target=run_in_process)</div></pre></td></tr></table></figure>
<h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><p>于是，我就打算用multiprocessing试试了。multiprocessing有个比较坑爹的地方就是它会用pickle来序列化一些数据，<br>因为要把数据复制到新spawn出来的进程。</p>
<h4 id="关于pickle"><a href="#关于pickle" class="headerlink" title="关于pickle"></a>关于pickle</h4><p>首先我们要了解下pickle，虽然pickle挺坑的。哪些内容可以pickle呢，官网上说：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">What can be pickled and unpickled?</div><div class="line">The following types can be pickled:</div><div class="line"></div><div class="line">None, True, and False</div><div class="line"></div><div class="line">integers, floating point numbers, complex numbers</div><div class="line"></div><div class="line">strings, bytes, bytearrays</div><div class="line"></div><div class="line">tuples, lists, sets, and dictionaries containing only picklable objects</div><div class="line"></div><div class="line">functions defined at the top level of a module (using def, not lambda)</div><div class="line"></div><div class="line">built-in functions defined at the top level of a module</div><div class="line"></div><div class="line">classes that are defined at the top level of a module</div><div class="line"></div><div class="line">instances of such classes whose __dict__ or the result of calling __getstate__() is picklable (see section Pickling Class Instances for details).</div></pre></td></tr></table></figure></p>
<p>比如, 我们举个简单的例子：<br><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> pymysql</div><div class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</div><div class="line"><span class="comment">#engine = create_engine(f'mysql+pymysql://root:ignorance@localhost:3306/', server_side_cursors=True, pool_size=20)</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Client</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment">#self.engine = create_engine(f'mysql+pymysql://root:ignorance@localhost:3306/', server_side_cursors=True, pool_size=20)</span></div><div class="line">        self.pool = Pool(<span class="number">5</span>)</div><div class="line">        self.connection = pymysql.connect(host=<span class="string">'localhost'</span>,</div><div class="line">                             user=<span class="string">'root'</span>,</div><div class="line">                             password=<span class="string">'ignorance'</span>,</div><div class="line">                             port=<span class="number">3306</span>,</div><div class="line">                             charset=<span class="string">'utf8mb4'</span>,</div><div class="line">                             cursorclass=pymysql.cursors.DictCursor)</div><div class="line">        self.engine = create_engine(f<span class="string">'mysql+pymysql://root:ignorance@localhost:3306/'</span>, server_side_cursors=<span class="keyword">True</span>, pool_size=<span class="number">20</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run_in_process</span><span class="params">(self, x, y=<span class="string">'hehe'</span>)</span>:</span></div><div class="line">        print(<span class="string">'run in process'</span>)</div><div class="line">        self.engine.dispose()</div><div class="line">        conn = self.engine.connect()</div><div class="line">        res = conn.execute(<span class="string">'select count(1) from zhihu.zhihu_answer_meta limit 10'</span>)</div><div class="line">        print(res.fetchall())</div><div class="line">        time.sleep(<span class="number">5</span>)</div><div class="line">        print(conn)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></div><div class="line">        x = <span class="string">'x'</span> </div><div class="line">        res_list = []</div><div class="line">        res = self.pool.apply_async(self.run_in_process, args=(x,), kwds=&#123;<span class="string">'y'</span>:<span class="string">'shit'</span>&#125;)</div><div class="line">        res_list.append(res)</div><div class="line">        <span class="comment">#[each.get(3) for each in res_list]</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run_pool</span><span class="params">(self)</span>:</span></div><div class="line">        self.pool.close()</div><div class="line">        self.pool.join()</div><div class="line"></div><div class="line">client = Client()</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    client.run()</div><div class="line"></div><div class="line">client.run_pool()</div></pre></td></tr></table></figure></p>
<p>这段代码我的目的是想用多个进程多个connector连接到MySQL，然后各自进程同时去查询，这样便可以实现并行处理，<br>提升效率。然而，实际上这段代码不能正常地执行，而且没有任何报错提示。这是为何呢？</p>
<p>有些时候可能会看到这样的报错:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">TypeError: can&apos;t pickle _thread._local objects</div></pre></td></tr></table></figure></p>
<p><a href="https://stackoverflow.com/questions/58022926/cant-pickle-the-sqlalchemy-engine-in-the-class" target="_blank" rel="external">https://stackoverflow.com/questions/58022926/cant-pickle-the-sqlalchemy-engine-in-the-class</a></p>
<p>下面我把上面的代码修改下：<br><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> pymysql</div><div class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Client</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.pool = Pool(<span class="number">5</span>)</div><div class="line">        self.connection = pymysql.connect(host=<span class="string">'localhost'</span>,</div><div class="line">                             user=<span class="string">'root'</span>,</div><div class="line">                             password=<span class="string">'ignorance'</span>,</div><div class="line">                             port=<span class="number">3306</span>,</div><div class="line">                             charset=<span class="string">'utf8mb4'</span>,</div><div class="line">                             cursorclass=pymysql.cursors.DictCursor)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run_in_process</span><span class="params">(self, x, y=<span class="string">'hehe'</span>)</span>:</span></div><div class="line">        <span class="comment"># 这里我们把engine改为了方法的内部变量，而不是实例变量。这样就不报错了</span></div><div class="line">        engine = create_engine(f<span class="string">'mysql+pymysql://root:ignorance@localhost:3306/'</span>, server_side_cursors=<span class="keyword">True</span>, pool_size=<span class="number">20</span>)</div><div class="line">        engine.dispose()</div><div class="line">        conn = engine.connect()</div><div class="line">        res = conn.execute(<span class="string">'select count(1) from zhihu.zhihu_answer_meta limit 10'</span>)</div><div class="line">        print(res.fetchall())</div><div class="line">        time.sleep(<span class="number">5</span>)</div><div class="line">        print(conn)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></div><div class="line">        x = <span class="string">'x'</span></div><div class="line">        res_list = []</div><div class="line">        res = self.pool.apply_async(self.run_in_process, args=(x,), kwds=&#123;<span class="string">'y'</span>:<span class="string">'shit'</span>&#125;)</div><div class="line">        res_list.append(res)</div><div class="line">        <span class="comment">#[each.get(3) for each in res_list]</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run_pool</span><span class="params">(self)</span>:</span></div><div class="line">        self.pool.close()</div><div class="line">        self.pool.join()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getstate__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># 这里我增加了这个魔术方法</span></div><div class="line">        self_dict = self.__dict__.copy()</div><div class="line">        <span class="keyword">del</span> self_dict[<span class="string">'pool'</span>]</div><div class="line">        <span class="keyword">del</span> self_dict[<span class="string">'connection'</span>]  <span class="comment"># if conenction is not deleted, it would be silent without any errors</span></div><div class="line">        <span class="keyword">return</span> self_dict</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span><span class="params">(self, state)</span>:</span></div><div class="line">        <span class="comment"># 这里我增加了这个魔术方法</span></div><div class="line">        self.__dict__.update(state)</div><div class="line"></div><div class="line"></div><div class="line">client = Client()</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    client.run()</div><div class="line"></div><div class="line">client.run_pool()</div></pre></td></tr></table></figure></p>
<h4 id="我们看到，这段代码有一些变化："><a href="#我们看到，这段代码有一些变化：" class="headerlink" title="我们看到，这段代码有一些变化："></a>我们看到，这段代码有一些变化：</h4><blockquote>
<p>增加了<strong>getstate</strong>， <strong>getstate</strong>这两个魔术方法。为什么要加呢？<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">首先，我们 self.pool.apply_async(self.run_in_process) 可以看出apply_async调用的是实例的方法，所以Python需要pickle整个Client对象，</div><div class="line">包括它的所有实例变量。在第一个代码片段中我们可以看到它的实例变量有pool, connection, engine等等。然而这些对象都是不可以被pickle的，</div><div class="line">所以代码执行的时候会有问题。所以就有了__getstate__， __getstate__ 这两个东西。</div></pre></td></tr></table></figure></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">__getstate__ 总是在对象pickle之前调用， 同时，它让你可以指定你想pickle的对象状态。然后unpickle的时候，</div><div class="line">如果__setstate__被实现了，则__setstate__(state)会被调用。如果没有被实现的话，__getstate__返回的dict将会被</div><div class="line">unpickle的实例使用。在上面例子中，__setstate__ 其实没有实际效果，不写也可以。</div></pre></td></tr></table></figure>
<h3 id="multiprocessing的一些细节"><a href="#multiprocessing的一些细节" class="headerlink" title="multiprocessing的一些细节"></a>multiprocessing的一些细节</h3><h4 id="传多个参数在target函数"><a href="#传多个参数在target函数" class="headerlink" title="传多个参数在target函数"></a>传多个参数在target函数</h4><p>有时候当我们想在apply_async 的 target函数上传指定参数的时候, 可以用kwds传进去,比如：<br><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_in_process</span><span class="params">(self, x, y=<span class="string">'hehe'</span>)</span>:</span></div><div class="line">    <span class="comment"># 这里我们把engine改为了方法的内部变量，而不是实例变量。</span></div><div class="line">    print(x)</div><div class="line">    print(y)</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></div><div class="line">    x = <span class="string">'x'</span></div><div class="line">    res = self.pool.apply_async(self.run_in_process, args=(x,), kwds=&#123;<span class="string">'y'</span>:<span class="string">'shit'</span>&#125;)</div></pre></td></tr></table></figure></p>
<h4 id="map和apply的区别"><a href="#map和apply的区别" class="headerlink" title="map和apply的区别"></a>map和apply的区别</h4><p>map执行的顺序是和参数的顺序一致的，apply_async的顺序是随机的<br>map一般用来切分参数执行在同一个方法上。而apply_async可以调用不同的方法。</p>
<p>此外,<br>map相当于 map_async().get()<br>apply相当于 apply_async().get()</p>
<h5 id="子进程报错没有提示"><a href="#子进程报错没有提示" class="headerlink" title="子进程报错没有提示"></a>子进程报错没有提示</h5><p>有个很坑的地方，有些时候逻辑明明没有执行，但又没有任何报错！<br><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment">#@staticmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">work</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"error"</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    pool = mp.Pool()</div><div class="line">    foo = Foo()</div><div class="line">    res = pool.apply_async(foo.work)</div><div class="line">    pool.close()</div><div class="line">    pool.join()</div><div class="line">    <span class="comment">#print(res.get())</span></div></pre></td></tr></table></figure></p>
<p> 比如这个，如果我不get() 一下 apply_async后的返回的话，看不到任何报错信息，解决办法就是用get()后才<br> 能得知报错的信息。</p>
<h4 id="加装饰器后报错"><a href="#加装饰器后报错" class="headerlink" title="加装饰器后报错"></a>加装饰器后报错</h4><p> 比如我们在<code>run_in_process</code>方法上了个装饰器，然后就报错了。<br> <figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeit</span><span class="params">(method)</span>:</span></div><div class="line">   <span class="function"><span class="keyword">def</span> <span class="title">timed</span><span class="params">(*args, **kw)</span>:</span></div><div class="line">       ts = time.time()</div><div class="line">       result = method(*args, **kw)</div><div class="line">       te = time.time()</div><div class="line">       <span class="keyword">if</span> <span class="string">'log_time'</span> <span class="keyword">in</span> kw: </div><div class="line">           name = kw.get(<span class="string">'log_name'</span>, method.__name__.upper())</div><div class="line">           kw[<span class="string">'log_time'</span>][name] = int((te - ts) * <span class="number">1000</span>)</div><div class="line">       <span class="keyword">else</span>:</div><div class="line">           print(<span class="string">'%r 执行时长  %2.2f s'</span> % (method.__name__, (te - ts) ))</div><div class="line">       <span class="keyword">return</span> result</div><div class="line">   <span class="keyword">return</span> timed</div><div class="line">   </div><div class="line"><span class="meta">   @timeit</span></div><div class="line">   <span class="function"><span class="keyword">def</span> <span class="title">run_in_process</span><span class="params">(self, x, y=<span class="string">'hehe'</span>)</span>:</span></div><div class="line">       <span class="comment"># 这里我们把engine改为了方法的内部变量，而不是实例变量。</span></div><div class="line">       print(x)</div><div class="line">       print(y)</div><div class="line">       </div><div class="line">   <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></div><div class="line">       x = <span class="string">'x'</span></div><div class="line">       res = self.pool.apply_async(self.run_in_process, args=(x,), kwds=&#123;<span class="string">'y'</span>:<span class="string">'shit'</span>&#125;)</div></pre></td></tr></table></figure></p>
<p>报错说:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Traceback (most recent call last):</div><div class="line">  File &quot;/data/software/miniconda3/lib/python3.7/multiprocessing/process.py&quot;, line 297, in _bootstrap</div><div class="line">    self.run()</div><div class="line">  File &quot;/data/software/miniconda3/lib/python3.7/multiprocessing/process.py&quot;, line 99, in run</div><div class="line">    self._target(*self._args, **self._kwargs)</div><div class="line">  File &quot;/data/software/miniconda3/lib/python3.7/multiprocessing/pool.py&quot;, line 110, in worker</div><div class="line">    task = get()</div><div class="line">  File &quot;/data/software/miniconda3/lib/python3.7/multiprocessing/queues.py&quot;, line 354, in get</div><div class="line">    return _ForkingPickler.loads(res)</div><div class="line">AttributeError: &apos;Client&apos; object has no attribute &apos;timed&apos;</div></pre></td></tr></table></figure></p>
<p>解决办法：<br>比较麻烦，不能用@了。可以以这种办法代替装饰器：<br><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">decorate_func</span><span class="params">(f)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_decorate_func</span><span class="params">(*args, **kwargs)</span>:</span></div><div class="line">        <span class="keyword">print</span> <span class="string">"I'm decorating"</span></div><div class="line">        <span class="keyword">return</span> f(*args, **kwargs)</div><div class="line">    <span class="keyword">return</span> _decorate_func</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">actual_func</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> x ** <span class="number">2</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">wrapped_func</span><span class="params">(*args, **kwargs)</span>:</span></div><div class="line">    <span class="keyword">return</span> decorate_func(actual_func)(*args, **kwargs)</div><div class="line"></div><div class="line">my_swimming_pool = Pool()</div><div class="line">result = my_swimming_pool.apply_async(wrapped_func,(<span class="number">2</span>,))</div><div class="line"><span class="keyword">print</span> result.get()</div></pre></td></tr></table></figure></p>
<h4 id="关于处理ctrl-c-的报错"><a href="#关于处理ctrl-c-的报错" class="headerlink" title="关于处理ctrl-c 的报错:"></a>关于处理ctrl-c 的报错:</h4><p>有时候我们用multiprocessing处理一些任务，当我们想终止任务时候，用Ctrl+C 然后会看到一堆的报错，有时候还得连续按很多CTRL+C完全终止掉。<br>下面是最佳解决方案：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">解决办法是 首先防止子进程接收KeyboardInterrupt，然后完全交给父进程catch interrupt然后清洗进程池。通过这种方法可以</div><div class="line">避免在子进程写处理异常逻辑，并且防止了由idle workers 生成的无止尽的Error。</div></pre></td></tr></table></figure></p>
<p>解决方案代码:<br><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">import</span> multiprocessing</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> signal</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_worker</span><span class="params">()</span>:</span></div><div class="line">    signal.signal(signal.SIGINT, signal.SIG_IGN)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_worker</span><span class="params">()</span>:</span></div><div class="line">    time.sleep(<span class="number">15</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"Initializng 5 workers"</span></div><div class="line">    pool = multiprocessing.Pool(<span class="number">5</span>, init_worker)</div><div class="line"></div><div class="line">    <span class="keyword">print</span> <span class="string">"Starting 3 jobs of 15 seconds each"</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">        pool.apply_async(run_worker)</div><div class="line"></div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        print(<span class="string">"Waiting 10 seconds"</span>)</div><div class="line">        time.sleep(<span class="number">10</span>)</div><div class="line"></div><div class="line">    <span class="keyword">except</span> KeyboardInterrupt:</div><div class="line">        print(<span class="string">"Caught KeyboardInterrupt, terminating workers"</span>)</div><div class="line">        pool.terminate()</div><div class="line">        pool.join()</div><div class="line"></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">print</span> <span class="string">"Quitting normally"</span></div><div class="line">        pool.close()</div><div class="line">        pool.join()</div></pre></td></tr></table></figure></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://docs.python.org/3/library/pickle.html" target="_blank" rel="external">https://docs.python.org/3/library/pickle.html</a><br><a href="https://stackoverflow.com/questions/25382455/python-notimplementederror-pool-objects-cannot-be-passed-between-processes/25385582#25385582" target="_blank" rel="external">https://stackoverflow.com/questions/25382455/python-notimplementederror-pool-objects-cannot-be-passed-between-processes/25385582#25385582</a></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive 使用orc进行事务操作(update)</title>
    <url>/2019/hive_orc.html</url>
    <content><![CDATA[<h2 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h2><p>需求方需要用Hive来进行一些update操作。以往一般用Parquet这种格式作为Hive的存储格式，查文档得知Parquet不支持<br>update，orc格式可以支持update。</p>
<h2 id="开始试验"><a href="#开始试验" class="headerlink" title="开始试验"></a>开始试验</h2><h3 id="创建测试数据"><a href="#创建测试数据" class="headerlink" title="创建测试数据"></a>创建测试数据</h3><p>首先我们在Hive上简单地创建一个表作为测试：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">create table test_format (</div><div class="line">    car_name string,</div><div class="line">    series string,</div><div class="line">    e_series string,</div><div class="line">    model string,</div><div class="line">    variant string</div><div class="line">) clustered by (car_name) into 5 buckets stored as orc TBLPROPERTIES(&apos;transactional&apos;=&apos;true&apos;);</div><div class="line"></div><div class="line">insert into test_format values</div><div class="line">(&apos;2017款宝马3系320i M运动型&apos;,&apos;3&apos;,&apos;F30&apos;,&apos;320i&apos;,&apos;320i M Sport Package&apos;),</div><div class="line">(&apos;2018款宝马3系320i M运动套装&apos;,&apos;3&apos;,&apos;F30&apos;,&apos;320i&apos;,&apos;320i M Sport Package&apos;),</div><div class="line">(&apos;2018款宝马3系320i M运动曜夜版&apos;,&apos;3&apos;,&apos;F30&apos;,&apos;320i&apos;,&apos;320i M Sport Dark Edition&apos;),</div><div class="line">(&apos;2019款宝马3系320i M 运动套装&apos;,&apos;3&apos;,&apos;F30&apos;,&apos;320i&apos;,&apos;320i M Sport Package&apos;);</div></pre></td></tr></table></figure></p>
<p>刚开始会有报错：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">This command is not allowed on an ACID table auto_projects.test_format with a non-ACID transaction manager. Failed command: insert into test_format value</div></pre></td></tr></table></figure></p>
<p>这是因为有些配置文件需要修改才能支持transaction操作。</p>
<p>在hive-site.xml里面添加如下配置：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.optimize.sort.dynamic.partition&lt;/name&gt;</div><div class="line">    &lt;value&gt;false&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.support.concurrency&lt;/name&gt;</div><div class="line">    &lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.enforce.bucketing&lt;/name&gt;</div><div class="line">    &lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.exec.dynamic.partition.mode&lt;/name&gt;</div><div class="line">    &lt;value&gt;nonstrict&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.txn.manager&lt;/name&gt;</div><div class="line">    &lt;value&gt;org.apache.hadoop.hive.ql.lockmgr.DbTxnManager&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.compactor.initiator.on&lt;/name&gt;</div><div class="line">    &lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.compactor.worker.threads&lt;/name&gt;</div><div class="line">    &lt;value&gt;1&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.in.test&lt;/name&gt;</div><div class="line">    &lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure></p>
<p>Cloudera Manager的话则可以在WebUI上完成。</p>
<ol>
<li>在WebUI上先点击Hive集群。</li>
<li>点击配置，然后找到一个叫hive-site.xml 的 HiveServer2 高级配置代码段（安全阀）的tag中</li>
<li>将上述配置内容复制在文本框内。</li>
<li>重启集群</li>
</ol>
<h3 id="测试更新数据"><a href="#测试更新数据" class="headerlink" title="测试更新数据"></a>测试更新数据</h3><p>测试更新和删除部分数据<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">update test_format set model=&apos;test&apos; where series=&apos;3&apos;;</div><div class="line"></div><div class="line">delete  from test_format where model=&apos;test&apos;;</div></pre></td></tr></table></figure></p>
<p>这里需要注意的一个地方就是 要更新的字段不能是设置为bucket的那个字段，不然会报错：</p>
<p>成功执行！</p>
<h3 id="踩的一些坑"><a href="#踩的一些坑" class="headerlink" title="踩的一些坑"></a>踩的一些坑</h3><p>因为我是用的单节点部署的CDH集群。刚开始，在Hive上执行select count(*) ; 和 insert操作时候，没有任何报错，<br>但会一直卡在那里。后来查看到cloudera community说单节点要改个mapred-site.xml的一个参数：<br>mapreduce.framework.name</p>
<p>默认是用的yarn，单机的话要改为local，然后重启集群insert 和 select count(*) 就不会卡住了,<br>CDH也是在WebUI上的Yarn集群上的配置上修改。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.cnblogs.com/qifengle-2446/p/6424620.html" target="_blank" rel="external">https://www.cnblogs.com/qifengle-2446/p/6424620.html</a><br><a href="https://community.cloudera.com/t5/Batch-SQL-Apache-Hive/Hive-server-query-hanging-when-issue-ing-select-count-on-CLI/m-p/67119#M2662" target="_blank" rel="external">https://community.cloudera.com/t5/Batch-SQL-Apache-Hive/Hive-server-query-hanging-when-issue-ing-select-count-on-CLI/m-p/67119#M2662</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>经典的大数据面试题</title>
    <url>/2019/typical_bigdata_interview_question.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近面试大数据工程师岗位，同一个问题被连续问了两次。题目大概是这样的：<br>如果你有一台机器，内存是有限的，要你统计一个很大的日志文件里的数据，比如统计UV top-N；<br>另外一个公司是这么问：<br>如果你有一台机器，内存有限，要你对某个指标做一个全局排序。比如对单词频次进行全局排序。</p>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>第一种问题是比较简单的。这种问题一般都问在某个条件下如何在大数据中求出top-N。这种问题的解决方案如下步骤：</p>
<ol>
<li>分而治之/hash映射。</li>
<li>hash统计。</li>
<li>堆/快速/归并排序；</li>
</ol>
<p>首先，为什么药hash映射呢？<br>因为，我们的目的要将一个大文件切割成N个小文件，去进行分而治之。而hash 加 取模 这种方式可以帮我们把数据<br>均匀随机地分布在N个地方。更重要的是，hash可以让我们把相同的东西放在同一个地方！</p>
<p>其实这种做法在有些地方出现过，比如Hive的bucket实现原理就是这个中道理。用hash加取模的方式保证同样的值被<br>分配到同一个地方。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://blog.csdn.net/v_july_v/article/details/7382693" target="_blank" rel="external">https://blog.csdn.net/v_july_v/article/details/7382693</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive bucket和partition的区别</title>
    <url>/2019/hive_bucket_partition.html</url>
    <content><![CDATA[<h2 id="Hive-partition和bucket的区别"><a href="#Hive-partition和bucket的区别" class="headerlink" title="Hive partition和bucket的区别"></a>Hive partition和bucket的区别</h2><ul>
<li>翻译文</li>
</ul>
<p>为了更好地阐述partition和bucket的区别，我们先看看数据是怎么保存在Hive上面的。比如，你有一个表：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">CREATE TABLE mytable ( </div><div class="line">         name string,</div><div class="line">         city string,</div><div class="line">         employee_id int ) </div><div class="line">PARTITIONED BY (year STRING, month STRING, day STRING) </div><div class="line">CLUSTERED BY (employee_id) INTO 256 BUCKETS</div></pre></td></tr></table></figure></p>
<p>然后，Hive会将数据保存为如下的层级：<br>/user/hive/warehouse/mytable/y=2015/m=12/d=02<br>所以，用partition的时候必须小心，因为如果你用employee_id来partition的话，如果有上百万个employee，那么你会看到有上百万个目录被创建在你的系统上。<br>“cardinality” 这个术语被用来表示不同字段值的数量。比如，你有country这个字段，世界上会有300个国家，所以这里的”cardinality”是300这样。<br>对于像’timestamp_ms’的字段，它的’cardinality’会有几十亿。<br>总的来说，当我们用partition的时候，不要用在cardinality很高的字段上。因为它会导致生成太多的目录。</p>
<p>说说bucket了，在指定了bucket数后，会使得文件的数量固定。Hive会做的是计算字段的hash值，然后分发一个记录给那个bucket。<br>但是比如说你用总共256个bucket在一个较低的cardinality的字段上会发生什么呢？（比如，美国的州，只有50个）<br>只有50个bucket有数据，其它206个bucket是没有数据的。</p>
<p>有人已经提到partition可以极大地减少查询数据的量。<br>那在我这个例子中，如果你想在某个日期上进一步查询，对 yearn/month/day的partition会大大地减少IO。<br>我觉得有人已经提到bucket可以加速和其它恰好在同一个bucket的表的join操作。那么在我的案例中，如果你正在通过employee_id来join两个表，<br>Hive能够在每个每个bucket地内部进行join（如果它们已经通过employee_id排序好的话，效果会更好！）</p>
<p>总结，<br>bucket在字段有很高的cardinality，和在数据在不同bucket中均匀地分布的时候，会表现出优越性。<br>partition在cardinality partition的字段不是很多的时候，会表现出优越性。</p>
<p>另外，你可以按顺序同时partiton多个字段，比如（yearn/month/day），但bucket只能取一个字段。</p>
<ul>
<li>原文<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">There are a few details missing from the previous explanations. To better understand how partitioning and bucketing works,</div><div class="line"> you should look at how data is stored in hive. Let&apos;s say you have a table</div><div class="line"></div><div class="line">CREATE TABLE mytable ( </div><div class="line">         name string,</div><div class="line">         city string,</div><div class="line">         employee_id int ) </div><div class="line">PARTITIONED BY (year STRING, month STRING, day STRING) </div><div class="line">CLUSTERED BY (employee_id) INTO 256 BUCKETS</div><div class="line">then hive will store data in a directory hierarchy like</div><div class="line"></div><div class="line">/user/hive/warehouse/mytable/y=2015/m=12/d=02</div><div class="line">So, you have to be careful when partitioning, because if you for instance partition by employee_id and you have millions of employees, </div><div class="line">you&apos;ll end up having millions of directories in your file system. The term &apos;cardinality&apos; refers to the number of possible value a field can have. </div><div class="line">For instance, if you have a &apos;country&apos; field, the countries in the world are about 300, so cardinality would be ~300. </div><div class="line">For a field like &apos;timestamp_ms&apos;, which changes every millisecond, cardinality can be billions. </div><div class="line">In general, when choosing a field for partitioning, it should not have a high cardinality, because you&apos;ll end up with way too many directories in your file system.</div><div class="line"></div><div class="line">Clustering aka bucketing on the other hand, will result with a fixed number of files, since you do specify the number of buckets. </div><div class="line">What hive will do is to take the field, calculate a hash and assign a record to that bucket. </div><div class="line">But what happens if you use let&apos;s say 256 buckets and the field you&apos;re bucketing on has a low cardinality (for instance, it&apos;s a US state, so can be only 50 different values) ? </div><div class="line">You&apos;ll have 50 buckets with data, </div><div class="line">and 206 buckets with no data.</div><div class="line"></div><div class="line">Someone already mentioned how partitions can dramatically cut the amount of data you&apos;re querying. </div><div class="line">So in my example table, if you want to query only from a certain date forward, the partitioning by year/month/day is going to dramatically cut the amount of IO. </div><div class="line">I think that somebody also mentioned how bucketing can speed up joins with other tables that have exactly the same bucketing, </div><div class="line">so in my example, if you&apos;re joining two tables on the same employee_id, </div><div class="line">hive can do the join bucket by bucket (even better if they&apos;re already sorted by employee_id since it&apos;s going to mergesort parts that are already sorted, </div><div class="line">which works in linear time aka O(n) ).</div><div class="line"></div><div class="line">So, bucketing works well when the field has high cardinality and data is evenly distributed among buckets. </div><div class="line">Partitioning works best when the cardinality of the partitioning field is not too high.</div><div class="line"></div><div class="line">Also, you can partition on multiple fields, with an order (year/month/day is a good example), while you can bucket on only one field.</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://stackoverflow.com/questions/19128940/what-is-the-difference-between-partitioning-and-bucketing-a-table-in-hive" target="_blank" rel="external">https://stackoverflow.com/questions/19128940/what-is-the-difference-between-partitioning-and-bucketing-a-table-in-hive</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark什么时候用 persist</title>
    <url>/2019/spark_when_to_cache.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在用Spark做一些数据统计，有个任务要跑几个小时，所以需要优化一下。首先想到的是用 persist或者cache(persist的其中一种方式)</p>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="场景一"><a href="#场景一" class="headerlink" title="场景一"></a>场景一</h3><h4 id="首先看看在Stackoverflow的一个回答"><a href="#首先看看在Stackoverflow的一个回答" class="headerlink" title="首先看看在Stackoverflow的一个回答"></a>首先看看在Stackoverflow的一个回答</h4><p>Spark很多惰性算子，它并不会立即执行，persist就是惰性的。只有当被action trigger的时候，叫 lineage的RDD 链才会被执行。<br>如果是线性的lineage的话，persist是没用的。但如果RDD的lineage被分流出去的话，那么persist就有用了。</p>
<p>举个例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># 比如这种情况，有个两个action算子count；</div><div class="line">val positiveWordsCount = wordsRDD.filter(word =&gt; isPositive(word)).count()</div><div class="line">val negativeWordsCount = wordsRDD.filter(word =&gt; isNegative(word)).count()</div><div class="line"></div><div class="line"></div><div class="line"># 可以在公用的RDD上加个cache，让这个flatMap只计算一次</div><div class="line">val textFile = sc.textFile(&quot;/user/emp.txt&quot;)</div><div class="line">val wordsRDD = textFile.flatMap(line =&gt; line.split(&quot;\\W&quot;))</div><div class="line">wordsRDD.cache()</div><div class="line">val positiveWordsCount = wordsRDD.filter(word =&gt; isPositive(word)).count()</div><div class="line">val negativeWordsCount = wordsRDD.filter(word =&gt; isNegative(word)).count()</div></pre></td></tr></table></figure>
<p>另外一个案例：<br><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_url</span><span class="params">(value)</span>:</span></div><div class="line">    url = <span class="string">'http://domain.com/'</span> + str(value)</div><div class="line">    <span class="keyword">if</span> value == <span class="string">'1134021255504498689'</span>:</div><div class="line">        print(value)  <span class="comment"># 这里print出来可以作为一个标记，知道 这个udf执行了几次</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> url </div><div class="line">    </div><div class="line">url_udf = udf(make_url, StringType())</div><div class="line">df = spark.read.csv(<span class="string">'file:///Users/xiaofeilong/Documents/twitter_tweet_keyword.csv'</span>)</div><div class="line"></div><div class="line">df = df.withColumn(<span class="string">'url'</span>, url_udf(<span class="string">'_c0'</span>))</div><div class="line"><span class="comment"># df.cache() </span></div><div class="line">df1 = df.filter(df._c1.isin([<span class="string">'petcare'</span>]))</div><div class="line">df2 = df.filter(df._c1.isin([<span class="string">'hound'</span>]))</div><div class="line"></div><div class="line">df_union = df1.union(df2)</div><div class="line">df_union = df_union.orderBy(<span class="string">'url'</span>)  <span class="comment"># 这里加order by url的原因是，要让它执行url_udf。不然如果这个字段没用上的话，Spark并不会执行url_udf</span></div><div class="line">print(df_union.count())</div></pre></td></tr></table></figure></p>
<p>上述例子中，如果不加cache的话。log中可以看到打印了两次 1134021255504498689。而且会看到有两条这样的信息：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">INFO FileScanRDD: Reading File path: file:///Users/xiaofeilong/Documents/twitter_tweet_keyword.csv, range: 0-2315578, partition values: [empty row]</div><div class="line">.</div><div class="line">.</div><div class="line">.</div><div class="line">FileScanRDD: Reading File path: file:///Users/xiaofeilong/Documents/twitter_tweet_keyword.csv, range: 0-2315578, partition values: [empty row]</div></pre></td></tr></table></figure></p>
<h4 id="测试结论！"><a href="#测试结论！" class="headerlink" title="测试结论！"></a>测试结论！</h4><ul>
<li><p>说明csv文件被load了两次！！！</p>
</li>
<li><p>如果用了cache的话，只会出现一次！！！</p>
</li>
</ul>
<h3 id="场景二"><a href="#场景二" class="headerlink" title="场景二"></a>场景二</h3><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://stackoverflow.com/questions/28981359/why-do-we-need-to-call-cache-or-persist-on-a-rdd" target="_blank" rel="external">https://stackoverflow.com/questions/28981359/why-do-we-need-to-call-cache-or-persist-on-a-rdd</a><br><a href="https://blog.csdn.net/ainidong2005/article/details/53152605" target="_blank" rel="external">https://blog.csdn.net/ainidong2005/article/details/53152605</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Git删除误add的文件</title>
    <url>/2019/git_filter_branch.html</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h4 id="很多时候，我们经常会不小心把一个不应该add进来的文件-add到-Repo里了。比如："><a href="#很多时候，我们经常会不小心把一个不应该add进来的文件-add到-Repo里了。比如：" class="headerlink" title="很多时候，我们经常会不小心把一个不应该add进来的文件 add到 Repo里了。比如："></a>很多时候，我们经常会不小心把一个不应该add进来的文件 add到 Repo里了。比如：</h4><p>password.txt，<em>.log  </em>.mp4 等等。</p>
<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><h4 id="这种情况下，我们可以选择用-git-filter-branch-来将历史的所有commit都重新过滤一下。"><a href="#这种情况下，我们可以选择用-git-filter-branch-来将历史的所有commit都重新过滤一下。" class="headerlink" title="这种情况下，我们可以选择用 git filter-branch 来将历史的所有commit都重新过滤一下。"></a>这种情况下，我们可以选择用 git filter-branch 来将历史的所有commit都重新过滤一下。</h4><h4 id="查看Git的文档得知，filter-branch是一个核弹级操作"><a href="#查看Git的文档得知，filter-branch是一个核弹级操作" class="headerlink" title="查看Git的文档得知，filter-branch是一个核弹级操作:"></a>查看Git的文档得知，filter-branch是一个核弹级操作:</h4><blockquote>
<p>如果你想用脚本的方式修改大量的提交，还有一个重写历史的选项可以用——例如，全局性地修改电子邮件地址或者将一个文件从所有提交中删除。<br>这个命令是filter-branch，这个会大面积地修改你的历史，所以你很有可能不该去用它，除非你的项目尚未公开，没有其他人在你准备修改的提交的基础上工作。<br>尽管如此，这个可以非常有用。你会学习一些常见用法，借此对它的能力有所认识。<br><br>从所有提交中删除一个文件<br>这个经常发生。有些人不经思考使用git add .，意外地提交了一个巨大的二进制文件，你想将它从所有地方删除。<br>也许你不小心提交了一个包含密码的文件，而你想让你的项目开源。filter-branch大概会是你用来清理整个历史的工具。<br>要从整个历史中删除一个名叫password.txt的文件，你可以在filter-branch上使用–tree-filter选项</p>
</blockquote>
<h4 id="Example：把历史中不小心添加进来的所有mp4文件清除。"><a href="#Example：把历史中不小心添加进来的所有mp4文件清除。" class="headerlink" title="Example：把历史中不小心添加进来的所有mp4文件清除。"></a>Example：把历史中不小心添加进来的所有mp4文件清除。</h4><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"></div><div class="line">删除所有后缀为mp4 的历史提交文件</div><div class="line"></div><div class="line">git filter-branch --index-filter &apos;git rm -r --cached --ignore-unmatch *.mp4&apos; --prune-empty -f</div><div class="line">git for-each-ref --format=&apos;delete %(refname)&apos; refs/original | git update-ref --stdin </div><div class="line">git reflog expire --expire=now --all &amp;&amp; git gc --aggressive --prune=now </div><div class="line"></div><div class="line">如果要push到remote Repo的话，需要加-f 强推</div></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Celery让某个task一个一个地执行</title>
    <url>/2019/celery_lock_task.html</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>最近有个需求是这样子的，某个task要求一定要一个一个地执行，不能并发执行。<br>比较简单的办法是直接将 celery worker启动为 一个进程: “-c 1”。 但是，这种方法会导致其它的task也只能单进程了。</p>
<p>后来通过Google，查找了很多例子，最普遍的一个做法是参考官方文档的做法， 代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">from</span> celery <span class="keyword">import</span> task</div><div class="line"><span class="keyword">from</span> celery.five <span class="keyword">import</span> monotonic</div><div class="line"><span class="keyword">from</span> celery.utils.log <span class="keyword">import</span> get_task_logger</div><div class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> contextmanager</div><div class="line"><span class="keyword">from</span> django.core.cache <span class="keyword">import</span> cache</div><div class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</div><div class="line"><span class="keyword">from</span> djangofeeds.models <span class="keyword">import</span> Feed</div><div class="line"></div><div class="line">logger = get_task_logger(__name__)</div><div class="line"></div><div class="line">LOCK_EXPIRE = <span class="number">60</span> * <span class="number">10</span>  <span class="comment"># Lock expires in 10 minutes</span></div><div class="line"></div><div class="line"><span class="meta">@contextmanager</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">memcache_lock</span><span class="params">(lock_id, oid)</span>:</span></div><div class="line">    timeout_at = monotonic() + LOCK_EXPIRE - <span class="number">3</span></div><div class="line">    <span class="comment"># cache.add fails if the key already exists</span></div><div class="line">    status = cache.add(lock_id, oid, LOCK_EXPIRE)  </div><div class="line">    <span class="comment"># 如果存在lock_id的话会返回False，不存在的话会返回True。这个也可以换成用Redis实现，比如用 setnx</span></div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        <span class="keyword">yield</span> status</div><div class="line">    <span class="keyword">finally</span>:</div><div class="line">        <span class="comment"># memcache delete is very slow, but we have to use it to take</span></div><div class="line">        <span class="comment"># advantage of using add() for atomic locking</span></div><div class="line">        <span class="keyword">if</span> monotonic() &lt; timeout_at <span class="keyword">and</span> status:</div><div class="line">            <span class="comment"># don't release the lock if we exceeded the timeout</span></div><div class="line">            <span class="comment"># to lessen the chance of releasing an expired lock</span></div><div class="line">            <span class="comment"># owned by someone else</span></div><div class="line">            <span class="comment"># also don't release the lock if we didn't acquire it</span></div><div class="line">            cache.delete(lock_id)</div><div class="line"></div><div class="line"><span class="meta">@task(bind=True)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">import_feed</span><span class="params">(self, feed_url)</span>:</span></div><div class="line">    <span class="comment"># The cache key consists of the task name and the MD5 digest</span></div><div class="line">    <span class="comment"># of the feed URL.</span></div><div class="line">    feed_url_hexdigest = md5(feed_url).hexdigest()</div><div class="line">    lock_id = <span class="string">'&#123;0&#125;-lock-&#123;1&#125;'</span>.format(self.name, feed_url_hexdigest)</div><div class="line">    logger.debug(<span class="string">'Importing feed: %s'</span>, feed_url)</div><div class="line">    <span class="keyword">with</span> memcache_lock(lock_id, self.app.oid) <span class="keyword">as</span> acquired:</div><div class="line">        <span class="keyword">if</span> acquired:</div><div class="line">            <span class="keyword">return</span> Feed.objects.import_feed(feed_url).url</div><div class="line">    logger.debug(</div><div class="line">        <span class="string">'Feed %s is already being imported by another worker'</span>, feed_url)</div></pre></td></tr></table></figure>
<p>但是，上面的逻辑只是在有task正执行的时候忽略了新增task。比如说有个import_feed task 正在运行，还没有运行完，<br>再调用apply_async的时候就会不做任何操作。</p>
<p>所以得在上面代码的基础上改一改。</p>
<h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><p>这里我用了一个上厕所的例子。假设有一个公共厕所。如果有人在用着这个厕所的时候其他人就不能使用了，得在旁边排队等候。<br>一次只能进去一个人。废话少说直接上代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"></div><div class="line"><span class="meta">@app.task(bind=True, base=ShitTask, max_retries=10)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">shit_task</span><span class="params">(self, toilet_id)</span>:</span></div><div class="line">    lock_id = <span class="string">'&#123;0&#125;-lock'</span>.format(toilet_id)</div><div class="line">    shit_queue = <span class="string">'&#123;0&#125;-queue'</span>.format(self.name)</div><div class="line">    <span class="comment"># 这里我选用了Redis来处理队列</span></div><div class="line">    <span class="keyword">with</span> memcache_lock(lock_id, self.app.oid) <span class="keyword">as</span> acquired:</div><div class="line">        <span class="keyword">if</span> acquired:</div><div class="line">            <span class="comment"># 当进入到厕所的时候，先把门锁上（其他人就进不来了，然后再拉</span></div><div class="line">            print(<span class="string">'Oh yes, Lock the door and it is my time to shit. '</span>)</div><div class="line">            time.sleep(<span class="number">5</span>)</div><div class="line">            <span class="keyword">return</span> <span class="string">'I finished shit'</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># 有人在用厕所，得在外面等着 </span></div><div class="line">            print(<span class="string">'Oops, somebody engaged the toilet, I have to queue up'</span>)</div><div class="line">            rdb.lpush(shit_queue, json.dumps(list(self.request.args)))</div><div class="line">            <span class="comment"># 将其他人放到Redis里排队等候</span></div><div class="line">            <span class="keyword">raise</span> Ignore()</div></pre></td></tr></table></figure>
<p>上面代码是主要的task处理所及。另外，要先重写一下Task类的 after_return 方法，使得当没能执行的task（在门口排队的人）<br>在正在执行task（正在用厕所的人）成功执行完后，接着执行下一个task（下个人接着用厕所）。</p>
<p>完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment"># coding=u8</span></div><div class="line"><span class="keyword">from</span> celery <span class="keyword">import</span> Celery, Task</div><div class="line"><span class="keyword">from</span> celery.exceptions <span class="keyword">import</span> Ignore</div><div class="line"><span class="keyword">from</span> celery <span class="keyword">import</span> task</div><div class="line"><span class="keyword">import</span> redis</div><div class="line"><span class="keyword">import</span> pickle</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">from</span> celery.five <span class="keyword">import</span> monotonic</div><div class="line"><span class="keyword">from</span> celery.utils.log <span class="keyword">import</span> get_task_logger</div><div class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> contextmanager</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line">app = Celery(<span class="string">'tasks'</span>, broker=<span class="string">'redis://localhost:6379/10'</span>)</div><div class="line"></div><div class="line">logger = get_task_logger(__name__)</div><div class="line"></div><div class="line">LOCK_EXPIRE = <span class="number">60</span> * <span class="number">10</span>  <span class="comment"># Lock expires in 10 minutes</span></div><div class="line"></div><div class="line">rdb = redis.Redis(db=<span class="number">11</span>)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShitTask</span><span class="params">(Task)</span>:</span></div><div class="line">    abstract = <span class="keyword">True</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">after_return</span><span class="params">(self, status, retval, task_id, args, kwargs, einfo)</span>:</span></div><div class="line">        print(status)</div><div class="line">        <span class="keyword">if</span> retval:</div><div class="line">            <span class="comment"># 这个retval的内容就是task return过来的内容</span></div><div class="line">            print(<span class="string">'somebody finished shit, calling the next one to shit'</span>)</div><div class="line">            shit_queue = <span class="string">'&#123;0&#125;-queue'</span>.format(self.name)</div><div class="line">            task_args = rdb.rpop(shit_queue)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> task_args:</div><div class="line">                task_args = json.loads(task_args)</div><div class="line">                self.delay(*task_args)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="meta">@contextmanager</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">memcache_lock</span><span class="params">(lock_id, oid)</span>:</span></div><div class="line">    timeout_at = monotonic() + LOCK_EXPIRE - <span class="number">3</span></div><div class="line">    <span class="comment"># status = cache.add(lock_id, oid, LOCK_EXPIRE)  # 如果lock_id 存在则返回False，如果不存在则返回True</span></div><div class="line">    status = rdb.setnx(lock_id, oid)</div><div class="line">    rdb.expire(lock_id, LOCK_EXPIRE)</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        <span class="keyword">yield</span> status</div><div class="line">    <span class="keyword">finally</span>:</div><div class="line">        <span class="keyword">if</span> monotonic() &lt; timeout_at <span class="keyword">and</span> status:</div><div class="line">            <span class="comment"># 设置一个时间限制，一个人不能占用厕所太久，而且只有占用厕所的那人才能开锁把厕所门打开</span></div><div class="line">            print(<span class="string">'release the lock and open the door of the toilet %s'</span> % lock_id)</div><div class="line">            rdb.delete(lock_id)</div><div class="line"></div><div class="line"></div><div class="line"><span class="meta">@app.task(bind=True, base=ShitTask, max_retries=10)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">shit_task</span><span class="params">(self)</span>:</span></div><div class="line">    print(<span class="string">'task name %s'</span> % self.name)</div><div class="line">    </div><div class="line">    lock_id = <span class="string">'&#123;0&#125;-lock'</span>.format(self.name)</div><div class="line">    shit_queue = <span class="string">'&#123;0&#125;-queue'</span>.format(self.name)</div><div class="line">    print(lock_id)</div><div class="line">    <span class="keyword">with</span> memcache_lock(lock_id, self.app.oid) <span class="keyword">as</span> acquired:</div><div class="line">        print(<span class="string">'acquired'</span>, acquired)</div><div class="line">        <span class="keyword">if</span> acquired:</div><div class="line">            print(<span class="string">'Oh yes, Lock the door and it is my time to shit. '</span>)</div><div class="line">            time.sleep(<span class="number">5</span>)</div><div class="line">            <span class="keyword">return</span> <span class="string">'I finished shit'</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            print(<span class="string">'Oops, somebody engaged the toilet, I have to queue up'</span>)</div><div class="line">            <span class="comment">#pending_task = pickle.dumps(self)</span></div><div class="line">            <span class="comment">#rdb.lpush(shit_queue, pending_task)</span></div><div class="line">            <span class="comment"># 不能用pickle 的去序列化task。在after_return load的时候会出现很诡异的现象。load出的task是第一个acquired的task</span></div><div class="line">            <span class="comment"># 改为用json来做序列化</span></div><div class="line">            rdb.lpush(shit_queue, json.dumps(list(self.request.args)))</div><div class="line">            <span class="keyword">raise</span> Ignore()</div></pre></td></tr></table></figure>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>可以新建一个test_celery项目来检验一下。新建一个目录名叫 test_celery<br>然后新建一个tasks.py文件，内容就是上面代码。<br>用下面命令来启动Celery worker，这里用了8个进程来处理。设置多点可以增加task的并行执行任务数。<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">celery -A tasks worker -l info -c 8</div></pre></td></tr></table></figure></p>
<p>然后，可以启动ipython 进行调用task。<br><img src="/images/2019/celery_lock_task/1d60a617.png" alt=""><br>快速地敲几个task.delay</p>
<p>在celery日志中我们可以看到两个shit_tasks 是一个接一个来运行的。而不是并行执行。<br><img src="/images/2019/celery_lock_task/50ef5fbc.png" alt=""></p>
<p>代码放在了：<br><a href="https://github.com/mikolaje/celery_toys/tree/master/test_lock" target="_blank" rel="external">https://github.com/mikolaje/celery_toys/tree/master/test_lock</a></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="http://docs.celeryproject.org/en/latest/tutorials/task-cookbook.html#ensuring-a-task-is-only-executed-one-at-a-time" target="_blank" rel="external">http://docs.celeryproject.org/en/latest/tutorials/task-cookbook.html#ensuring-a-task-is-only-executed-one-at-a-time</a></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>celery</tag>
      </tags>
  </entry>
  <entry>
    <title>新西兰十二日自由行攻略</title>
    <url>/2019/nz_travel.html</url>
    <content><![CDATA[<p>新西兰:flag-nz:十二日自由行攻略<br>更多详情可上新西兰旅游局官方网站查询<a href="https://www.newzealand.com/cn/" target="_blank" rel="external">https://www.newzealand.com/cn/</a></p>
<p>:memo:签证：提前三个月办理即可，一般都能拿到有效期五年的签证。现在新西兰是电子签证，无需贴签，本人是找淘宝旅行社办理，如果个人不嫌麻烦完全可以自己登陆相关网站申请签证，蜂窝网有很全面的介绍，我是懒得自己弄。新西兰签证比欧美签证简单，银行卡半年流水只需三万，无需按指纹面试等，两周内可以出签。签证出来后留电子版或者打印出来，到新西兰过关时直接刷护照即可过关，都没人检查电子签，因为系统已有纪录。</p>
<p>另外如果是要转机的话，需要提前确认是否需要过境签。比如，在澳大利亚转机就要申请过境签。澳大利亚的过境签申请挺简单的，在官网注册个账号然后填个表就好了，免费的。</p>
<p><a href="https://immi.homeaffairs.gov.au/visas/getting-a-visa/visa-listing/transit-771" target="_blank" rel="external">https://immi.homeaffairs.gov.au/visas/getting-a-visa/visa-listing/transit-771</a></p>
<p>:airplane:飞机：建议提前买，签证没出来之前也可以买了（因为签证很容易过），越早买越便宜！直飞大概要11.5个小时，飞奥克兰的班次多且便宜。</p>
<p>我们来回买的都是在澳大利亚悉尼转机的。去的是维珍航空，回的是澳大利亚航空。维珍的机票比较便宜，便宜的缺点就是他们的refreshment好难吃，有一餐我根本就没吃饱，另外要了一小碗泡面。相比之下，澳大利亚航空的伙食就好太多了，吃得都好。</p>
<p>:dollar:现金：提前去银行换好新西兰币，汇率大概是1:4.6。准备好visa或者master卡，新西兰旅游景点很多可以刷微信、支付宝、银联。</p>
<p>:phone:电话卡：建议提前在淘宝买电话卡，比较便宜。朋友建议买Vodafone的卡，价格比较高，也可以买2degree的卡，华为背景价格便宜。我买了Vodafone和2degree的卡，感觉没啥区别，到了偏远地区都是没有信号。如果要自驾游的朋友最好提早在谷歌下载离线地图，免得走错路线。</p>
<p>:moneybag:保险：建议出发前买个基础保险，新西兰看病非常贵，发生拉肚子阑尾炎等情况必须看病，买了保险可以减轻负担。我们买的是安联保险，性价比很高。</p>
<p>:house:住宿：为了深入了解当地文化以及省钱，我们全程在Airbnb上订民宿，大家一定要订超赞房东，看住客的评论，以免入坑。</p>
<p>:bus:交通：新西兰交通费很贵，出租车尤其贵，Uber稍微比的士便宜点。公交车也比国内贵很多，如果是在皇后镇没办卡的情况下，每个人是五刀。办卡的话大概是2刀一次这样。自驾行虽然方便但是难度大，新西兰驾驶方向和国内相反，靠左行驶，山路多，当地人开车速度快。如果对国外驾驶规则不是很熟的话，不建议自驾。</p>
<p>具体行程:car:<br>前四天：奥克兰:boat:<br>奥克兰是新西兰最大的城市，一半是海水，一半是都市，是著名的风帆之都。这是我们在新西兰最喜欢的城市，大部分华人都住在奥克兰。我们参观了</p>
<p>【奥克兰战争纪念博物馆】</p>
<p><img src="/images/2019/nz_travel/DSC00223.JPG" alt="Sample Image Added via Markdown"></p>
<p>【伊甸山】</p>
<p>【天空塔】</p>
<p><img src="/images/2019/nz_travel/DSC00091.JPG" alt="Sample Image Added via Markdown"></p>
<p>【皇后大街】【海边码头】<br>强烈推荐新西兰唯一官方旅行社isite，在天空塔楼下就有一家，我们报了三次一日游，大巴包接送。最后一天，我们报了一日游前往【玛塔玛塔霍比特村】<br><img src="/images/2019/nz_travel/DSC00356.JPG" alt="Sample Image Added via Markdown"></p>
<p>，中土世界的童话小镇，满眼都是绿油油的草地。在奥克兰的交通方式就是uber和走路。<br>:fork_and_knife:推荐餐厅：Depot（天空塔下）、Giapo冰淇淋</p>
<p>中间四天：皇后镇:crown:<br>皇后镇是非常出名的旅游城市啦，超级多游客，中国游客尤其多。皇后镇衣食住行都很贵。皇后镇风光旖旎，生态环境也很好，新西兰的自来水可以直接喝。我们参观了</p>
<p>【格林诺奇】</p>
<p>【箭镇】</p>
<p>【瓦卡蒂普湖】</p>
<p><img src="/images/2019/nz_travel/IMG_3222.JPG" alt="Sample Image Added via Markdown"></p>
<p>【坐蒸汽船看牧场喂动物】</p>
<p><img src="/images/2019/nz_travel/IMG_3200.JPG" alt="Sample Image Added via Markdown"></p>
<p>皇后镇可以买优惠公交卡，10纽币可以坐5次，非常划算！最后一天，我们报了I-site的一日游，从皇后镇坐大巴到基督城，沿途经过库克山/蒂卡波湖/好牧羊人教堂。<br>:fork_and_knife:推荐餐厅：Fergburger汉堡</p>
<p><img src="/images/2019/nz_travel/IMG_3081.JPG" alt="Sample Image Added via Markdown"></p>
<p>最后两天：基督城:latin_cross:<br>基督城于2011年发生大地震，现在城市还没恢复过来，比较萧条，没什么人气。可以购买有轨复古电车:railway_car:一日游，随上随下，</p>
<p><img src="/images/2019/nz_travel/DSC00670.JPG" alt="Sample Image Added via Markdown"></p>
<p>在站点下车可参观【基督城博物馆】</p>
<p>【超级漂亮的海格利公园】</p>
<p><img src="/images/2019/nz_travel/DSC00650.JPG" alt="Sample Image Added via Markdown"></p>
<p>【基督城大教堂】</p>
<p><img src="/images/2019/nz_travel/IMG_3253.jpg" alt="Sample Image Added via Markdown"></p>
<p>【图书馆】</p>
<p>【纸板大教堂】</p>
<p>【皇后广场】基</p>
<p>督城内很多涂鸦，比较文艺。</p>
<p><img src="/images/2019/nz_travel/IMG_3266.JPG" alt="Sample Image Added via Markdown"></p>
<p>留一两天就够了。<br>newzealand.com<br>新西兰旅游局欢迎您 | 最全面最官方的新西兰旅游攻略<br>欢迎您来新西兰旅游。敬请浏览新西兰旅游局官方网站，了解新西兰旅游攻略、经典观光线路推荐、特色景点介绍、当地美酒佳肴、酒店住宿推荐、航班与机场信息等内容。</p>
]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
        <tag>travel</tag>
      </tags>
  </entry>
  <entry>
    <title>Ngrok服务器部署</title>
    <url>/2019/ngrok_deployment.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>自己使用Google Cloud，阿里云，AWS有好几年了，在云服务器上花了不少钱，有段时间做爬虫，需要较大的磁盘空间（几百G），每个月大概要5，600元。<br>一年下来就6，7钱了。想想，还是直接买一台主机或服务器在家里放着好了，也就几千块钱。于是我买了台主机在家里放着，配置远远比原来ECS的配置要高，<br>而且还更便宜。把数据全部迁移到了自己的主机，但很多时候自己都不在家，不可能把笨重的主机带出去外面。于是得想办法在其它地方通过网络连接到主机。<br>刚开选择用Teamviewer，可以远程连接到主机的桌面，我的主机的操作系统是Ubuntu的，感觉还行。但是，慢慢感觉效率不是很高，因为是用桌面远程，</p>
<p>所以网速要求，还是有点高，更大的问题是Teamviewer是收费的，而且价格不菲，这样成本算下来一年下来也要花费挺多钱。<br>于是，我想到了以前的花生壳 内网穿透映射，但那个也是收费的。终于找到了一个令我满意的解决方案—-Ngrok！</p>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>Ngrok到底好在哪里呢？ 首先，也是最重要的是它的成本底。<br>主机买服务的话也就10多块钱一个月，网上一搜就很多Ngrok的提供商；自己搭建的话，也就买一台虚拟机的价格，我买的是某云的ECS。大概也就几十来块钱。</p>
<p>其次，Ngrok可以把你家里的电脑当成服务器来使用，远程登陆SSH；或者作为数据库，暴露对应的端口提供服务；或者直接作为Web服务，直接提供http访问。<br>这使得不用买昂贵的云服务，省了不少钱。不够配置的话可以自己买内存，SSD增加机器配置。</p>
<h3 id="准备工作："><a href="#准备工作：" class="headerlink" title="准备工作："></a>准备工作：</h3><h4 id="需要购买的服务"><a href="#需要购买的服务" class="headerlink" title="需要购买的服务"></a>需要购买的服务</h4><ol>
<li>有一个外网的虚拟机，我用的是国内的阿里云的，操作系统是Ubuntu。</li>
<li>有个自己注册的域名。<br>需要添加2条域名解析：<br><img src="/images/2019/ngrok_deployment/647f55bb.png" alt=""></li>
</ol>
<h4 id="部署工作："><a href="#部署工作：" class="headerlink" title="部署工作："></a>部署工作：</h4><ol>
<li><p>apt-get install golang<br>因为ngrok是用Go 来写的，所以要安装一下Go环境</p>
</li>
<li><p>git clone <a href="https://github.com/inconshreveable/ngrok.git" target="_blank" rel="external">https://github.com/inconshreveable/ngrok.git</a><br>然后把ngrok的源码clone下来。</p>
</li>
</ol>
<h4 id="证书安装："><a href="#证书安装：" class="headerlink" title="证书安装："></a>证书安装：</h4><blockquote>
<p>使用ngrok.com官方服务时，我们使用的是官方的SSL证书。自己建立ngrok服务，需要我们生成自己的证书，并提供携带该证书的ngrok客户端。首先指定域名：</p>
</blockquote>
<p>进入ngrok目录后，运行下面的指令<br><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="built_in">export</span> NGROK_DOMAIN=<span class="string">"ngrok.xfl.host"</span>  </div><div class="line">//这里换成你的自己注册的域名，比如你注册的域名为abc.com。这里可以填 ngrok.abc.com</div><div class="line"></div><div class="line">openssl genrsa -out rootCA.key 2048</div><div class="line">openssl req -x509 -new -nodes -key rootCA.key -subj <span class="string">"/CN=<span class="variable">$NGROK_DOMAIN</span>"</span> -days 5000 -out rootCA.pem</div><div class="line">openssl genrsa -out device.key 2048</div><div class="line">openssl req -new -key device.key -subj <span class="string">"/CN=<span class="variable">$NGROK_DOMAIN</span>"</span> -out device.csr</div><div class="line">openssl x509 -req -in device.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out device.crt -days 5000</div></pre></td></tr></table></figure></p>
<p>我们在编译可执行文件之前，需要把生成的证书分别替换到 assets/client/tls和assets/server/tls中，这两个目录分别存放着ngrok和ngrokd的默认证书。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">cp rootCA.pem assets/client/tls/ngrokroot.crt</div><div class="line">cp device.crt assets/server/tls/snakeoil.crt</div><div class="line">cp device.key assets/server/tls/snakeoil.key</div></pre></td></tr></table></figure>
<h3 id="ngrok部署安装"><a href="#ngrok部署安装" class="headerlink" title="ngrok部署安装"></a>ngrok部署安装</h3><h4 id="编译ngrokd-和-ngrok"><a href="#编译ngrokd-和-ngrok" class="headerlink" title="编译ngrokd 和 ngrok"></a>编译ngrokd 和 ngrok</h4><p>首先需要知道，ngrokd 为服务端的执行文件，ngrok为客户端的执行文件。</p>
<p>有没有release的区别是，包含release的编译结果会把assets目录下的内容包括进去，从而可以独立执行。<br>如果你今后还要更换证书，建议编译不包含release的版本。。首先编译ngrok服务端（ngrokd），默认为Linux版本：<br><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">make clean</div><div class="line">make release-server</div></pre></td></tr></table></figure></p>
<p>编译ngrokd后，我们来编译ngrok(客户端)<br>在编译客户端的时候需要指明对应的操作系统和构架：</p>
<p>Linux 平台 32 位系统：GOOS=linux GOARCH=386<br>Linux 平台 64 位系统：GOOS=linux GOARCH=amd64<br>Windows 平台 32 位系统：GOOS=windows GOARCH=386<br>Windows 平台 64 位系统：GOOS=windows GOARCH=amd64<br>MAC 平台 32 位系统：GOOS=darwin GOARCH=386<br>MAC 平台 64 位系统：GOOS=darwin GOARCH=amd64<br>ARM 平台：GOOS=linux GOARCH=arm</p>
<p>我的Ubuntu属于linux 64，所以我执行如下指令：<br><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">GOOS=linux GOARCH=amd64 make release-client</div></pre></td></tr></table></figure></p>
<h4 id="启动ngrokd"><a href="#启动ngrokd" class="headerlink" title="启动ngrokd"></a>启动ngrokd</h4><p>编译后生成两个文件分别为服务端（ngrokd）和客户端(ngrok)。切换到对应的文件夹，运行服务端：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">./ngrokd -domain=<span class="string">"<span class="variable">$NGROK_DOMAIN</span>"</span> -httpAddr=<span class="string">":801"</span> -httpsAddr=<span class="string">":802"</span></div><div class="line">//801 是访问的http端口，比如，在本例子中，访问http://ngrok.xfl.host:801 就可以看到映射出的网页</div></pre></td></tr></table></figure>
<p>参数-domain表示服务器域名，请改成你自己的域名；-httpAddr表示默认监听的HTTP端口，-httpsAddr表示默认监听的HTTPS端口，<br>因为我用不到所以都设置成空字符串”“来关闭监听，如果需要打开的话记得格式是:12345（冒号+端口号）这样的；-tunnelAddr表示服务器监听客户端连接的隧道端口号，格式和前面一样；<br>-log表示日志文件位置；还有个-log-level用来控制日志记录的事件级别，选项有DEBUG、INFO、WARNING、ERROR。</p>
<p>如果编译的是不带release的版本，还可以通过-tlsCrt和-tlsKey选项来指定证书文件的位置。</p>
<p>出现类似以下内容，则说明我们的服务器端ngrokd正常运行了:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[16:41:56 CST 2017/04/20] [INFO] (ngrok/log.(*PrefixLogger).Info:83) [registry] [tun] No affinity cache specified</div><div class="line">[16:41:56 CST 2017/04/20] [INFO] (ngrok/log.(*PrefixLogger).Info:83) [metrics] Reporting every 30 seconds</div><div class="line">[16:41:57 CST 2017/04/20] [INFO] (ngrok/log.Info:112) Listening for public http connections on [::]:80</div><div class="line">[16:41:57 CST 2017/04/20] [INFO] (ngrok/log.Info:112) Listening for public https connections on [::]:443</div><div class="line">[16:41:57 CST 2017/04/20] [INFO] (ngrok/log.Info:112) Listening for control and proxy connections on [::]:4443</div><div class="line">[16:41:57 CST 2017/04/20] [INFO] (ngrok/log.(*PrefixLogger).Info:83) [tun:627acc92] New connection from 42.53.196.242:9386</div><div class="line">[16:41:57 CST 2017/04/20] [DEBG] (ngrok/log.(*PrefixLogger).Debug:79) [tun:627acc92] Waiting to read message</div><div class="line">[16:41:57 CST 2017/04/20] [DEBG] (ngrok/log.(*PrefixLogger).Debug:79) [tun:627acc92] Reading message with length: 159</div></pre></td></tr></table></figure>
<h4 id="配置ngrok客户端"><a href="#配置ngrok客户端" class="headerlink" title="配置ngrok客户端"></a>配置ngrok客户端</h4><p>将之前编译好的客户端文件(ngrok 文件)拷贝到需要使用服务的设备（自己买的那台笨重的主机）上。</p>
<h4 id="启动ngrok客户端"><a href="#启动ngrok客户端" class="headerlink" title="启动ngrok客户端"></a>启动ngrok客户端</h4><p>在ngrok同路径下建立配置文件ngrok.yml<br><figure class="highlight yaml"><table><tr><td class="code"><pre><div class="line"><span class="attr">server_addr:</span> <span class="string">"ngrok.xfl.host:4443"</span></div><div class="line"><span class="attr">trust_host_root_certs:</span> <span class="literal">false</span></div><div class="line"><span class="attr">tunnels:</span></div><div class="line"><span class="attr">  ssh:</span></div><div class="line"><span class="attr">    remote_port:</span> <span class="number">6666</span></div><div class="line"><span class="attr">    proto:</span></div><div class="line"><span class="attr">      tcp:</span> <span class="number">22</span></div></pre></td></tr></table></figure></p>
<p>server_addr端口默认4443，可通过ngrokd服务端启动修改端口。在tunnels里配置隧道信息，<br>注意http和https隧道可设置subdomain和auth，而tcp里只能设置remote_port。</p>
<p>使用如下命令启动ngrok客户端：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ngrok -log=stdout -config=./ngrok.yml start ssh</div></pre></td></tr></table></figure></p>
<p>正常启动，你将会看到如下日志：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ngrok                                                                                                                                                                                     (Ctrl+C to quit)</div><div class="line">                                                                                                                                                                                                          </div><div class="line">Tunnel Status                 online                                                                                                                                                                      </div><div class="line">Version                       1.7/1.7                                                                                                                                                                     </div><div class="line">Forwarding                    http://demo.ngrok.xfl.host -&gt; 127.0.0.1:19999                                                                                                                               </div><div class="line">Forwarding                    https://demo.ngrok.xfl.host -&gt; 127.0.0.1:19999                                                                                                                              </div><div class="line">Web Interface                 127.0.0.1:4040                                                                                                                                                              </div><div class="line"># Conn                        0                                                                                                                                                                           </div><div class="line">Avg Conn Time                 0.00ms</div></pre></td></tr></table></figure></p>
<p>Notice:如果显示reconnecting说明连接有错，在运行时加入-log=stdout来进行debug。</p>
<ol>
<li>有可能是因为你的Ngrok服务器没有开 4443端口</li>
<li>有可能是域名没有解析成功</li>
</ol>
<h4 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h4><p>ngrok 的配置文件是完全可选的非常简单 YAML 格式文件，他可以允许你使用 ngrok 一些更高级的功能，例如：</p>
<p>同时运行多个隧道</p>
<ul>
<li>连接到自定义的 ngrok 服务器</li>
<li>调整 ngrok 一些很神秘的功能</li>
<li><p>ngrok 的配置文件默认从 ~/.ngrok 加载。你可以通过 -config 参数重写配置文件的地址</p>
</li>
<li><p>同时运行多个隧道<br>为了运行多个隧道，你需要在配置文件当中使用 tunnels 参数配置每个隧道。隧道的参数以字典的形式配置在配置文件当中。<br>举个例子，让我们来定义三个不同的隧道。第一个隧道是一个有认证的只转发 https 的隧道。第二个隧道转发我们自己机器的 22 端口以便让我可以通过隧道连接到自己的电脑。<br>最后，我们使用自己的域名创造了一个隧道，我们将要在黑客马拉松中展示这个。</p>
</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><div class="line"><span class="attr">tunnels:</span></div><div class="line"><span class="attr">  client:</span></div><div class="line"><span class="attr">    auth:</span> <span class="string">"user:password"</span></div><div class="line"><span class="attr">    proto:</span></div><div class="line"><span class="attr">      https:</span> <span class="number">8080</span></div><div class="line"><span class="attr">  ssh:</span></div><div class="line"><span class="attr">    proto:</span></div><div class="line"><span class="attr">      tcp:</span> <span class="number">22</span></div><div class="line">  hacks.inconshreveable.com:</div><div class="line"><span class="attr">    proto:</span></div><div class="line"><span class="attr">      http:</span> <span class="number">9090</span></div></pre></td></tr></table></figure>
<p>通过 ngrok start 命令，我们可以同时运行三个隧道，后面要接上我们要启动的隧道名。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">ngrok start client ssh hacks.inconshreveable.com</div></pre></td></tr></table></figure>
<ul>
<li>隧道设置<br>每一个隧道都可以设置以下五个参数：proto，subdomain，auth，hostname 以及 remote_port。<br>每一个隧道都必须定义 proto ，因为这定义了协议的类型以及转发的目标。当你在运行 http/https 隧道时， auth 参数是可选的，<br>同样， remote_port 也是可选的，他声明了某个端口将要作为远程服务器转发的端口，请注意这只适用于 TCP 隧道。<br>ngrok 使用每个隧道的名字做到子域名或者域名，但你可以重写他：</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><div class="line"><span class="attr">tunnels:</span></div><div class="line"><span class="attr">  client:</span></div><div class="line"><span class="attr">    subdomain:</span> <span class="string">"example"</span></div><div class="line"><span class="attr">    auth:</span> <span class="string">"user:password"</span></div><div class="line"><span class="attr">    proto:</span></div><div class="line"><span class="attr">      https:</span> <span class="number">8080</span></div></pre></td></tr></table></figure>
<p>对于 TCP 隧道，你可以会通过 remote_port 参数来指定一个远程服务器的端口作为映射。如果没有声明，服务器将会给你随机分配一个端口。</p>
<h3 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h3><p>如果使用国内IP或者国内的域名的话，用http的话貌似因为要备案，所以访问不了。前两天我是可以正常访问的，但是过了两天后就突然访问不了了。<br>可以改为用TCP的方式比如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">netdata:</div><div class="line">  remote_port: 801 </div><div class="line">  proto:</div><div class="line">    tcp: 19999</div></pre></td></tr></table></figure></p>
<p>访问 ngnrok.xfl.host:801 即可。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://morongs.github.io/2016/12/28/dajian-ngrok/" target="_blank" rel="external">https://morongs.github.io/2016/12/28/dajian-ngrok/</a><br><a href="https://luozm.github.io/ngrok" target="_blank" rel="external">https://luozm.github.io/ngrok</a><br><a href="https://imlonghao.com/28.html" target="_blank" rel="external">https://imlonghao.com/28.html</a></p>
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>ngrok</tag>
      </tags>
  </entry>
  <entry>
    <title>CDH 在ubuntu上的部署和安装，以及一些坑</title>
    <url>/2019/cdh_install.html</url>
    <content><![CDATA[<p>最近两天在自己电脑上搭建一个Cloudera Manager来玩玩。本来以为挺简单的，只是在Web UI上无脑下一步就好了，<br>但其实还是遇到挺多问题的。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="在服务器上的操作"><a href="#在服务器上的操作" class="headerlink" title="在服务器上的操作"></a>在服务器上的操作</h3><p>刚开始基本上，就是按照官网的步骤来走，首先做一些前置工作:</p>
<ol>
<li>配置下apt</li>
<li>安装JDK.</li>
<li>安装下NTP时间同步的程序；</li>
<li>安装好Mysql，MariaDB，Posgres。<br>其中的一个数据库，刚开始以为都要安装。。。然后又把MariaDB这些一个个卸载了；</li>
<li>在Mysql中创建一些CM所需的数据库和表。如下所是。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">CREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON scm.* TO &apos;scm&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON rman.* TO &apos;rman&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON hue.* TO &apos;hue&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON metastore.* TO &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON sentry.* TO &apos;sentry&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE nav DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON nav.* TO &apos;nav&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE navms DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON navms.* TO &apos;navms&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON oozie.* TO &apos;oozie&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div></pre></td></tr></table></figure>
<ol>
<li>通过scm_prepare_database.sh 脚本来进一步设置Manager Database<br>sudo /opt/cloudera/cm/schema/scm_prepare_database.sh [options] <databasetype> <databasename> <databaseuser> <password><br>只需要执行一次<code>sudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm mypassword</code>就好了。</password></databaseuser></databasename></databasetype></li>
</ol>
<h3 id="在Web-UI上的安装"><a href="#在Web-UI上的安装" class="headerlink" title="在Web UI上的安装"></a>在Web UI上的安装</h3><ol>
<li>首先记得在每台机器上配置好/etc/hosts</li>
<li>在Web上的安装基本上就是点继续。有些地方要注意。<br>这一步会等待比较长的时间，会下载安装一些parcels。</li>
</ol>
<p><img src="/images/2019/cdh_install/323205c4.png" alt="Sample Image Added via Markdown"></p>
<ol>
<li>然后，基本上就是下一步了。到这一步，我是创建了一个叫cloudera的用户，要给与它sudo以及password-exempt<br><img src="/images/2019/cdh_install/cloudera_install.jpg" alt="Sample Image Added via Markdown"></li>
</ol>
<p>对了，因为我的是单机版的，所以HDFS那边会报错一个叫：副本不足的块 存在隐患。<br>这是因为只有一个节点，Block块无法，分配到其它的节点作为备份。默认是有2个备份Block分发到其它节点。</p>
<h3 id="启动CDH"><a href="#启动CDH" class="headerlink" title="启动CDH"></a>启动CDH</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">sudo systemctl start cloudera-scm-server</div><div class="line"></div><div class="line"># 查看scm server日志，scm的全称是：The Service and Configuration Manager </div><div class="line">sudo tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log</div></pre></td></tr></table></figure>
<h3 id="停止CDH"><a href="#停止CDH" class="headerlink" title="停止CDH"></a>停止CDH</h3><p>有些时候我们要停机检修一下电脑，所以要停止Cluster。很简单，首先进入到CDH的Web UI的Cluster主界面, 左上角有个Action<br>，点一下弹出下拉条，然后选停止。等几分钟后集群的所有组件就会停止了。</p>
<p>然后进入到主节点的终端，输入<code>sudo systemctl stop cloudera-scm-server</code>，就全部停止了。</p>
<h2 id="CDH的一些配置"><a href="#CDH的一些配置" class="headerlink" title="CDH的一些配置"></a>CDH的一些配置</h2><h3 id="Yarn-RM-NM共用一个host"><a href="#Yarn-RM-NM共用一个host" class="headerlink" title="Yarn: RM,NM共用一个host"></a>Yarn: RM,NM共用一个host</h3><p>默认情况下Resource Manager会单独用一个节点。但是我的RM host内存和CPU都有剩余，跑app的时候把资源压在<br>device2上有点浪费了，我利用起device1的资源来。<br>首先进入到Yarn的版块，Action下拉框，点击Add Role Instance<br><img src="/images/2019/cdh_install/yarn_rm_nm1.png" alt="Sample Image Added via Markdown"><br><img src="/images/2019/cdh_install/yarn_rm_nm1.png" alt="Sample Image Added via Markdown"></p>
<blockquote>
<p>注意，如果该instance的commission state为decommissioned的话要把它改为commissioned</p>
</blockquote>
<h3 id="增加服务"><a href="#增加服务" class="headerlink" title="增加服务"></a>增加服务</h3><p>如果我们想新增加一些组件，比如Kafka或Spark，然后我们可以点击Cluster版块的Action下拉框，选中第一个 Add Service<br>进入新增Service的页面。</p>
<h2 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h2><h3 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h3><p>我增加一个节点的时候遇到如下报错<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Host with invalid Cloudera Manager GUID is detected</div><div class="line">...</div><div class="line">Error, CM server guid updated, expected c3b5fe15-5f29-434b-ae0a-4750b56c72ab, received dc1d28d4-4c78-4a07-919b-a9eaf7190d41</div></pre></td></tr></table></figure></p>
<p>解决方法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">验证如下配置文件，确定hostname是否正确</div><div class="line">$ nano /etc/cloudera-scm-agent/config.ini</div><div class="line">so that the hostname where the same as the command $ hostname returned.</div><div class="line">Then rm /var/lib/cloudera-scm-agent/cm_guid</div><div class="line">然后删除每个节点的cm_guid</div><div class="line">then I restarted the agent and the server of cloudera:</div><div class="line">然后重启</div><div class="line">$ service cloudera-scm-agent restart</div><div class="line">$ service cloudera-scm-server restart</div></pre></td></tr></table></figure></p>
<h3 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h3><p>在NameNode Format的时候遇到如下报错<br>Running in non-interactive mode, and data appears to exist in Storage Directory /dfs/nn. Not formatting.</p>
<p>解决方案：<br>删除/dfs/nn 以及 /dfs/dn里面的所有数据<br>因为之前我安装了一个单机集群，HDFS里面放了一些数据</p>
<h3 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h3><p>Cloudera 在Validate Hive Metastore schema的时候出现如下错误，发现metastore里面没有VERSION table<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Fri Jul 19 14:06:33 CST 2019 WARN: Establishing SSL connection without server&apos;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&apos;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &apos;false&apos;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</div><div class="line">org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version, Cause:Table &apos;metastore.VERSION&apos; doesn&apos;t exist</div><div class="line">org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version, Cause:Table &apos;metastore.VERSION&apos; doesn&apos;t exist</div><div class="line">	at org.apache.hadoop.hive.metastore.CDHMetaStoreSchemaInfo.getMetaStoreSchemaVersion(CDHMetaStoreSchemaInfo.java:342)</div><div class="line">	at org.apache.hive.beeline.HiveSchemaTool.validateSchemaVersions(HiveSchemaTool.java:685)</div><div class="line">	at org.apache.hive.beeline.HiveSchemaTool.doValidate(HiveSchemaTool.java:578)</div><div class="line">	at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1142)</div><div class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</div><div class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</div><div class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class="line">	at java.lang.reflect.Method.invoke(Method.java:498)</div><div class="line">	at org.apache.hadoop.util.RunJar.run(RunJar.java:313)</div><div class="line">	at org.apache.hadoop.util.RunJar.main(RunJar.java:227)</div><div class="line">*** schemaTool failed ***</div></pre></td></tr></table></figure></p>
<p>解决方案：<br><code>dennis@device1:/opt/cloudera/parcels/CDH/lib/hive/bin$ schematool -dbType mysql -initSchema -passWord password -userName hive</code></p>
<h3 id="问题4"><a href="#问题4" class="headerlink" title="问题4"></a>问题4</h3><p>在Hue上的hive上运行一些 insert 和count(*) 操作时候会一直卡住（stuck, hang），没有任何反应，也没报错。<br>看日志是说MR 还没有启动。在Cloudera的community上查到 要mapred-site.xml的参数 mapreduce.framework.name 设置为 local</p>
<p>于是我在CDH中的Yarn集群下修改了mapreduce.framework.name 为 local，然后重启集群后就成功了。 select count(*) 和 insert就不会卡住了。</p>
<h3 id="问题5"><a href="#问题5" class="headerlink" title="问题5"></a>问题5</h3><p>在hue上面可以正常地使用Hive。在device2下用hive cli没有问题。但在device1 下的bash执行hive command，里面输入show databases后报错：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</div></pre></td></tr></table></figure></p>
<p> 解决方案：<br> 先用 hive -hiveconf hive.root.logger=DEBUG,console<br> 在调试，查看到更多有价值的报错信息。<br> 果然，查到如下信息：<br> <figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"> Caused by: java.io.IOException: Keystore was tampered with, or password was incorrect</div><div class="line">	at com.sun.crypto.provider.JceKeyStore.engineLoad(JceKeyStore.java:865) ~[sunjce_provider.jar:1.8.0_112]</div><div class="line">	at java.security.KeyStore.load(KeyStore.java:1445) ~[?:1.8.0_121]</div><div class="line">	at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.locateKeystore(AbstractJavaKeyStoreProvider.java:322) ~[hadoop-common-3.0.0-cdh6.2.0.jar:?]</div><div class="line">	at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.&lt;init&gt;(AbstractJavaKeyStoreProvider.java:86) ~[hadoop-common-3.0.0-cdh6.2.0.jar:?]</div><div class="line">	at org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider.&lt;init&gt;(LocalJavaKeyStoreProvider.java:58) ~[hadoop-common-3.0.0-cdh6.2.0.jar:?]</div><div class="line"></div><div class="line">	at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:237) ~[hive-exec-2.1.1-cdh6.2.0.jar:2.1.1-cdh6.2.0]</div><div class="line">	... 23 more</div><div class="line">Caused by: java.security.UnrecoverableKeyException: Password verification failed</div></pre></td></tr></table></figure></p>
<p>按日志的报错信息来说是我的源数据库密码不对，于是我查看hive-site.xml配置文件，发现<br>/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/hive/conf/hive-site.xml<br>也就是/etc/hive/conf/hive-site.xml(我猜CDH会把上面目录的所有配置文件复制一遍到 /etc/hive/conf/下)<br>我之前把它改了，所以那配置有问题，我把它改为默认的配置重启后就恢复正常了！</p>
<h3 id="问题6"><a href="#问题6" class="headerlink" title="问题6"></a>问题6</h3><p>启动 cloudera-scm-server 时候报错如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table &apos;metastore.CM_VERSION&apos; doesn&apos;t exist</div></pre></td></tr></table></figure></p>
<p> 解决方案：<br><code>sudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm mypassword</code><br>重新执行下这条命令。之前我把所有的服务都执行了一遍（amon, rman, … metastore, … etc)，是我误解了scm_prepare_database.sh的作用。<br>按官网所说的：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Cloudera Manager Server includes a script that can create and configure a database for itself. The script can:</div><div class="line">Create the Cloudera Manager Server database configuration file.</div><div class="line">(MariaDB, MySQL, and PostgreSQL) Create and configure a database for Cloudera Manager Server to use.</div><div class="line">(MariaDB, MySQL, and PostgreSQL) Create and configure a user account for Cloudera Manager Server.</div></pre></td></tr></table></figure></p>
<p>这个脚本只需要执行一次就好了，就是<code>sudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm mypassword</code><br>然后重启cloudera-scm-server解决问题。此外，通过<code>/etc/cloudera-scm-server/db.properties</code> 也可以确定目前scm用的是哪个数据库。</p>
<h3 id="问题7"><a href="#问题7" class="headerlink" title="问题7"></a>问题7</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">The host&apos;s NTP service could not be located or did not respond to a request for the clock offset.</div></pre></td></tr></table></figure>
<p>解决方法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">service ntp restart</div><div class="line"># 加上自启动</div><div class="line">sudo systemctl enable ntp</div></pre></td></tr></table></figure></p>
<h3 id="问题8"><a href="#问题8" class="headerlink" title="问题8"></a>问题8</h3><blockquote>
<p>CDH官方是要求安装Oracle的JDK的，其它版本的JDK使用的话也行，但不保证一定兼容。<br>在增加节点的时候，在安装agent的时候 apt报错:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">root@device3:/data/software/jdk1.8.0_121# apt --fix-broken install</div><div class="line">Reading package lists... Done</div><div class="line">Building dependency tree       </div><div class="line">Reading state information... Done</div><div class="line">Correcting dependencies... Done</div><div class="line">The following additional packages will be installed:</div><div class="line">  cloudera-manager-daemons</div><div class="line">  The following NEW packages will be installed:</div><div class="line">    cloudera-manager-daemons</div><div class="line">    0 upgraded, 1 newly installed, 0 to remove and 106 not upgraded.</div><div class="line">    26 not fully installed or removed.</div><div class="line">    Need to get 0 B/1,218 MB of archives.</div><div class="line">    After this operation, 1,420 MB of additional disk space will be used.</div><div class="line">    Do you want to continue? [Y/n] y</div><div class="line">    (Reading database ... 106482 files and directories currently installed.)</div><div class="line">    Preparing to unpack .../cloudera-manager-daemons_6.2.0~968826.ubuntu1804_all.deb ...</div><div class="line">    +======================================================================+</div><div class="line">    |      Error: Unable to find a compatible version of Java on this host,|</div><div class="line">    |             either because JAVA_HOME has not been set or because a   |</div><div class="line">    |             compatible version of Java is not installed.             |</div><div class="line">    +----------------------------------------------------------------------+</div><div class="line">    | Please download a supported version of the Oracle JDK from the       |</div><div class="line">    | Oracle Java web site:                                                |</div><div class="line">    |                                                                      |</div><div class="line">    |  &gt; http://www.oracle.com/technetwork/java/javase/index.html &lt;        |</div><div class="line">    |                                                                      |</div><div class="line">    | Cloudera Manager requires Oracle JDK 1.8 or later.                   |</div><div class="line">    | NOTE: Cloudera Manager will find the Oracle JDK when starting,       |</div><div class="line">    |       regardless of whether you installed the JDK using a binary     |</div><div class="line">    |       installer or the RPM-based installer.                          |</div><div class="line">    +======================================================================+</div><div class="line">    dpkg: error processing archive /var/cache/apt/archives/cloudera-manager-daemons_6.2.0~968826.ubuntu1804_all.deb (--unpack):</div><div class="line">     new cloudera-manager-daemons package pre-installation script subprocess returned error exit status 1</div><div class="line">     Errors were encountered while processing:</div><div class="line">      /var/cache/apt/archives/cloudera-manager-daemons_6.2.0~968826.ubuntu1804_all.deb</div><div class="line">      E: Sub-process /usr/bin/dpkg returned an error code (1)</div></pre></td></tr></table></figure></p>
</blockquote>
<p>这是因为<code>sudo java -version</code>没有调通。<br>解决办法：<br>把sudo 下的java环境搞好就OK了。<br>或者直接切换到root用户下 <code>apt --fix-broken install</code>, 因为root用户下就不会有java路径找不到的问题</p>
<p>另外增加节点的话，要记得把hosts DNS复制一份在新加的节点上</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.cloudera.com/documentation/enterprise/6/6.2/topics/introduction.html" target="_blank" rel="external">https://www.cloudera.com/documentation/enterprise/6/6.2/topics/introduction.html</a><br><a href="https://blog.csdn.net/qq_24409555/article/details/76139886" target="_blank" rel="external">https://blog.csdn.net/qq_24409555/article/details/76139886</a><br><a href="https://community.cloudera.com/t5/Batch-SQL-Apache-Hive/Hive-Errors-happend-when-execute-hive-service-metastore/m-p/93050#M3282" target="_blank" rel="external">https://community.cloudera.com/t5/Batch-SQL-Apache-Hive/Hive-Errors-happend-when-execute-hive-service-metastore/m-p/93050#M3282</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>cloudera</tag>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title>启德教育被坑记</title>
    <url>/2018/study_abroad_agency.html</url>
    <content><![CDATA[<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>留学一直都是我的梦想，但因为种种原因毕业后没能实现，工作了几年后，还是放不下，于是打算去香港留学，因为离家里比较近，年纪大了，不方便出走太远。<br>但由于平时要上班工作，没有时间，也不想花太多时间在申请上，于是就打算找中介。在国内，留学中介中知名度<br>最高的是启德吧。我看他们家的广告随处可见，之前也些朋友找的启德做中介的。于是我就选了启德。</p>
<p>当时中介告诉我，已经是年底了，申请的时间比较晚，要赶紧行动。于是中介耐心地给我介绍了我这条件能去的学校和专业，就赶紧交钱申请。<br>当时交了1.5万（还不包括申请费，申请费还得要近3000RMB），6各专业好像。他们是按专业数量收费的。加专业还要加钱。</p>
<p>当时我完全不知道他们是怎么操作的，对留学申请我也完全没有经验和概念。我只是在很多年前考过一次雅思，6分。就这样，我被中介牵着鼻子走了。<br>中介让我干嘛我就干嘛。</p>
<p>中介说让我先准备考雅思。然后在用中文写下各专业的文书，再递给中介让她们翻译。</p>
<p>经过一两个月的努力，我把雅思考到6.5(低分6)了。于是就把中文的文书交给中介，结果他们等了大半个月才翻译出来。翻译出来的质量当时我根本没有看。<br>于是他们就帮我投递了。最先投的是浸会大学，很快就有回复了，于是安排我面试。面试时候，面试官只是简单跟我聊聊，为什么选择这个专业呀，对选择这个program有什么挑战，<br>等等。很快，一个礼拜后就给我发offer了。</p>
<p>收到offer，意味着不能退款了。1.5万就没了。后来等了很久也没等到，其它专业的offer，城大，理工的都没有任何回复，连面试通知都没。很失落，最后也没有接收浸会的offer，<br>没有交留位费。就放弃了当年的入学机会。</p>
<p>时隔半年后，我再次打算申请。我又交了1.2万给启德，当时也是怪自己没有花精力去了解留学申请，太过于依赖中介了。并且，自己的留学欲望又很强。这就导致我很轻易地把这钱交给中介了。<br>我都没怎么犹豫，也不觉得一两万前很多，很果断地打钱过去了。</p>
<p>去年的入学失败，使得我今年必须得抓住机会，不然，很有可能以后都没有机会出去了。于是，我今年的申请盯得特别紧。我第一次认真看启德写的PS文书，对比地看了下985学生申请的文书，<br>那差距啊。。。启德写的文书这是什么玩意啊，简直就是笑话。写的基本上通篇都是废话。开头就是说自己的人生格言，然后是出生家庭什么的。说了一大堆跟自己的能力展现没任何关系的<br>废话内容。于是，我基本上把所有专业的PS文书都重新写了遍。然后是进行艰苦的修改阶段， 改了又改。自己还另外掏了千百块块找了其他人帮忙修改查看。花了好多心血才把终稿定下来了。</p>
<p>TM的我这一切努力都和中介没毛关系啊，我出这么钱，中介没帮我啥事情啊。从备战雅思，去学校申请资料，修改文书PS，都是我自己的努力和艰辛。中介做的就是帮你递交申请资料而已，<br>PS这些虽然他们写了，但质量太差，基本上用不了，还是要你自己重新再写一遍。这些都算了，最让我气愤的是：交完钱后，中介就对你不上心了。比如微信回复不及时回，而且回复的态度<br>完全和你交钱之前的不一样。带电话咨询她，她还说，明天有问题问助理，我明天休息不方便接听电话。这让人听起来感觉是非常不热情，不友好的。</p>
]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>daily</tag>
      </tags>
  </entry>
  <entry>
    <title>华强北被坑记</title>
    <url>/2018/huaqiangbei_black_market.html</url>
    <content><![CDATA[<h2 id="好奇心"><a href="#好奇心" class="headerlink" title="好奇心"></a>好奇心</h2><p>之前有段时间一直为了好奇，想拍摄一些比较隐蔽的探访视频然后放YouTube上赚点广告费。想了很久，终于跑到忍不住<br>跑到华强北去寻找买一个眼镜摄像仪。就是带着摄像头在眼镜上的偷拍神器。去到华强北摄像头市场里，问了好多商铺，<br>他们都不卖，当然是私底下地问，因为这东西肯定不是不能明着卖的。后来还是找了个路边站街拉客的大妈，一问就有了。<br>她让我跟着她走，走到一个稍微人少点地方，她打电话给卖这东西的贩子，然后就在那等那贩子过来。</p>
<p>10分钟后，一个矮矮的中年男子（四五十岁），拿着个黑色大袋子走过来，说带我去一个更少人的角落验货。终于看到了<br>传说中的眼镜拍摄神器。这玩意超出我的预期，并没有我想象中的那么好，因为它一看就有些不太对头，不像一个正常的眼镜，<br>左右两根支架特别粗，因为里面有电路板，还要放置MINI-SD卡。它的摄像头在眼镜的最中间位置，也就是连接两个镜片<br>的地方，不仔细看的话，不会发现，不过稍微仔细点看还是能看出来它是有个摄像头在上面的。</p>
<p>接着，那个贩子拍摄了一段视频，我感觉视频的质量，分别率不是很好，大概就是几百块钱手机的拍摄效果吧。虽然不太满意，<br>但感觉跟那贩子看了有点久，有点骑虎难下，最后还是买下来了，花了750，他开价是900块。终于止住我那段时间以来的好奇心和欲望。</p>
<p>这东西还不带SD卡，于是我还花了几十块钱买了个SD卡。那天总共花了850左右吧，现在想想感觉还是挺不值的，因为拿回来后<br>也就拍了一下，就再也没用过了，感觉就是一个玩具而已，没有什么实际的用途。</p>
<p><img src="/images/2018/huaqiangbei_black_market/36750b47.png" alt=""></p>
<p><img src="/images/2018/huaqiangbei_black_market/8984d475.png" alt=""></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>就当花钱买个教训吧，我记得，我不是我第一次在华强北被坑，以前还很想买一种录音手表，后来以色列人送了我一个，大概是一百多块钱吧，用了几次就扔了。<br>以后还是要注意下自己的消费欲望。买之前再三思考，这东西值不值。不过很多东西其实还是买来后才知道它值不值。</p>
]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
  <entry>
    <title>新疆北疆自驾12日游（下）</title>
    <url>/2018/xinjiang_trip2.html</url>
    <content><![CDATA[<h1 id="伊犁"><a href="#伊犁" class="headerlink" title="伊犁"></a>伊犁</h1><p>来到伊犁这边后，天气变得越来越冷了。尤其是早晨晚上这段时间都非常寒冷。有时候还会下点雨</p>
<h2 id="巴音布鲁克"><a href="#巴音布鲁克" class="headerlink" title="巴音布鲁克"></a>巴音布鲁克</h2><p>巴音布鲁克的九曲十八弯<br><img src=".xinjiang_trip2_images/Bayanbulak_lake.png" alt=""></p>
<p>湖中的天鹅<br><img src=".xinjiang_trip2_images/swarm.png" alt=""></p>
<p>在巴音布鲁克草原上第一次骑马, 骑了有一两公里路吧，大概是2百块钱<br><img src=".xinjiang_trip2_images/riding_horse.png" alt=""></p>
<p><img src=".xinjiang_trip2_images/ab3db0f8.png" alt=""></p>
<p>路边的一只小野狗，分点食物给它吃，看上去挺可怜的<br><img src=".xinjiang_trip2_images/dog_on_the_road.png" alt=""></p>
<p>路边的雪景<br><img src=".xinjiang_trip2_images/snow_env.png" alt=""></p>
<p>25元一碗的手抓牛肉饭。<br><img src=".xinjiang_trip2_images/shouzhuafan.png" alt=""><br>越往北走，消费越贵了，因为这边是热点旅游景区。虽然这边的景色会比<br>南疆更漂亮，这边的高山，草原湖水更多，但是这边非常利益，就是不像南边那么淳朴，这里都是<br>些商家，商人，我不喜欢</p>
<h2 id="新源县"><a href="#新源县" class="headerlink" title="新源县"></a>新源县</h2><p>我们开车进了一个叫新源县的地方，打算在这县城住下来。</p>
<p>晚上跳广场舞的人们<br><img src=".xinjiang_trip2_images/square_dance.png" alt=""></p>
<p>晚上街边的人不是很多, 这边治安应该还是非常好的，你经常可以看到有特警在街上<br><img src=".xinjiang_trip2_images/xinyuanxian.png" alt=""></p>
<h1 id="果子沟大桥"><a href="#果子沟大桥" class="headerlink" title="果子沟大桥"></a>果子沟大桥</h1><p>路过果子沟大桥，挺气派的<br><img src=".xinjiang_trip2_images/guozigou_bridge.png" alt=""></p>
<h1 id="博尔塔拉蒙古自治州"><a href="#博尔塔拉蒙古自治州" class="headerlink" title="博尔塔拉蒙古自治州"></a>博尔塔拉蒙古自治州</h1><p>这边很多哈萨克族，因为和哈萨克紧挨着</p>
<p>赛里木湖，因为要门票我就在旁边看了看没进去<br><img src=".xinjiang_trip2_images/sailimuhu.png" alt=""></p>
<p>景区停的特警装甲车<br><img src=".xinjiang_trip2_images/police_car.png" alt=""></p>
<p>这边没有水泥做的旅店，基本上都只能住当地人的那种帐篷<br>我们走在路边找住宿的时候，碰到一个哈萨克族老板在路边拉客，那老板很会拉客，一直说带我们<br>去看看他的帐篷，看看不要钱。我们就跟着去了。就这么在他那里住下来了。<br><img src=".xinjiang_trip2_images/tent.png" alt=""></p>
<p>附近挺多山的，安顿好行李我们趁着天还没黑就去附近爬爬山了。傍晚爬山的感觉非常惬意<br><img src=".xinjiang_trip2_images/moutain_view.png" alt=""></p>
<p><img src=".xinjiang_trip2_images/40c2312a.png" alt=""></p>
<p>山上有牛看着我们这些不速之客<br><img src=".xinjiang_trip2_images/cows.png" alt=""></p>
<p>晚上我们就在帐篷里吃饭，老板在旁边的小帐篷里做饭。在帐篷住的感觉就是有点不卫生，<br>被子啥的肯定不干净，不过入乡随俗吧。<br><img src=".xinjiang_trip2_images/tent_eating.png" alt=""></p>
<p>我们吃了一半只烤羊，这里住的大概是人均100吧，不是很贵。吃的老板就坑了我们，大概也是人均100多吧。<br>不过没关系，他们当地人只能靠旅游业来赚点钱吧。都不容易。<br><img src=".xinjiang_trip2_images/kazakh_boss.png" alt=""></p>
<p>晚餐<br><img src=".xinjiang_trip2_images/kazakh_dinner.png" alt=""></p>
<p>老板有个孙女非常可爱，眼睛非常大，而且眼睛颜色会发亮的<br><img src=".xinjiang_trip2_images/kid.png" alt=""></p>
<p>再次路过赛里木湖，漂亮吧, 这些照片都没有任何PS<br><img src=".xinjiang_trip2_images/sailimu.png" alt=""></p>
<h1 id="阿泰勒"><a href="#阿泰勒" class="headerlink" title="阿泰勒"></a>阿泰勒</h1><p>新疆最北的一个自治州。</p>
<h2 id="喀纳斯湖"><a href="#喀纳斯湖" class="headerlink" title="喀纳斯湖"></a>喀纳斯湖</h2><p>哈萨克边境的一个村庄<br><img src=".xinjiang_trip2_images/village.png" alt=""></p>
<p>我们又在当地人的一个tent里住了一晚，这边的帐篷更贵了，好像是人均200<br><img src=".xinjiang_trip2_images/tent2.png" alt=""></p>
<p>终于来到传说中的喀纳斯湖了，这边人特别多，感觉所有来新疆的旅客都集中在这个地方了，<br>看来新疆也有人口密集的地方。这里都是从全国各地来游客。大部分是旅行团直接拉过来的，<br>所以大叔大妈会很多。</p>
<p><img src=".xinjiang_trip2_images/kanas_travel.png" alt=""></p>
<p>这里特别冷，手拿出来拍个照都很容易冻僵。当时才是9月不到10月啊，听人说再过一个多月就会<br>更冷，这里会结冰，就不对外开放了。所以，现在正是旺季，人特别多。<br><img src=".xinjiang_trip2_images/kanas_travel2.png" alt=""></p>
<p><img src=".xinjiang_trip2_images/kanas_travel3.png" alt=""></p>
<p><img src=".xinjiang_trip2_images/kanas_travel4.png" alt=""></p>
<p>这里应该是新疆物价最贵的地方了,一碗肉抓饭高达40元</p>
<h2 id="禾木村"><a href="#禾木村" class="headerlink" title="禾木村"></a>禾木村</h2><p>也是一个比较热门的旅游景区，其实也没啥好看的。就一个村庄，吸引消费的。住的集体宿舍，还很贵的，又冷，晚上洗澡都很麻烦<br><img src=".xinjiang_trip2_images/hemu_village.png" alt=""></p>
<p><img src=".xinjiang_trip2_images/hemu_village2.png" alt=""></p>
<h1 id="昌吉"><a href="#昌吉" class="headerlink" title="昌吉"></a>昌吉</h1><p>一个在新疆里很现代化的城市, 离乌鲁木齐只有100多公里</p>
<p><img src=".xinjiang_trip2_images/changji_city.png" alt=""></p>
<h1 id="乌鲁木齐"><a href="#乌鲁木齐" class="headerlink" title="乌鲁木齐"></a>乌鲁木齐</h1><p>回到乌鲁木齐，只身一人去看看大巴扎吧。去看看新疆的大步行街。好多维吾尔族人。<br>长得都和中亚人很想，浓眉大眼睛，皮肤比黝黑。如果是我年轻几岁，我肯定回想留下来这里工作<br>找个当地的维吾尔妹子，哈哈</p>
<p><img src=".xinjiang_trip2_images/bazzar1.png" alt=""></p>
]]></content>
      <categories>
        <category>travel</category>
      </categories>
      <tags>
        <tag>travel</tag>
      </tags>
  </entry>
  <entry>
    <title>新疆北疆自驾12日游（上）</title>
    <url>/2018/xinjiang_trip.html</url>
    <content><![CDATA[<h1 id="出发"><a href="#出发" class="headerlink" title="出发"></a>出发</h1><p>辞职后已经有一段时间了，每天都是在家Remote着帮香港人干活。虽然钱不多，但很自由，当时想，要不要趁现在<br>有自由时间来一场旅行吧。想来想去还是决定在国内吧，第一，国内方便，不用另外去办护照。第二，国内便宜点，<br>还是穷啊。</p>
<p>新疆，国内是最异域风情的一片土地，对我来说一直很神秘。趁这次机会我去那边探个究竟。</p>
<p>从罗湖火车站正式出发。即将开始我48小时的火车旅程<br><img src="/images/2018/xinjiang_trip/678583b0.png" alt=""></p>
<p>我的床位。我将在这里睡两天两夜。我的舍友还挺好的。一个在新疆哈密上班的湖南大哥，一个区甘肃读书的年轻小伙子</p>
<p><img src="/images/2018/xinjiang_trip/bed_in_train.png" alt=""></p>
<p>在漫长的火车路途中，我除了玩手机电脑打发时间，就是和舍友聊天。我从来没去过中国的西北，就跟他们聊<br>西北那边的文化，主要聊得是那边吃的饮食吧。我对那里充满着憧憬。</p>
<p>做了两个晚上的火车，到兰州站歇歇，出来活动活动。<br><img src="/images/2018/xinjiang_trip/lanzhou_station.png" alt=""></p>
<h1 id="踏上新疆"><a href="#踏上新疆" class="headerlink" title="踏上新疆"></a>踏上新疆</h1><p>快到新疆的路上，看到路边已经没有什么绿色的植物了，都是辽阔的荒漠地带。<br><img src="/images/2018/xinjiang_trip/on_the_way.png" alt=""></p>
<p>第一晚住的乌鲁木齐市区的酒店，还不错。两百多吧<br><img src="/images/2018/xinjiang_trip/Urumqi_hotel.png" alt=""></p>
<p>乌鲁木齐市区。到了新疆移动网络都很差，因为这里的4G 被shutdown了。<br><img src="/images/2018/xinjiang_trip/Urumuqi_city_road.png" alt=""></p>
<p>第一餐吃的拉面。这边一碗面的价格跟广州深圳差不多吧，十几二十块一碗。但口味远远比广东的好吃。<br><img src="/images/2018/xinjiang_trip/first_lunch.png" alt=""></p>
<p>乌鲁木齐夜景，这边路上的人不是很多<br><img src="/images/2018/xinjiang_trip/Urumuqi_road_night.png" alt=""></p>
<h1 id="第一天景点游"><a href="#第一天景点游" class="headerlink" title="第一天景点游"></a>第一天景点游</h1><p>在乌鲁木齐独自闲逛了一天后的第二天，我和网友在指定的地点汇合了。我是在豆瓣上加的，上面会有很多人<br>寻找驴友伙伴的。<br>我们是一共4个人，2个女的来自广西，一个男的来自北京。一起在乌鲁木齐租了一辆CRV，在新疆租车还挺贵的，<br>CRV 用了10天大概每天要4-5百加上汽油费。</p>
<p>广西的一个妹子是女老司机，开车开得很好。超过一半时间都是她开的。另外，在新疆开车的感觉是路比较大，车<br>又不多，比较畅通。更重要的是风景很辽阔，视野很好。<br><img src="/images/2018/xinjiang_trip/driving_on_highway.png" alt=""></p>
<h2 id="火焰山"><a href="#火焰山" class="headerlink" title="火焰山"></a>火焰山</h2><p>我们看的一个大景点是火焰山。主要就是看一些西游记人物的雕像，没啥好看的，我也没买票，就在外面观望<br>了一下。<br><img src="/images/2018/xinjiang_trip/huoyanshan.png" alt=""></p>
<h2 id="一个城堡"><a href="#一个城堡" class="headerlink" title="一个城堡"></a>一个城堡</h2><p>然后是在吐鲁番的一个城堡里游览，忘了叫什么名。就拍个照片到此一游吧，我只是跟着来的哈<br><img src="/images/2018/xinjiang_trip/castle.png" alt=""></p>
<p>哦，这应该是阿凡提的家</p>
<p><img src="/images/2018/xinjiang_trip/apanti_home.png" alt=""></p>
<h2 id="吐鲁番市"><a href="#吐鲁番市" class="headerlink" title="吐鲁番市"></a>吐鲁番市</h2><p>吐鲁番市我在游逛新疆以来，最推荐的一个城市，原因很简单，这里物美价廉！哈密瓜满地都是，吃不完直接扔了。<br>吃得东西很好吃很正宗，又很便宜。<br>住宿就更能体现了，我们住了个两房，一晚上才一百多。还有一点我挺喜欢的是这边的空气湿度非常低，我衣服晚上洗，<br>洗完不用晒，第二天早上就干了。</p>
<p>路边一个可爱的小女孩<br><img src="/images/2018/xinjiang_trip/lovely_girl.png" alt=""></p>
<p>BTW, before you enter any city in Xinjiang, there is a serious security check, sometimes,<br>passengers in the car have to get off the car to pass the security check.<br><img src="/images/2018/xinjiang_trip/station_check.png" alt=""></p>
<p>wonderful landscape on dessert area<br><img src="/images/2018/xinjiang_trip/desert_area1.png" alt=""></p>
<p><img src="/images/2018/xinjiang_trip/dessert_area2.png" alt=""></p>
<p>骆驼，世界上最温顺的大家伙，很大很温柔<br><img src="/images/2018/xinjiang_trip/camel.png" alt=""></p>
<p>在新疆开车你会有不一样的体验。当然，开久了也会很无聊。<br><img src="/images/2018/xinjiang_trip/driving_on_empty_road.png" alt=""></p>
<h1 id="库车"><a href="#库车" class="headerlink" title="库车"></a>库车</h1><p>来到另外一个南部城市，库车。库车这城市我也挺喜欢的，因为和吐鲁番一样，都很物美价廉。<br><img src="/images/2018/xinjiang_trip/kuche.png" alt=""></p>
<p>在新疆，很少能吃到青菜，很多时候只能吃肉<br><img src="/images/2018/xinjiang_trip/meat.png" alt=""></p>
<p>那天停车在路边，本来想看看风景，突然闪电，在大自然面前，感觉到人类很渺小<br><img src="/images/2018/xinjiang_trip/wilderness.png" alt=""></p>
<p>这里的厕所简直了，很有味道。<br><img src="/images/2018/xinjiang_trip/publi_toilet.png" alt=""></p>
<p>在一个街边的露天市场里吃了个早餐。老板很友好很热情，旁边的大妈还给我们分享食物，虽然语言不通，<br>但能感受到他们很淳朴。说句心里话，我很向往这种生活，人与人之间没有提防，没有戒备，没有不信任感。<br>只因为这里太穷了，不像大城市里的生活那么复杂。我吃着这碗面也就几块钱，能赚个啥啊。为什么他们<br>还乐此不彼。 我觉得，富裕和淳朴是不能共存的。你富裕了必然会牺牲掉淳朴，人与人之间的关系变得<br>复杂。我喜欢淳朴，但不忍心看着他们如此贫穷，生活过得如此简陋。<br><img src="/images/2018/xinjiang_trip/street_food.png" alt=""></p>
<p><img src="/images/2018/xinjiang_trip/street_food2.png" alt=""></p>
<p><img src="/images/2018/xinjiang_trip/food_in_kuche.png" alt=""></p>
<p><img src="/images/2018/xinjiang_trip/fruit_stall.png" alt=""></p>
<p>一个峡谷的旅游景点</p>
<p><img src="/images/2018/xinjiang_trip/daxiagu_.png" alt=""><br>库车青旅里捡的弟弟。在库车，我们把原来那北京约的驴友给踢了，因为他太不好相处，总以为自己很牛。<br>哈哈，所以我们在青旅把他给踢了，另外拉了一个小伙子，后来果然气氛就不一样了，弟弟很随和，挺好相处的。</p>
<p>这里的水很清澈，看起来没有任何污染。<br><img src="/images/2018/xinjiang_trip/stream.png" alt=""></p>
<p>山路上经常会被羊群堵路，我真想下车去摸摸它们。<br><img src="/images/2018/xinjiang_trip/sheep_on_road.png" alt=""></p>
<p>九月中旬，可以看到，北疆已经下雪了<br><img src="/images/2018/xinjiang_trip/driving_on_the_way.png" alt=""></p>
<p>接下来我们的行程也会慢慢往北走，前往北疆探索。在下一章里，我讲继续分享北疆的旅途。</p>
]]></content>
      <categories>
        <category>travel</category>
      </categories>
      <tags>
        <tag>travel</tag>
      </tags>
  </entry>
  <entry>
    <title>Calcite介绍</title>
    <url>/2018/calcite_intro.html</url>
    <content><![CDATA[<h1 id="什么是Apache-Calcite"><a href="#什么是Apache-Calcite" class="headerlink" title="什么是Apache Calcite ?"></a>什么是Apache Calcite ?</h1><p>Apache Calcite 是一款开源SQL解析工具, 可以将各种SQL语句解析成抽象语法术AST(Abstract Syntax Tree),<br>之后通过操作AST就可以把SQL中所要表达的算法与关系体现在具体代码之中。</p>
<p>Calcite的生前为Optiq(也为Farrago), 为Java语言编写, 通过十多年的发展, 在2013年成为Apache旗下顶级项目，并还在持续发展中。<br>该项目的创始人为Julian Hyde, 其拥有多年的SQL引擎开发经验, 目前在Hortonworks工作, 主要负责Calcite项目的开发与维护。</p>
<p>目前, 使用Calcite作为SQL解析与处理引擎有Hive、Drill、Flink、Phoenix和Storm。<br>可以肯定的是还会有越来越多的数据处理引擎采用Calcite作为SQL解析工具。</p>
<h1 id="Calcite-主要功能"><a href="#Calcite-主要功能" class="headerlink" title="Calcite 主要功能"></a>Calcite 主要功能</h1><p>总结来说Calcite有以下主要功能：</p>
<ul>
<li>SQL 解析</li>
<li>SQL 校验</li>
<li>查询优化</li>
<li>SQL 生成器</li>
<li>数据连接</li>
</ul>
<h1 id="3-Calcite-解析SQl的步骤"><a href="#3-Calcite-解析SQl的步骤" class="headerlink" title="3. Calcite 解析SQl的步骤"></a>3. Calcite 解析SQl的步骤</h1><p><img src="/images/2018/calcite_intro/391eae29.png" alt=""><br>如上图中所述，一般来说Calcite解析SQL有以下几步:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Parser. 此步中Calcite通过Java CC将SQL解析成未经校验的AST</div><div class="line">Validate. 该步骤主要作用是校证Parser步骤中的AST是否合法,如验证SQL scheme、字段、函数等是否存在; SQL语句是否合法等. 此步完成之后就生成了RelNode树（关于RelNode树, 请参考下文）</div><div class="line">Optimize. 该步骤主要的作用优化RelNode树, 并将其转化成物理执行计划。主要涉及SQL规则优化如:基于规则优化(RBO)及基于代价(CBO)优化; Optimze 这一步原则上来说是可选的, 通过Validate后的RelNode树已经可以直接转化物理执行计划，但现代的SQL解析器基本上都包括有这一步，目的是优化SQL执行计划。此步得到的结果为物理执行计划。</div><div class="line">Execute，即执行阶段。此阶段主要做的是:将物理执行计划转化成可在特定的平台执行的程序。如Hive与Flink都在在此阶段将物理执行计划CodeGen生成相应的可执行代码。</div></pre></td></tr></table></figure>
<h1 id="Calcite相关组件"><a href="#Calcite相关组件" class="headerlink" title="Calcite相关组件"></a>Calcite相关组件</h1><p>Calcite主要有以下概念：</p>
<p>Catelog: 主要定义SQL语义相关的元数据与命名空间。<br>SQL parser: 主要是把SQL转化成AST.<br>SQL validator: 通过Catalog来校证AST.<br>Query optimizer: 将AST转化成物理执行计划、优化物理执行计划.<br>SQL generator: 反向将物理执行计划转化成SQL语句.</p>
<h2 id="category"><a href="#category" class="headerlink" title="category"></a>category</h2><p>Catalog:主要定义被SQL访问的命名空间，主要包括以下几点：</p>
<p>schema: 主要定义schema与表的集合，schame 并不是强制一定需要的，比如说有两张同名的表T1, T2，就需要schema要区分这两张表，如A.T1, B.T1<br>表:对应关系数据库的表，代表一类数据，在calcite中由RelDataType定义<br>RelDataType 代表表的数据定义，如表的数据列名称、类型等。</p>
<p>一句Sql<br><figure class="highlight sql"><table><tr><td class="code"><pre><div class="line">selcct id, name, cast(age as bigint) from A.INFO</div></pre></td></tr></table></figure></p>
<p>id, name则为data type field<br>bigint为 data type<br>A 为schema<br>INFO 为表</p>
<h2 id="SQL-Parser"><a href="#SQL-Parser" class="headerlink" title="SQL Parser"></a>SQL Parser</h2><p>由Java CC编写，将SQL转化成AST.</p>
<p>Java CC 指的是Java Compiler Compiler, 可以将一种特定域相关的语言转化成Java语言<br>在Calcite中将标记（Token)表示为 SqlNode, 并且Sqlnode可以通过unparse方法反向转化成SQL<br>cast(id as float)<br>Java CC 可表示为</p>
<p><cast></cast></p>
<p><lparen><br>e = Expression(ExprContext.ACCEPT_SUBQUERY)</lparen></p>
<p><as><br>dt = DataType() {agrs.add(dt);}</as></p>
<p><rparen><br>….</rparen></p>
<h2 id="Query-Optimizer"><a href="#Query-Optimizer" class="headerlink" title="Query Optimizer"></a>Query Optimizer</h2><p>首先看一下<br><figure class="highlight sql"><table><tr><td class="code"><pre><div class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> tmp_node</div><div class="line"><span class="keyword">SELECT</span> s1.id1, s1.id2, s2.val1</div><div class="line"><span class="keyword">FROM</span> source1 <span class="keyword">as</span> s1 <span class="keyword">INNER</span> <span class="keyword">JOIN</span> source2 <span class="keyword">AS</span> s2</div><div class="line"><span class="keyword">ON</span> s1.id1 = s2.id1 <span class="keyword">and</span> s1.id2 = s2.id2 <span class="keyword">where</span> s1.val1 &gt; <span class="number">5</span> <span class="keyword">and</span> s2.val2 = <span class="number">3</span>;</div></pre></td></tr></table></figure></p>
<p>通过Calcite转化为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">LogicalTableModify(table=[[TMP_NODE]], operation=[INSERT], flattened=[false])</div><div class="line">  LogicalProject(ID1=[$0], ID2=[$1], VAL1=[$7])</div><div class="line">    LogicalFilter(condition=[AND(&gt;($2, 5), =($8, 3))])</div><div class="line">      LogicalJoin(condition=[AND(=($0, $5), =($1, $6))], joinType=[INNER])</div><div class="line">        LogicalTableScan(table=[[SOURCE1]])</div><div class="line">        LogicalTableScan(table=[[SOURCE2]])</div></pre></td></tr></table></figure></p>
<p>是未经优化的RelNode树，可以发现最底层是TableScan，也是读取表的原始数据，紧接着是LogicalJoin,Joiner的类型为INNER JOIN, LogicalJoin之后接下做LogicalFilter 操作，对应SQL中的WHERE条件，最后做Project也就是投影操作。</p>
<p>但是我们可以观察到对于INNER JOIN而言, WHERE 条件是可以下推，如<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">LogicalTableModify(table=[[TMP_NODE]], operation=[INSERT], flattened=[false])</div><div class="line">  LogicalProject(ID1=[$0], ID2=[$1], VAL1=[$7])</div><div class="line">      LogicalJoin(condition=[AND(=($0, $5), =($1, $6))], joinType=[inner])</div><div class="line">        LogicalFilter(condition=[=($4, 3)])  </div><div class="line">          LogicalProject(ID1=[$0], ID2=[$1],      ID3=[$2], VAL1=[$3], VAL2=[$4],VAL3=[$5])</div><div class="line">            LogicalTableScan(table=[[SOURCE1]])</div><div class="line">        LogicalFilter(condition=[&gt;($3,5)])    </div><div class="line">          LogicalProject(ID1=[$0], ID2=[$1], ID3=[$2], VAL1=[$3], VAL2=[$4],VAL3=[$5])</div><div class="line">            LogicalTableScan(table=[[SOURCE2]])</div></pre></td></tr></table></figure></p>
<p>这样可以减少JOIN的数据量，提高SQL效率</p>
<p>实际过程中可以将JOIN 的中条件下推以较少Join的数据量<br><figure class="highlight sql"><table><tr><td class="code"><pre><div class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> tmp_node</div><div class="line"><span class="keyword">SELECT</span> s1.id1, s1.id2, s2.val1</div><div class="line"><span class="keyword">FROM</span> source1 <span class="keyword">as</span> s1 <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> source2 <span class="keyword">AS</span> s2</div><div class="line"><span class="keyword">ON</span> s1.id1 = s2.id1 <span class="keyword">and</span> s1.id2 = s2.id2 <span class="keyword">and</span> s1.id3 = <span class="number">5</span></div><div class="line">s1.id3 = <span class="number">5</span></div></pre></td></tr></table></figure></p>
<p>这个条件可以先下推过滤s1中的数据, 但在特定场景下，有些不能下推，如下sql:<br><figure class="highlight sql"><table><tr><td class="code"><pre><div class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> tmp_node</div><div class="line"><span class="keyword">SELECT</span> s1.id1, s1.id2, s2.val1</div><div class="line"><span class="keyword">FROM</span> source1 <span class="keyword">as</span> s1 <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> source2 <span class="keyword">AS</span> s2</div><div class="line"><span class="keyword">ON</span> s1.id1 = s2.id1 <span class="keyword">and</span> s1.id2 = s2.id2 <span class="keyword">and</span> s2.id3 = <span class="number">5</span></div></pre></td></tr></table></figure></p>
<p>如果s1,s2是流式表（动态表，请参考Flink流式概念）的话，就不能下推，因为s1下推的话，由于过滤后没有数据驱动join操作，因而得不到想要的结果（详见Flink/Sparking-Streaming)</p>
<p>那接下来我们可能有一个疑问，在什么情况下可以做类似下推、上推操作，又是根据什么原则进行的呢？如下图所示</p>
<p><img src="/images/2018/calcite_intro/43aec32e.png" alt=""><br><figure class="highlight sql"><table><tr><td class="code"><pre><div class="line">T1 JOIN T2 JOIN T3</div></pre></td></tr></table></figure></p>
<p>类似于此种情况JOIN的顺序是上图的前者还是后者？这就涉及到Optimizer所使用的方法，Optimizer主要目的就是减小SQL所处理的数据量、减少所消耗的资源并最大程度提高SQL执行效率如:剪掉无用的列、合并投影、子查询转化成JOIN、JOIN重排序、下推投影、下推过滤等。目前主要有两类优化方法:基于语法(RBO)与基于代价(CBO)的优化</p>
<p>RBO(Rule Based Optimization)<br>通俗一点的话就是事先定义一系列的规则，然后根据这些规则来优化执行计划。<br>如</p>
<p>ProjectFilterRule</p>
<p>此Rule的使用场景为Filter在Project之上，可以将Filter下推。假如某一个RelNode树<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">LogicalFilter</div><div class="line">  LogicalProject</div><div class="line">    LogicalTableScan</div></pre></td></tr></table></figure></p>
<p>则可优化成<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">    LogicalProject</div><div class="line">      LogicalFilter</div><div class="line">        LogicalTableScan</div><div class="line">FilterJoinRule</div></pre></td></tr></table></figure></p>
<p>此Rule的使用场景为Filter在Join之上，可以先做Filter然后再做Join, 以减少Join的数量</p>
<p>等等，还有很多类似的规则。但RBO一定程度上是经验试的优化方法，无法有一个公式上的判断哪种优化更优。 在Calcite中实现方法为 HepPlanner</p>
<p>CBO(Cost Based Optimization)<br>通俗一点的说法是：通过某种算法计算SQL所有可能的执行计划的“代价”，选择某一个代价较低的执行计划，如上文中三张表作JOIN, 一般来说RBO无法判断哪种执行计划优化更好，只有分别计算每一种JOIN方法的代价。</p>
<p>Calcite会将每一种操作（如LogicaJoin、LocialFilter、 LogicalProject、LogicalScan) 结合实际的Schema转化成具体的代价数，比较不同的执行计划所具有的代价，然后选择相对小计划作为最终的结果，之所以说相对小，这是因为如果要完全遍历计算所有可能的代价可能得不偿失，花费更多的人力与资源，因此只是说选择相对最优的执行计划。CBO目的是“避免使用最差的执行计划，而不是找到最好的”</p>
<p>目前Calcite中就是采用CBO进行优化,实现方法为VolcanoPlanner,有关此算法的具体内容可以参考原码</p>
<ol>
<li>如何使用Calcite<br>由于Calcite是Java语言编写，因此只需要在工程或项目中引入相应的Jar包即可，下面为一个可以运行的例子：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">public class TestOne &#123;</div><div class="line">    public static class TestSchema &#123;</div><div class="line">        public final Triple[] rdf = &#123;new Triple(&quot;s&quot;, &quot;p&quot;, &quot;o&quot;)&#125;;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    public static void main(String[] args) &#123;</div><div class="line">        SchemaPlus schemaPlus = Frameworks.createRootSchema(true);</div><div class="line">        </div><div class="line">        //给schema T中添加表</div><div class="line">        schemaPlus.add(&quot;T&quot;, new ReflectiveSchema(new TestSchema()));</div><div class="line">        Frameworks.ConfigBuilder configBuilder = Frameworks.newConfigBuilder();</div><div class="line">        //设置默认schema</div><div class="line">        configBuilder.defaultSchema(schemaPlus);</div><div class="line"></div><div class="line">        FrameworkConfig frameworkConfig = configBuilder.build();</div><div class="line"></div><div class="line">        SqlParser.ConfigBuilder paresrConfig = SqlParser.configBuilder(frameworkConfig.getParserConfig());</div><div class="line">        </div><div class="line">        //SQL 大小写不敏感</div><div class="line">        paresrConfig.setCaseSensitive(false).setConfig(paresrConfig.build());</div><div class="line"></div><div class="line">        Planner planner = Frameworks.getPlanner(frameworkConfig);</div><div class="line"></div><div class="line">        SqlNode sqlNode;</div><div class="line">        RelRoot relRoot = null;</div><div class="line">        try &#123;</div><div class="line">            //parser阶段</div><div class="line">            sqlNode = planner.parse(&quot;select \&quot;a\&quot;.\&quot;s\&quot;, count(\&quot;a\&quot;.\&quot;s\&quot;) from \&quot;T\&quot;.\&quot;rdf\&quot; \&quot;a\&quot; group by \&quot;a\&quot;.\&quot;s\&quot;&quot;);</div><div class="line">            //validate阶段</div><div class="line">            planner.validate(sqlNode);</div><div class="line">            //获取RelNode树的根</div><div class="line">            relRoot = planner.rel(sqlNode);</div><div class="line">        &#125; catch (Exception e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        RelNode relNode = relRoot.project();</div><div class="line">        System.out.print(RelOptUtil.toString(relNode));</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>类Triple 对应的表定义:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">public class Triple &#123;</div><div class="line">    public String s;</div><div class="line">    public String p;</div><div class="line">    public String o;</div><div class="line"></div><div class="line">    public Triple(String s, String p, String o) &#123;</div><div class="line">        super();</div><div class="line">        this.s = s;</div><div class="line">        this.p = p;</div><div class="line">        this.o = o;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="Calcite-其它方面"><a href="#Calcite-其它方面" class="headerlink" title="Calcite 其它方面"></a>Calcite 其它方面</h1><p>Calcite的功能远不止以上介绍，除了标准SQL的，还支持以下内容：</p>
<p>对流相对概念支持，如在SQL层面支持Window概念,如Session Window, Hopping Window等。<br>支持物化视图等复杂概念。<br>独立于编程语言和数据源，可以支持不同的前端和后端。</p>
<p>值得一提的是，Calcite支持异构数据源查询，比如数据存在es和mysql，可以通过写sql join之类的操作，<br>让calcite分别先从不同的数据源查询数据，然后再在内存里进行合并计算；<br>另外，它本身提供了许多优化规则，也支持我们自定义优化规则，来优化整个查询。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>以上内容主要介绍上Calcite相关概念并通过相例子说明了Calcite使用方法, 希望通过上述内容，读者能对Calcite有初步的了解。</p>
<p>由于笔者使用和探索Calcite时间也不长，以上内容难免有错误与不准确之处，还望各位读者不吝指正，相互学习。</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.jianshu.com/p/2dfbd71b7f0f" target="_blank" rel="external">https://www.jianshu.com/p/2dfbd71b7f0f</a><br><a href="https://www.infoq.cn/article/new-big-data-hadoop-query-engine-apache-calcite/" target="_blank" rel="external">https://www.infoq.cn/article/new-big-data-hadoop-query-engine-apache-calcite/</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Calcite</tag>
      </tags>
  </entry>
  <entry>
    <title>Git stash的用处</title>
    <url>/2018/git_stash.html</url>
    <content><![CDATA[<h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>1 当正在dev分支上开发某个项目，这时项目中出现一个bug，需要紧急修复，<br>但是正在开发的内容只是完成一半，还不想提交，这时可以用git stash命令将修改的内容保存至堆栈区，<br>然后顺利切换到hotfix分支进行bug修复，修复完成后，再次切回到dev分支，从堆栈中恢复刚刚保存的内容。</p>
<p>2 由于疏忽，本应该在dev分支开发的内容，却在master上进行了开发，需要重新切回到dev分支上进行开发，<br>可以用git stash将内容保存至堆栈中，切回到dev分支后，再次恢复内容即可。</p>
<p>总的来说，git stash命令的作用就是将目前还不想提交的但是已经修改的内容进行保存至堆栈中，后续可以在某个分支上恢复出堆栈中的内容。<br>这也就是说，stash中的内容不仅仅可以恢复到原先开发的分支，也可以恢复到其他任意指定的分支上。<br>git stash作用的范围包括工作区和暂存区中的内容，也就是说没有提交的内容都会保存至堆栈中。</p>
<h2 id="命令详解"><a href="#命令详解" class="headerlink" title="命令详解"></a>命令详解</h2><h3 id="1-git-stash"><a href="#1-git-stash" class="headerlink" title="1 git stash"></a>1 git stash</h3><p>能够将所有未提交的修改（工作区和暂存区）保存至堆栈中，用于后续恢复当前工作目录。<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$ git status</div><div class="line">On branch master</div><div class="line">Changes not staged for commit:</div><div class="line">  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</div><div class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</div><div class="line"></div><div class="line">        modified:   src/main/java/com/wy/CacheTest.java</div><div class="line">        modified:   src/main/java/com/wy/StringTest.java</div><div class="line"></div><div class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</div><div class="line"></div><div class="line">$ git stash</div><div class="line">Saved working directory and index state WIP on master: b2f489c second</div><div class="line"></div><div class="line">$ git status</div><div class="line">On branch master</div><div class="line">nothing to commit, working tree clean</div></pre></td></tr></table></figure></p>
<h3 id="2-git-stash-save"><a href="#2-git-stash-save" class="headerlink" title="2 git stash save"></a>2 git stash save</h3><p>作用等同于git stash，区别是可以加一些注释，如下：<br>git stash的效果：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">stash@&#123;0&#125;: WIP on master: b2f489c second</div></pre></td></tr></table></figure></p>
<p>git stash save “test1”的效果：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">stash@&#123;0&#125;: On master: test1</div></pre></td></tr></table></figure></p>
<h3 id="3-git-stash-list"><a href="#3-git-stash-list" class="headerlink" title="3. git stash list"></a>3. git stash list</h3><p>查看当前stash中的内容</p>
<h3 id="4-git-stash-pop"><a href="#4-git-stash-pop" class="headerlink" title="4 git stash pop"></a>4 git stash pop</h3><p>将当前stash中的内容弹出，并应用到当前分支对应的工作目录上。<br>注：该命令将堆栈中最近保存的内容删除（栈是先进后出）<br>顺序执行git stash save “test1”和git stash save “test2”命令，效果如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$ git stash list</div><div class="line">stash@&#123;0&#125;: On master: test2</div><div class="line">stash@&#123;1&#125;: On master: test1</div><div class="line"></div><div class="line"></div><div class="line">$ git stash pop</div><div class="line">On branch master</div><div class="line">Changes not staged for commit:</div><div class="line">  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</div><div class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</div><div class="line"></div><div class="line">        modified:   src/main/java/com/wy/StringTest.java</div><div class="line"></div><div class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</div><div class="line">Dropped refs/stash@&#123;0&#125; (afc530377eacd4e80552d7ab1dad7234edf0145d)</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$ git stash list</div><div class="line">stash@&#123;0&#125;: On master: test1</div></pre></td></tr></table></figure>
<p>可见，test2的stash是首先pop出来的。<br>如果从stash中恢复的内容和当前目录中的内容发生了冲突，也就是说，恢复的内容和当前目录修改了同一行的数据，那么会提示报错，需要解决冲突，可以通过创建新的分支来解决冲突。</p>
<h3 id="5-git-stash-apply"><a href="#5-git-stash-apply" class="headerlink" title="5 git stash apply"></a>5 git stash apply</h3><p>将堆栈中的内容应用到当前目录，不同于git stash pop，该命令不会将内容从堆栈中删除，也就说该命令能够将堆栈的内容多次应用到工作目录中，适应于多个分支的情况。<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$ git stash apply</div><div class="line">On branch master</div><div class="line">Changes not staged for commit:</div><div class="line">  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</div><div class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</div><div class="line"></div><div class="line">        modified:   src/main/java/com/wy/StringTest.java</div><div class="line"></div><div class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$ git stash list</div><div class="line">stash@&#123;0&#125;: On master: test2</div><div class="line">stash@&#123;1&#125;: On master: test1</div><div class="line">堆栈中的内容并没有删除。</div></pre></td></tr></table></figure>
<p>可以使用git stash apply + stash名字（如stash@{1}）指定恢复哪个stash到当前的工作目录。</p>
<h3 id="6-git-stash-drop-名称"><a href="#6-git-stash-drop-名称" class="headerlink" title="6 git stash drop + 名称"></a>6 git stash drop + 名称</h3><p>从堆栈中移除某个指定的stash</p>
<h3 id="7-git-stash-clear"><a href="#7-git-stash-clear" class="headerlink" title="7 git stash clear"></a>7 git stash clear</h3><p>清除堆栈中的所有 内容</p>
<h3 id="8-git-stash-show"><a href="#8-git-stash-show" class="headerlink" title="8 git stash show"></a>8 git stash show</h3><p>查看堆栈中最新保存的stash和当前目录的差异。<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$ git stash show</div><div class="line"> src/main/java/com/wy/StringTest.java | 2 +-</div><div class="line">  file changed, 1 insertion(+), 1 deletion(-)</div></pre></td></tr></table></figure></p>
<p>git stash show stash@{1}查看指定的stash和当前目录差异。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://blog.csdn.net/stone_yw/article/details/80795669" target="_blank" rel="external">https://blog.csdn.net/stone_yw/article/details/80795669</a></p>
]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Python unittest来测试项目</title>
    <url>/2018/python_unittest.html</url>
    <content><![CDATA[<h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>比如，我在Airflow上新增了一个feature，我想测试一下，就需要在tests目录下新增一个功能测试。<br>目录结构大概是这样的：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">├── airflow</div><div class="line">│   ├── model.py</div><div class="line">│   ├── test</div><div class="line">│   │   ├── test_model.py</div></pre></td></tr></table></figure></p>
<p>测试代码 test_model.py如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">import unittest</div><div class="line">from model import BaseModel</div><div class="line"></div><div class="line">class TestModel(unittest.TestCase):</div><div class="line">    def test1(self):</div><div class="line">         print(&apos;here is the test logic&apos;)</div></pre></td></tr></table></figure></p>
<p>来主目录下，可以通过运行如下命令来启动测试:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">python -m unittest test.test_model</div></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu的.so文件问题</title>
    <url>/2018/ubuntu_shared_object.html</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>昨天在anaconda的虚拟环境py3 下装uwsgi</p>
<p>但执行uwsgi命令的时候报错提示：<br>uwsgi: error while loading shared libraries: libpcre.so.1: cannot open…</p>
<p>找不到libpcre.so.1 这个动态链接库。</p>
<p>这个东西叫做 动态链接库。</p>
<ul>
<li>An .so file is a compiled library file. It stands for “Shared Object” and is analogous to a Windows DLL</li>
</ul>
<h3 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h3><p>首先我学习了几个linux命令：</p>
<ol>
<li>ldd命令用于打印程序或者库文件所依赖的共享库列表。<br>2.ldconfig命令的用途主要是在默认搜寻目录/lib和/usr/lib以及动态库配置文件/etc/ld.so.conf内所列的目录下，搜索出可共享的动态链接库（格式如lib<em>.so</em>）,进而创建出动态装入程序(ld.so)所需的连接和缓存文件。<br>3.locate 让使用者可以很快速的搜寻档案系统内是否有指定的档案。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ldconfig /opt/anaconda3/lib/</div><div class="line">ldd /data/software/anaconda2/bin/uwsgi</div><div class="line">locate libcrypto.so.1.0.0</div><div class="line"></div><div class="line">libcrypto.so.1.0.0 =&gt; /data/software/anaconda2/lib/libcrypto.so.1.0.0的  (0x00007f90b5695000)</div><div class="line">表示libcrypto.so.1.0.0 用的是 /data/software/anaconda2/lib/libcrypto.so.1.0.0的 动态 链接库</div><div class="line">用locate libcrypto.so.1.0.0 查看系统一共有几个 libcrypto.so.1.0.0 动态链接库</div></pre></td></tr></table></figure>
<p>ldconfig的几点注意事项：</p>
<ol>
<li>往/lib和/usr/lib里面加东西，是不用修改/etc/ld.so.conf的，但是完了之后要调一下ldconfig，不然这个library会找不到。</li>
<li>想往上面两个目录以外加东西的时候，一定要修改/etc/ld.so.conf，然后再调用ldconfig，不然也会找不到。</li>
<li>比如安装了一个mysql到/usr/local/mysql，mysql有一大堆library在/usr/local/mysql/lib下面，这时就需要在/etc/ld.so.conf下面加一行/usr/local/mysql/lib，保存过后ldconfig一下，新的library才能在程序运行时被找到。</li>
<li>如果想在这两个目录以外放lib，但是又不想在/etc/ld.so.conf中加东西（或者是没有权限加东西）。那也可以，就是export一个全局变量LD_LIBRARY_PATH，然后运行程序的时候就会去这个目录中找library。一般来讲这只是一种临时的解决方案，在没有权限或临时需要的时候使用</li>
</ol>
<p>in my case:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">add the  line below in /etc/profile:</div><div class="line">export LD_LIBRARY_PATH=/data/software/anaconda2/lib</div><div class="line"></div><div class="line">ldconfig /lib/x86_64-linux-gnu/</div></pre></td></tr></table></figure></p>
<p>finished!</p>
<blockquote>
<p>另外 LD_LIBRARY_PATH的优先级是最高的。</p>
</blockquote>
<p>The order is documented in the manual of the dynamic linker, which is ld.so. It is:</p>
<ol>
<li>directories from LD_LIBRARY_PATH;</li>
<li>directories from /etc/ld.so.conf;</li>
<li>/lib;</li>
<li>/usr/lib.<br>(I’m simplifying a little, see the manual for the full details.)</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">The order makes sense when you consider that it&apos;s the only way to override a library in a default location with a custom library.</div><div class="line">LD_LIBRARY_PATH is a user setting, it has to come before the others. /etc/ld.so.conf is a local setting, it comes before the operating system default.</div><div class="line">So as a user, if I want to run a program with a different version of a library, I can run the program with LD_LIBRARY_PATH containing the location of that different library version.</div><div class="line">And as an administrator, I can put a different version of the library in /usr/local/lib and list /usr/local/lib in /etc/ld.so.conf.</div></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive UDF 快速教程</title>
    <url>/2018/hive_udf.html</url>
    <content><![CDATA[<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="Step1"><a href="#Step1" class="headerlink" title="Step1"></a>Step1</h3><p>首先，我们创建一个目录 udf_test/;<br>创建子目录org/dennis/udf<br>在子目录里创建一个MyUpper.java文件。里面内容为：<br><figure class="highlight java"><table><tr><td class="code"><pre><div class="line"><span class="keyword">package</span> org.dennis.udf;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">MyUpper</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(<span class="keyword">final</span> Text s)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (s == <span class="keyword">null</span>) &#123; <span class="keyword">return</span> <span class="keyword">null</span>; &#125;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Text(s.toString().toUpperCase());</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="Step2"><a href="#Step2" class="headerlink" title="Step2"></a>Step2</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">javac -cp  \</div><div class="line">/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/jars/hive-exec-2.1.1-cdh6.2.0.jar: \</div><div class="line">/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/jars/hadoop-common-3.0.0-cdh6.2.0.jar \</div><div class="line">org/dennis/udf/MyUpper.java</div></pre></td></tr></table></figure>
<p>这里我们要把依赖的jar包写进来，这里我们依赖hive-exec*.jar 以及 hadoop-common.jar</p>
<ul>
<li>javac -cp 的作用  <figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">javac -cp 指明了.java文件里import的类的位置</div><div class="line"></div><div class="line">java -cp 指明了执行这个class文件所需要的所有类的包路径-即系统类加载器的路径（涉及到类加载机制）</div><div class="line"></div><div class="line">路径在linux中用：隔开  在windows中用；隔开</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Step3"><a href="#Step3" class="headerlink" title="Step3"></a>Step3</h3><p>生成一个jar文件。<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">jar -cf myudfs.jar  -C . .</div><div class="line">会在当前目录下生成myudfs.jar 文件</div></pre></td></tr></table></figure></p>
<h3 id="Step4"><a href="#Step4" class="headerlink" title="Step4"></a>Step4</h3><h4 id="第一种方式"><a href="#第一种方式" class="headerlink" title="第一种方式"></a>第一种方式</h4><p>进入Hive命令行<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">hive&gt; add jar myudfs.jar;</div><div class="line">Added [myudfs.jar] to class path</div><div class="line">Added resources: [myudfs.jar]</div><div class="line">hive&gt; create temporary function dennisUpper as &apos;org.dennis.udf.MyUpper&apos;;</div><div class="line">OK</div><div class="line">Time taken: 0.067 seconds</div><div class="line">hive&gt;</div></pre></td></tr></table></figure></p>
<p>搞定！</p>
<h4 id="第二种方式"><a href="#第二种方式" class="headerlink" title="第二种方式"></a>第二种方式</h4><p>另外，也可以在Hue上点击 setting后，上传对应的jar文件，然后将对应的function name 和 class name也填一下就OK了。<br><img src="/images/2018/hive_udf/hue_udf_upload.jpg" alt="Sample Image Added via Markdown"></p>
<p>然后再Hue上执行<code>create temporary function dennisUpper as &#39;org.dennis.udf.MyUpper&#39;;</code>  注册下对应信息。</p>
<h3 id="Step5"><a href="#Step5" class="headerlink" title="Step5"></a>Step5</h3><p>执行SQL，检验一下:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">select dennisUpper(field) from table;</div></pre></td></tr></table></figure></p>
<h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><h3 id="找不到类"><a href="#找不到类" class="headerlink" title="找不到类"></a>找不到类</h3><p>有时候我们add jar后，create function时候会出现如下报错：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Added resources: [myudfs.jar]</div><div class="line">hive&gt; create temporary function dennisUpper as &apos;org.dennis.udf.MyUpper&apos;;</div><div class="line">FAILED: Class org.dennis.udf.MyUpper not found</div><div class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.FunctionTask</div></pre></td></tr></table></figure></p>
<p>我们可以解压一下jar包检查下:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">unzip myudf.jar -d check_jar</div><div class="line">解压到 check_jar目录</div></pre></td></tr></table></figure></p>
<p>如果有的话，check_dir下应该是有这么一个目录文件:<br><code>org/dennis/udf/MyUpper.class</code></p>
<h3 id="reload更新的jar包"><a href="#reload更新的jar包" class="headerlink" title="reload更新的jar包"></a>reload更新的jar包</h3><p>我们更新了UDF，重新打个jar包替换掉原来的。但如果要让它生效需要做一些操作：</p>
<ol>
<li><p>added a config in hive-site.xml, then restart the hive server.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.reloadable.aux.jars.path&lt;/name&gt;</div><div class="line">    &lt;value&gt;/user/hive/udf&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
</li>
<li><p>deleted the old jar file in HDFS, and upload the new jar file.</p>
</li>
<li><p>DROP TEMPORARY FUNCTION IF EXISTS isstopword;</p>
</li>
<li><p>in hive console, run <code>list jar;</code> to check the local jar files, it would print something like this:</p>
</li>
</ol>
<p><code>/tmp/83ce8586-7311-4e97-813f-f2fbcec63a55_resources/isstopwordudf.jar</code><br>then delete them in your server file system.</p>
<ol>
<li>create a temp function again.<br>create temporary function isstopword as ‘org.dennis.udf.IsStopWord’;</li>
</ol>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="http://bdlabs.edureka.co/static/help/topics/cm_mc_hive_udf.html#concept_zb2_rxr_lw_unique_1" target="_blank" rel="external">http://bdlabs.edureka.co/static/help/topics/cm_mc_hive_udf.html#concept_zb2_rxr_lw_unique_1</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>UDF</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper的用途</title>
    <url>/2018/zookeeper_service.html</url>
    <content><![CDATA[<p>数据发布与订阅（配置中心）发布与订阅模型，即所谓的配置中心，顾名思义就是发布者将数据发布到ZK节点上，供订阅者动态获取数据，实现配置信息的集中式管理和动态更新。<br>例如全局的配置信息，服务式服务框架的服务地址列表等就非常适合使用。应用中用到的一些配置信息放到ZK上进行集中管理。<br>这类场景通常是这样：应用在启动的时候会主动来获取一次配置，同时，在节点上注册一个Watcher，这样一来，以后每次配置有更新的时候，都会实时通知到订阅的客户端，从来达到获取最新配置信息的目的。<br>分布式搜索服务中，索引的元信息和服务器集群机器的节点状态存放在ZK的一些指定节点，供各个客户端订阅使用。分布式日志收集系统。这个系统的核心工作是收集分布在不同机器的日志。</p>
<p>收集器通常是按照应用来分配收集任务单元，因此需要在ZK上创建一个以应用名作为path的节点P，并将这个应用的所有机器ip，以子节点的形式注册到节点P上，这样一来就能够实现机器变动的时候，能够实时通知到收集器调整任务分配。<br>系统中有些信息需要动态获取，并且还会存在人工手动去修改这个信息的发问。通常是暴露出接口，例如JMX接口，来获取一些运行时的信息。引入ZK之后，就不用自己实现一套方案了，只要将这些信息存放到指定的ZK节点上即可。<br>注意：在上面提到的应用场景中，有个默认前提是：数据量很小，但是数据更新可能会比较快的场景。负载均衡这里说的负载均衡是指软负载均衡。<br>在分布式环境中，为了保证高可用性，通常同一个应用或同一个服务的提供方都会部署多份，达到对等服务。而消费者就须要在这些对等的服务器中选择一个来执行相关的业务逻辑，其中比较典型的是消息中间件中的生产者，消费者负载均衡。消息中间件中发布者和订阅者的负载均衡，linkedin开源的KafkaMQ和阿里开源的metaq都是通过zookeeper来做到生产者、消费者的负载均衡。<br>这里以metaq为例如讲下：<br>生产者负载均衡：metaq发送消息的时候，生产者在发送消息的时候必须选择一台broker上的一个分区来发送消息，因此metaq在运行过程中，会把所有broker和对应的分区信息全部注册到ZK指定节点上，默认的策略是一个依次轮询的过程，生产者在通过ZK获取分区列表之后，会按照brokerId和partition的顺序排列组织成一个有序的分区列表，发送的时候按照从头到尾循环往复的方式选择一个分区来发送消息。</p>
<p>消费负载均衡：在消费过程中，一个消费者会消费一个或多个分区中的消息，但是一个分区只会由一个消费者来消费。<br>MetaQ的消费策略是：每个分区针对同一个group只挂载一个消费者。如果同一个group的消费者数目大于分区数目，则多出来的消费者将不参与消费。如果同一个group的消费者数目小于分区数目，则有部分消费者需要额外承担消费任务。<br>在某个消费者故障或者重启等情况下，其他消费者会感知到这一变化（通过 zookeeper watch消费者列表），然后重新进行负载均衡，保证所有的分区都有消费者进行消费。</p>
<h3 id="命名服务-Naming-Service"><a href="#命名服务-Naming-Service" class="headerlink" title="命名服务(Naming Service)"></a>命名服务(Naming Service)</h3><p>命名服务也是分布式系统中比较常见的一类场景。在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，提供者等信息。被命名的实体通常可以是集群中的机器，提供的服务地址，远程对象等等——这些我们都可以统称他们为名字（Name）。其中较为常见的就是一些分布式服务框架中的服务地址列表。<br>通过调用ZK提供的创建节点的API，能够很容易创建一个全局唯一的path，这个path就可以作为一个名称。阿里巴巴集团开源的分布式服务框架Dubbo中使用ZooKeeper来作为其命名服务，维护全局的服务地址列表，点击这里查看Dubbo开源项目。<br>在Dubbo实现中：服务提供者在启动的时候，向ZK上的指定节点/dubbo/${serviceName}/providers目录下写入自己的URL地址，这个操作就完成了服务的发布。<br>服务消费者启动的时候，订阅/dubbo/${serviceName}/providers目录下的提供者URL地址， 并向/dubbo/${serviceName} /consumers目录下写入自己的URL地址。注意，所有向ZK上注册的地址都是临时节点，这样就能够保证服务提供者和消费者能够自动感应资源的变化。<br>另外，Dubbo还有针对服务粒度的监控，方法是订阅/dubbo/${serviceName}目录下所有提供者和消费者的信息。</p>
<h3 id="分布式通知-协调"><a href="#分布式通知-协调" class="headerlink" title="分布式通知/协调"></a>分布式通知/协调</h3><p>ZooKeeper中特有watcher注册与异步通知机制，能够很好的实现分布式环境下不同系统之间的通知与协调，实现对数据变更的实时处理。<br>使用方法通常是不同系统都对ZK上同一个znode进行注册，监听znode的变化（包括znode本身内容及子节点的），其中一个系统update了znode，那么另一个系统能够收到通知，并作出相应处理另一种心跳检测机制：检测系统和被检测系统之间并不直接关联起来，而是通过zk上某个节点关联，大大减少系统耦合。<br>另一种系统调度模式：某系统有控制台和推送系统两部分组成，控制台的职责是控制推送系统进行相应的推送工作。管理人员在控制台作的一些操作，实际上是修改了ZK上某些节点的状态，而ZK就把这些变化通知给他们注册Watcher的客户端，即推送系统，于是，作出相应的推送任务。<br>另一种工作汇报模式：一些类似于任务分发系统，子任务启动后，到zk来注册一个临时节点，并且定时将自己的进度进行汇报（将进度写回这个临时节点），这样任务管理者就能够实时知道任务进度。<br>总之，使用zookeeper来进行分布式通知和协调能够大大降低系统之间的耦合</p>
<h3 id="集群管理与Master选举"><a href="#集群管理与Master选举" class="headerlink" title="集群管理与Master选举"></a>集群管理与Master选举</h3><p>集群机器监控：这通常用于那种对集群中机器状态，机器在线率有较高要求的场景，能够快速对集群中机器变化作出响应。这样的场景中，往往有一个监控系统，实时检测集群机器是否存活。过去的做法通常是：监控系统通过某种手段（比如ping）定时检测每个机器，或者每个机器自己定时向监控系统汇报“我还活着”。<br>这种做法可行，但是存在两个比较明显的问题：集群中机器有变动的时候，牵连修改的东西比较多。有一定的延时。<br>利用ZooKeeper有两个特性，就可以实时另一种集群机器存活性监控系统：客户端在节点 x 上注册一个Watcher，那么如果 x?的子节点变化了，会通知该客户端。创建EPHEMERAL类型的节点，一旦客户端和服务器的会话结束或过期，那么该节点就会消失。<br>例如，监控系统在 /clusterServers 节点上注册一个Watcher，以后每动态加机器，那么就往 /clusterServers 下创建一个 EPHEMERAL类型的节点：/clusterServers/{hostname}. 这样，监控系统就能够实时知道机器的增减情况，至于后续处理就是监控系统的业务了。Master选举则是zookeeper中最为经典的应用场景了。<br>在分布式环境中，相同的业务应用分布在不同的机器上，有些业务逻辑（例如一些耗时的计算，网络I/O处理），往往只需要让整个集群中的某一台机器进行执行，其余机器可以共享这个结果，这样可以大大减少重复劳动，提高性能，于是这个master选举便是这种场景下的碰到的主要问题。<br>利用ZooKeeper的强一致性，能够保证在分布式高并发情况下节点创建的全局唯一性，即：同时有多个客户端请求创建 /currentMaster 节点，最终一定只有一个客户端请求能够创建成功。<br>利用这个特性，就能很轻易的在分布式环境中进行集群选取了。另外，这种场景演化一下，就是动态Master选举。<br>这就要用到?EPHEMERAL_SEQUENTIAL类型节点的特性了。上文中提到，所有客户端创建请求，最终只有一个能够创建成功。<br>在这里稍微变化下，就是允许所有请求都能够创建成功，但是得有个创建顺序，于是所有的请求最终在ZK上创建结果的一种可能情况是这样： /currentMaster/{sessionId}-1 ,?/currentMaster/{sessionId}-2 ,?/currentMaster/{sessionId}-3 ….. 每次选取序列号最小的那个机器作为Master，如果这个机器挂了，由于他创建的节点会马上小时，那么之后最小的那个机器就是Master了。<br>在搜索系统中，如果集群中每个机器都生成一份全量索引，不仅耗时，而且不能保证彼此之间索引数据一致。因此让集群中的Master来进行全量索引的生成，然后同步到集群中其它机器。另外，Master选举的容灾措施是，可以随时进行手动指定master，就是说应用在zk在无法获取master信息时，可以通过比如http方式，向一个地方获取master。在Hbase中，也是使用ZooKeeper来实现动态HMaster的选举。<br>在Hbase实现中，会在ZK上存储一些ROOT表的地址和HMaster的地址，HRegionServer也会把自己以临时节点（Ephemeral）的方式注册到Zookeeper中，使得HMaster可以随时感知到各个HRegionServer的存活状态，同时，一旦HMaster出现问题，会重新选举出一个HMaster来运行，从而避免了HMaster的单点问题。</p>
<h3 id="分布式锁"><a href="#分布式锁" class="headerlink" title="分布式锁"></a>分布式锁</h3><p>分布式锁，这个主要得益于ZooKeeper为我们保证了数据的强一致性。<br>锁服务可以分为两类，一个是保持独占，另一个是控制时序。所谓保持独占，就是所有试图来获取这个锁的客户端，最终只有一个可以成功获得这把锁。<br>通常的做法是把zk上的一个znode看作是一把锁，通过create znode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。<br>控制时序，就是所有视图来获取这个锁的客户端，最终都是会被安排执行，只是有个全局时序了。做法和上面基本类似，只是这里 /distribute_lock 已经预先存在，客户端在它下面创建临时有序节点（这个可以通过节点的属性控制：CreateMode.EPHEMERAL_SEQUENTIAL来指定）。<br>Zk的父节点（/distribute_lock）维持一份sequence,保证子节点创建的时序性，从而也形成了每个客户端的全局时序。</p>
<h3 id="分布式队列"><a href="#分布式队列" class="headerlink" title="分布式队列"></a>分布式队列</h3><p>队列方面，简单地讲有两种，一种是常规的先进先出队列，另一种是要等到队列成员聚齐之后的才统一按序执行。对于第一种先进先出队列，和分布式锁服务中的控制时序场景基本原理一致，这里不再赘述。第二种队列其实是在FIFO队列的基础上作了一个增强。通常可以在 /queue 这个znode下预先建立一个/queue/num 节点，并且赋值为n（或者直接给/queue赋值n），表示队列大小，之后每次有队列成员加入后，就判断下是否已经到达队列大小，决定是否可以开始执行了。<br>这种用法的典型场景是，分布式环境中，一个大任务Task A，需要在很多子任务完成（或条件就绪）情况下才能进行。这个时候，凡是其中一个子任务完成（就绪），那么就去 /taskList 下建立自己的临时时序节点（CreateMode.EPHEMERAL_SEQUENTIAL），当 /taskList 发现自己下面的子节点满足指定个数，就可以进行下一步按序进行处理了。</p>
]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Git rebase的一些运用场景</title>
    <url>/2018/git_rebase.html</url>
    <content><![CDATA[<h2 id="Git-rebase的使用场景"><a href="#Git-rebase的使用场景" class="headerlink" title="Git rebase的使用场景"></a>Git rebase的使用场景</h2><p>最近在为github开源项目修修bug。提了几个Pull Request。又巩固了下git的知识，熟练了git的使用。</p>
<p>我在提交PR后，发现有个地方改错了。那怎么改呢，其实很简单，只需要再次push 到自己fork的repo就可以了。</p>
<p>PR的branch是和自己fork的repo的branch联动的。</p>
<p>但是，这又有个问题，我再次commit后，就会有两个commit。这两个commit其实解决的都是同一个问题。<br>在正式的repo中尽量要精简commit，同一个问题的多个commit需要squash为一个commit。</p>
<p>于是，我就得用git rebase 来帮我完成了。</p>
<h3 id="精简commit"><a href="#精简commit" class="headerlink" title="精简commit"></a>精简commit</h3><p>通过这个命令，可以重新修改最近的两个commit<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">git rebase -i HEAD~2</div></pre></td></tr></table></figure></p>
<p>执行命令后会弹出一个vim编辑:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">pick 01d1124 Message....</div><div class="line">pick 6340aaa Message....</div></pre></td></tr></table></figure></p>
<p>然后，将要squash的commit的那行最前面的<code>pick</code> 改为 <code>squash</code><br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">p 01d1124 Message....</div><div class="line">s 6340aaa Message....</div></pre></td></tr></table></figure></p>
<h3 id="删掉commit"><a href="#删掉commit" class="headerlink" title="删掉commit"></a>删掉commit</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">git rebase -i HEAD~n</div><div class="line"></div><div class="line"># 在vim中直接把不要commit那行删掉即可</div></pre></td></tr></table></figure>
<h3 id="改commit名字"><a href="#改commit名字" class="headerlink" title="改commit名字"></a>改commit名字</h3><p>大项目的commit的名字都有比较严格的要求，有些时候，我们要改下自己不规范的命名的话。<br>可以也可以用 git rebase.</p>
<p>很简单，同样先用git rebase -i HEAD~n<br>在需要改的commit的最前面把 <code>pick</code> 改为<code>reword</code><br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">pick 01d1124 Message....</div><div class="line">reword 6340aaa Message....</div></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive 优化</title>
    <url>/2018/hive_optimization.html</url>
    <content><![CDATA[<p>对于如果join中有小表的话，可以开启map</p>
<h3 id="Dynamic-Partition-Pruning-for-Hive-Map-Joins"><a href="#Dynamic-Partition-Pruning-for-Hive-Map-Joins" class="headerlink" title="Dynamic Partition Pruning for Hive Map Joins"></a>Dynamic Partition Pruning for Hive Map Joins</h3><p>You can enable dynamic partition pruning for map joins when you are running Hive on Spark (HoS), it is not available for Hive on MapReduce.<br>Dynamic partition pruning (DPP) is a database optimization that can significantly decrease the amount of data that a query scans, thereby executing your workloads faster.<br>DPP achieves this by dynamically determining and eliminating the number of partitions that a query must read from a partitioned table.</p>
<p>Map joins also optimize how Hive executes queries. They cause a small table to be scanned and loaded in memory as a hash table<br>so that a fast join can be performed entirely within a mapper without having to use another reduce step.<br>If you have queries that join small tables, map joins can make them execute much faster.<br>Map joins are enabled by default in CDH with the Enable MapJoin Optimization setting for HiveServer2 in Cloudera Manager.<br>Hive automatically uses map joins for join queries that involve a set of tables where:</p>
<ul>
<li>There is one large table and there is no limit on the size of that large table.</li>
<li>All other tables involved in the join must have an aggregate size under the value set for Hive Auto Convert Join Noconditional Size for HiveServer2, which is set to 20MB by default in Cloudera Manager.</li>
</ul>
<p>关于map-side join的配置:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">SET hive.auto.convert.join=true;</div><div class="line">SET hive.auto.convert.join.noconditionaltask.size=&lt;number_in_megabytes&gt;;</div></pre></td></tr></table></figure></p>
<h2 id="一次调优实战"><a href="#一次调优实战" class="headerlink" title="一次调优实战"></a>一次调优实战</h2><p>最近在ETL过程中发现有条SQL执行时间非常长，其实数据量很小的，但为什么这么长呢。我带着极度好奇，抱着死缠烂打的精神，怎么也要把<br>问题给解决掉。SQL是这样的:<br><figure class="highlight sql"><table><tr><td class="code"><pre><div class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">from</span> zhihu_answer </div><div class="line"><span class="keyword">where</span> ym <span class="keyword">in</span> (<span class="keyword">select</span> <span class="keyword">distinct</span>(ym) <span class="keyword">from</span> zhihu.zhihu_answer_increment);</div></pre></td></tr></table></figure></p>
<p>先说说这两个表数据量吧：<br>zhihu_answer数据量大概是一亿，zhihu_answer_increment 也就是几十万条。<br>首先，我用<code>explain extended</code>查看下执行计划:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">explain extended</div><div class="line">select count(1) from zhihu_answer </div><div class="line">where ym in (select distinct(ym) from zhihu.zhihu_answer_increment);</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">Explain	</div><div class="line">STAGE DEPENDENCIES:	</div><div class="line">  Stage-3 is a root stage	</div><div class="line">  Stage-5 depends on stages: Stage-3 , consists of Stage-6, Stage-1	</div><div class="line">  Stage-6 has a backup stage: Stage-1	</div><div class="line">  Stage-4 depends on stages: Stage-6	</div><div class="line">  Stage-2 depends on stages: Stage-1, Stage-4	</div><div class="line">  Stage-1	</div><div class="line">  Stage-0 depends on stages: Stage-2	</div><div class="line">	</div><div class="line">STAGE PLANS:	</div><div class="line">  Stage: Stage-3	</div><div class="line">    Map Reduce	</div><div class="line">      Map Operator Tree:	</div><div class="line">          TableScan	</div><div class="line">            alias: zhihu_answer_increment	</div><div class="line">            filterExpr: ym is not null (type: boolean)	</div><div class="line">            Statistics: Num rows: 347468 Data size: 73315748 Basic stats: COMPLETE Column stats: COMPLETE	</div><div class="line">            GatherStats: false	</div><div class="line">            Select Operator	</div><div class="line">              expressions: ym (type: string)	</div><div class="line">              outputColumnNames: ym	</div><div class="line">              Statistics: Num rows: 347468 Data size: 73315748 Basic stats: COMPLETE Column stats: COMPLETE	</div><div class="line">              Group By Operator	</div><div class="line">                keys: ym (type: string)	</div><div class="line">                mode: hash	</div><div class="line">                outputColumnNames: _col0	</div><div class="line">                Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE	</div><div class="line">                Reduce Output Operator	</div><div class="line">                  key expressions: _col0 (type: string)	</div><div class="line">                  null sort order: a	</div><div class="line">                  sort order: +	</div><div class="line">                  Map-reduce partition columns: _col0 (type: string)	</div><div class="line">                  Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE	</div><div class="line">                  tag: -1	</div><div class="line">                  auto parallelism: false	</div><div class="line">      Execution mode: vectorized	</div><div class="line">      Path -&gt; Alias:	</div><div class="line">        nullscan://null/zhihu.zhihu_answer_increment/part_ym=201902_ [sq_1:zhihu_answer_increment]	</div><div class="line">      Path -&gt; Partition:	</div><div class="line">        nullscan://null/zhihu.zhihu_answer_increment/part_ym=201902_ 	</div><div class="line">          Partition	</div><div class="line">            input format: org.apache.hadoop.hive.ql.io.OneNullRowInputFormat	</div><div class="line">            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	</div><div class="line">            partition values:	</div><div class="line">              ym 201902	</div><div class="line">            properties:	</div><div class="line">              COLUMN_STATS_ACCURATE &#123;&quot;BASIC_STATS&quot;:&quot;true&quot;&#125;	</div><div class="line">              bucket_count 256	</div><div class="line">              bucket_field_name answer_id	</div><div class="line">              columns admin_closed_comment,answer_content,answer_created,answer_id,answer_updated,author_headline,author_id,author_name,author_type,author_url_token,avatar_url,badge_num,can_comment,comment_count,gender,insert_time,is_advertiser,is_collapsed,is_copyable,is_org,question_created,question_id,question_title,question_type,reward_member_count,reward_total_money,voteup_count	</div><div class="line">              columns.comments 	</div><div class="line">              columns.types boolean:string:string:string:string:string:string:string:string:string:string:smallint:boolean:int:string:string:boolean:boolean:boolean:boolean:string:string:string:string:int:int:int	</div><div class="line">              file.inputformat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat	</div><div class="line">              file.outputformat org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat	</div><div class="line">              location hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer_increment/ym=201902	</div><div class="line">              name zhihu.zhihu_answer_increment	</div><div class="line">              numFiles 256	</div><div class="line">              numRows 347468	</div><div class="line">              partition_columns ym	</div><div class="line">              partition_columns.types string	</div><div class="line">              rawDataSize 9381636	</div><div class="line">              serialization.ddl struct zhihu_answer_increment &#123; bool admin_closed_comment, string answer_content, string answer_created, string answer_id, string answer_updated, string author_headline, string author_id, string author_name, string author_type, string author_url_token, string avatar_url, i16 badge_num, bool can_comment, i32 comment_count, string gender, string insert_time, bool is_advertiser, bool is_collapsed, bool is_copyable, bool is_org, string question_created, string question_id, string question_title, string question_type, i32 reward_member_count, i32 reward_total_money, i32 voteup_count&#125;	</div><div class="line">              serialization.format 1	</div><div class="line">              serialization.lib org.apache.hadoop.hive.serde2.NullStructSerDe	</div><div class="line">              totalSize 433473813	</div><div class="line">              transient_lastDdlTime 1571983508	</div><div class="line">            serde: org.apache.hadoop.hive.serde2.NullStructSerDe	</div><div class="line">          	</div><div class="line">              input format: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat	</div><div class="line">              output format: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat	</div><div class="line">              properties:	</div><div class="line">                bucket_count 256	</div><div class="line">                bucket_field_name answer_id	</div><div class="line">                columns admin_closed_comment,answer_content,answer_created,answer_id,answer_updated,author_headline,author_id,author_name,author_type,author_url_token,avatar_url,badge_num,can_comment,comment_count,gender,insert_time,is_advertiser,is_collapsed,is_copyable,is_org,question_created,question_id,question_title,question_type,reward_member_count,reward_total_money,voteup_count	</div><div class="line">                columns.comments 	</div><div class="line">                columns.types boolean:string:string:string:string:string:string:string:string:string:string:smallint:boolean:int:string:string:boolean:boolean:boolean:boolean:string:string:string:string:int:int:int	</div><div class="line">                file.inputformat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat	</div><div class="line">                file.outputformat org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat	</div><div class="line">                location hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer_increment	</div><div class="line">                name zhihu.zhihu_answer_increment	</div><div class="line">                partition_columns ym	</div><div class="line">                partition_columns.types string	</div><div class="line">                serialization.ddl struct zhihu_answer_increment &#123; bool admin_closed_comment, string answer_content, string answer_created, string answer_id, string answer_updated, string author_headline, string author_id, string author_name, string author_type, string author_url_token, string avatar_url, i16 badge_num, bool can_comment, i32 comment_count, string gender, string insert_time, bool is_advertiser, bool is_collapsed, bool is_copyable, bool is_org, string question_created, string question_id, string question_title, string question_type, i32 reward_member_count, i32 reward_total_money, i32 voteup_count&#125;	</div><div class="line">                serialization.format 1	</div><div class="line">                serialization.lib org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe	</div><div class="line">                transient_lastDdlTime 1571983018	</div><div class="line">              serde: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe	</div><div class="line">              name: zhihu.zhihu_answer_increment	</div><div class="line">            name: zhihu.zhihu_answer_increment	</div><div class="line">      Truncated Path -&gt; Alias:	</div><div class="line">        nullscan://null/zhihu.zhihu_answer_increment/part_ym=201902_ [sq_1:zhihu_answer_increment]	</div><div class="line">      Needs Tagging: false	</div><div class="line">      Reduce Operator Tree:	</div><div class="line">        Group By Operator	</div><div class="line">          keys: KEY._col0 (type: string)	</div><div class="line">          mode: mergepartial	</div><div class="line">          outputColumnNames: _col0	</div><div class="line">          Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE	</div><div class="line">          Group By Operator	</div><div class="line">            keys: _col0 (type: string)	</div><div class="line">            mode: hash	</div><div class="line">            outputColumnNames: _col0	</div><div class="line">            Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE	</div><div class="line">            File Output Operator	</div><div class="line">              compressed: false	</div><div class="line">              GlobalTableId: 0	</div><div class="line">              directory: hdfs://device1:8020/tmp/hive/hive/7f0887a3-8c5a-44b6-b5ef-f0c7530a6b15/hive_2019-10-25_14-46-02_198_8962679143430564511-1/-mr-10004	</div><div class="line">              NumFilesPerFileSink: 1	</div><div class="line">              table:	</div><div class="line">                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat	</div><div class="line">                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat	</div><div class="line">                  properties:	</div><div class="line">                    columns _col0	</div><div class="line">                    columns.types string	</div><div class="line">                    escape.delim \	</div><div class="line">                    serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe	</div><div class="line">                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe	</div><div class="line">              TotalFiles: 1	</div><div class="line">              GatherStats: false	</div><div class="line">              MultiFileSpray: false	</div><div class="line">	</div><div class="line">  Stage: Stage-5	</div><div class="line">    Conditional Operator	</div><div class="line">	</div><div class="line">  Stage: Stage-6	</div><div class="line">    Map Reduce Local Work	</div><div class="line">      Alias -&gt; Map Local Tables:	</div><div class="line">        $INTNAME 	</div><div class="line">          Fetch Operator	</div><div class="line">            limit: -1	</div><div class="line">      Alias -&gt; Map Local Operator Tree:	</div><div class="line">        $INTNAME 	</div><div class="line">          TableScan	</div><div class="line">            GatherStats: false	</div><div class="line">            HashTable Sink Operator	</div><div class="line">              keys:	</div><div class="line">                0 ym (type: string)	</div><div class="line">                1 _col0 (type: string)	</div><div class="line">              Position of Big Table: 0	</div><div class="line">	</div><div class="line">  Stage: Stage-4	</div><div class="line">    Map Reduce	</div><div class="line">      Map Operator Tree:	</div><div class="line">          TableScan	</div><div class="line">            alias: zhihu_answer	</div><div class="line">            filterExpr: ym is not null (type: boolean)	</div><div class="line">            Statistics: Num rows: 102075765 Data size: 21537986328 Basic stats: COMPLETE Column stats: COMPLETE	</div><div class="line">            GatherStats: false	</div><div class="line">            Map Join Operator	</div><div class="line">              condition map:	</div><div class="line">                   Left Semi Join 0 to 1	</div><div class="line">              keys:	</div><div class="line">                0 ym (type: string)	</div><div class="line">                1 _col0 (type: string)	</div><div class="line">              Position of Big Table: 0	</div><div class="line">              Statistics: Num rows: 4253156 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE	</div><div class="line">              Group By Operator	</div><div class="line">                aggregations: count(1)	</div><div class="line">                mode: hash	</div><div class="line">                outputColumnNames: _col0	</div><div class="line">                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE	</div><div class="line">                File Output Operator	</div><div class="line">                  compressed: false	</div><div class="line">                  GlobalTableId: 0	</div><div class="line">                  directory: hdfs://device1:8020/tmp/hive/hive/7f0887a3-8c5a-44b6-b5ef-f0c7530a6b15/hive_2019-10-25_14-46-02_198_8962679143430564511-1/-mr-10003	</div><div class="line">                  NumFilesPerFileSink: 1	</div><div class="line">                  table:	</div><div class="line">                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat	</div><div class="line">                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat	</div><div class="line">                      properties:	</div><div class="line">                        columns _col0	</div><div class="line">                        columns.types bigint	</div><div class="line">                        escape.delim \	</div><div class="line">                        serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe	</div><div class="line">                      serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe	</div><div class="line">                  TotalFiles: 1	</div><div class="line">                  GatherStats: false	</div><div class="line">                  MultiFileSpray: false	</div><div class="line">      Local Work:	</div><div class="line">        Map Reduce Local Work	</div><div class="line">      Path -&gt; Alias:	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201705 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201706 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201707 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201708 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201709 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201710 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201711 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201712 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201801 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201802 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201803 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201804 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201805 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201806 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201807 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201808 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201809 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201810 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201811 [zhihu_answer]</div></pre></td></tr></table></figure></p>
<p>一脸懵逼，不会看呀。。<br>然后我测试了下单独执行：<code>select distinct(ym) from zhihu_answer_increment;</code>，也就不到2分钟就出结果了。为什么组合在一起就要这么长时间呢？？<br>这条SQL的执行结果就是<code>&quot;201902&quot;</code>。我把这个结果复制进去执行：<br><figure class="highlight sql"><table><tr><td class="code"><pre><div class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">from</span> zhihu_answer <span class="keyword">where</span> ym <span class="keyword">in</span> (<span class="number">201902</span>);</div></pre></td></tr></table></figure></p>
<p>可能是因为我之前执行过的原因，这条语句的执行时间基本上是秒出呀。几秒内就出结果了。</p>
<p>我再一次执行了那句执行时间很长的SQL，看它的执行时候的log，我发现慢原因是在Stage-4 ！！！回到上面那个explain的信息，我发现Hive在做全表扫描呀！Why?<br>为什么要做全表扫描呢？ 因为Hive还是要join的 in （select ** ） 这种子查询中用的是semi join，所以要进行join，它就会进行全表扫描。我的解释不是很详细，<br>但隐隐约约我能理解为什么Hive在这要做全表扫描了，其实如果写死的话，比如where ym in （201902）它就不会做join，也就不用全表扫描了。所以解决方案还是要能<br>拿到 <code>201902</code>这个变量，这个value，再拼接到Hive SQL中。我查了下，Hive貌似目前还不支持以SQL查询结果作为新的SQL变量。所以，暂时还是以这种办法解决吧。</p>
<p>让我无比开心的是，改进后，SQL执行快了N倍，因为避免了全表扫描。从原来2个小时的执行，变为了几分钟！</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过Explain打印看看执行计划有哪些；<br>通过执行的log看看到底是哪个Stage耗时比较长；</p>
<h1 id="Referrence"><a href="#Referrence" class="headerlink" title="Referrence"></a>Referrence</h1><p><a href="https://docs.cloudera.com/documentation/enterprise/latest/topics/admin_hos_oview.html#concept_i22_l1h_1v" target="_blank" rel="external">https://docs.cloudera.com/documentation/enterprise/latest/topics/admin_hos_oview.html#concept_i22_l1h_1v</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop 基础学习笔记</title>
    <url>/2018/hadoop_basic.html</url>
    <content><![CDATA[<h2 id="Secondary和standby"><a href="#Secondary和standby" class="headerlink" title="Secondary和standby"></a>Secondary和standby</h2><p>Secondary Namenode: 主要在1.x 版本中使用。这相当于一个辅助</p>
<p>Standby Namenode：只要是2.x版本使用。如果用Standby的时候，Secondary就用不了了。<br>相当于一个替身（完全和原来的Namenode一样）</p>
<h3 id="HDFS读写过程"><a href="#HDFS读写过程" class="headerlink" title="HDFS读写过程"></a>HDFS读写过程</h3><p>要掌握Hadoop，HDFS读写过程一定要了解。这里我摘抄了一段网络上比较清晰的解释：</p>
<p>写的过程：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1. CLIENT（客户端）：用来发起读写请求，并拆分文件成多个 Block；</div><div class="line">2. NAMENODE：全局的协调和把控所有的请求，提供 Block 存放在 DataNode 上的地址；</div><div class="line">3. DATANODE：负责数据的存储，可以有很多个；</div><div class="line"></div><div class="line">1. 客户端想 NameNode 发出请求（包含 Blocksize 和 副本数）；</div><div class="line">2. NameNode 经过计算，反馈给客户端相同副本数的 DataNode，切给出的 DataNode 有优先存储顺序要求；（数据与 DataNode 对应时，一般移动计算，不移动数据）</div><div class="line">3. 客户端得到信息后开始写数据，当第一个 DataNode 接受 Block 时，会将该数据传给第二个 DataNode ，第二个 DataNode 接受到数据时，也会将该数据传递给第三个 DataNode；在最后一个 DataNode 接受数据完毕时，则该 Block 全部传输完毕；</div><div class="line">4. DataNode 在接受数据完毕后，每一个 DataNode 都会将完毕信息传递给 NameNode；</div><div class="line">5. NameNode 将所有 DataNode 反馈的信息（所有数据以传输完毕），反馈给客户端；</div><div class="line">6. 客户端接受到 NamaNode 反馈的信息后（第一个 Block 传输完毕），开始发送请求传输第二个 Block；</div><div class="line">7. 传输完毕后，在关闭请求之前，NameNode 将该文件所有 Block 存放在 DataNode 上的 ID 保存在文件中；</div></pre></td></tr></table></figure></p>
<p>读过程：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1. 客户端：提供文件名、副本数、Block 数量、Block 地址；</div><div class="line">2. NameNode：提供 DataNode 地址及内部位置；</div><div class="line"></div><div class="line">1. 客户端提供提供文件名、副本数、Block 数量、Block 地址给 NameNode；</div><div class="line">2. NameNode 收到请求后，根据请求给出 副本及其 Block 所存放的 DataNode，以及Block 在 DataNode 中存放的位置；</div><div class="line">3. 客户端根据 NameNode 给的信息，给 DataNode 发出请求，由 DataNode 给出数据所在的具体块的信息；</div><div class="line">4. 客户端根据 DataNode 提供的信息，下载数据；</div></pre></td></tr></table></figure></p>
<p>HDFS优缺点：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">优点：</div><div class="line">数据冗余（文件以 Block 并且多副本的方式存储在集群的节点上）、硬件容错；</div><div class="line">处理流式的数据访问；（一次写入，多次读取）</div><div class="line">适合存储大文件；（通过扩展 DataNode 来实现存储大文件）</div><div class="line">可构建在廉价的机器上；（降低成本）</div><div class="line"> </div><div class="line">缺点：</div><div class="line">低延迟的数据访问；（一般数据较大，不容易实现在秒级别检索数据）</div><div class="line">不适合小文件的存储；（无论文件大小，都有对应的元数据存放在 NameNode 上，如果小文件较多，则对应的元数据较多，对应的元数据所占用的内存信息较大，给NameNode 压力较大）</div></pre></td></tr></table></figure></p>
<h3 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h3><p>我看有个选择题答案的正确选项是：<br>同一个MapReduce job的mapper和reducer可能同时在一台nodemanager上执行。<br>（像Spark的Narrow dependency那样，不同的mapper和reducer可能同时进行？）<br>然而我选的选项是：同一台nodemanager一次只会处理一个MapReduce job的作业。(一次可以同时处理很多个job？)</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.cnblogs.com/volcao/p/11446657.html" target="_blank" rel="external">https://www.cnblogs.com/volcao/p/11446657.html</a></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka Introduction</title>
    <url>/2018/kafka_intro.html</url>
    <content><![CDATA[<ul>
<li>最近面试经常会被问到Kafka的问题。看了很多Kafka的介绍，发现还是美团技术团队总结的最清楚易懂。下面转了美团的关于Kafka的一篇文章</li>
</ul>
<h2 id="Kafka是什么"><a href="#Kafka是什么" class="headerlink" title="Kafka是什么"></a>Kafka是什么</h2><p>Kafka是最初由Linkedin公司开发，是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统(也可以当做MQ系统)，常见可以用于web/nginx日志、访问日志，消息服务等等，Linkedin于2010年贡献给了Apache基金会并成为顶级开源项目。</p>
<p>一个商业化消息队列的性能好坏，其文件存储机制设计是衡量一个消息队列服务技术水平和最关键指标之一。 下面将从Kafka文件存储机制和物理结构角度，分析Kafka是如何实现高效文件存储，及实际应用效果。</p>
<p>Kafka部分名词解释如下：</p>
<ul>
<li>Broker：消息中间件处理结点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群。</li>
<li>Topic：一类消息，例如page view日志、click日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发。</li>
<li>Partition：topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。</li>
<li>Segment：partition物理上由多个segment组成，下面2.2和2.3有详细说明。</li>
<li>offset：每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中。partition中的每个消息都有一个连续的序列号叫做offset,用于partition唯一标识一条消息.</li>
</ul>
<p>分析过程分为以下4个步骤：</p>
<ul>
<li>topic中partition存储分布</li>
<li>partiton中文件存储方式</li>
<li>partiton中segment文件存储结构</li>
<li>在partition中如何通过offset查找message</li>
</ul>
<p>通过上述4过程详细分析，我们就可以清楚认识到kafka文件存储机制的奥秘。</p>
<h2 id="2-1-topic中partition存储分布"><a href="#2-1-topic中partition存储分布" class="headerlink" title="2.1 topic中partition存储分布"></a>2.1 topic中partition存储分布</h2><p>假设实验环境中Kafka集群只有一个broker，xxx/message-folder为数据文件存储根目录，在Kafka broker中server.properties文件配置(参数log.dirs=xxx/message-folder)，例如创建2个topic名称分别为report_push、launch_info, partitions数量都为partitions=4 存储路径和目录规则为： xxx/message-folder<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">|--report_push-0</div><div class="line">|--report_push-1</div><div class="line">|--report_push-2</div><div class="line">|--report_push-3</div><div class="line">|--launch_info-0</div><div class="line">|--launch_info-1</div><div class="line">|--launch_info-2</div><div class="line">|--launch_info-3</div></pre></td></tr></table></figure></p>
<p>在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。 如果是多broker分布情况，请参考kafka集群partition分布原理分析</p>
<h2 id="2-2-partiton中文件存储方式"><a href="#2-2-partiton中文件存储方式" class="headerlink" title="2.2 partiton中文件存储方式"></a>2.2 partiton中文件存储方式</h2><p>下面示意图形象说明了partition中文件存储方式:</p>
<p><img src="/images/2018/kafka_intro/kafka_partition.png" alt="Sample Image Added via Markdown"></p>
<ul>
<li>每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。但每个段segment file消息数量不一定相等，这种特性方便old segment file快速被删除。</li>
<li>每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。</li>
</ul>
<p>这样做的好处就是能快速删除无用文件，有效提高磁盘利用率。</p>
<h2 id="2-3-partiton中segment文件存储结构"><a href="#2-3-partiton中segment文件存储结构" class="headerlink" title="2.3 partiton中segment文件存储结构"></a>2.3 partiton中segment文件存储结构</h2><p>读者从2.2节了解到Kafka文件系统partition存储方式，本节深入分析partion中segment file组成和物理结构。</p>
<ul>
<li>segment file组成：由2大部分组成，分别为index file和data file，此2个文件一一对应，成对出现，后缀”.index”和“.log”分别表示为segment索引文件、数据文件.</li>
<li>segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值。数值最大为64位long大小，19位数字字符长度，没有数字用0填充。</li>
</ul>
<p>下面文件列表是笔者在Kafka broker上做的一个实验，创建一个topicXXX包含1 partition，设置每个segment大小为500MB,并启动producer向Kafka broker写入大量数据,如下图2所示segment文件列表形象说明了上述2个规则：</p>
<p><img src="/images/2018/kafka_intro/kafka_index.png" alt="Sample Image Added via Markdown"></p>
<p>以上述图2中一对segment file文件为例，说明segment中index&lt;—-&gt;data file对应关系物理结构如下：</p>
<p>上述图3中索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。 其中以索引文件中元数据3,497为例，依次在数据文件中表示第3个message(在全局partiton表示第368772个message)、以及该消息的物理偏移地址为497。</p>
<p>从上述图3了解到segment data file由许多message组成，下面详细说明message物理结构如下：<br><img src="/images/2018/kafka_intro/kafka_message.png" alt="Sample Image Added via Markdown"></p>
<p>参数说明：<br>关键字    解释说明<br>8 byte offset    在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message<br>4 byte message size    message大小<br>4 byte CRC32    用crc32校验message<br>1 byte “magic”    表示本次发布Kafka服务程序协议版本号<br>1 byte “attributes”    表示为独立版本、或标识压缩类型、或编码类型。<br>4 byte key length    表示key的长度,当key为-1时，K byte key字段不填<br>K byte key    可选<br>value bytes payload    表示实际消息数据。</p>
<h2 id="2-4-在partition中如何通过offset查找message"><a href="#2-4-在partition中如何通过offset查找message" class="headerlink" title="2.4 在partition中如何通过offset查找message"></a>2.4 在partition中如何通过offset查找message</h2><p>例如读取offset=368776的message，需要通过下面2个步骤查找。</p>
<p>第一步查找segment file 上述图2为例，其中00000000000000000000.index表示最开始的文件，起始偏移量(offset)为0.第二个文件00000000000000368769.index的消息量起始偏移量为368770 = 368769 + 1.同样，第三个文件00000000000000737337.index的起始偏移量为737338=737337 + 1，其他后续文件依次类推，以起始偏移量命名并排序这些文件，只要根据offset <strong>二分查找</strong>文件列表，就可以快速定位到具体文件。 当offset=368776时定位到00000000000000368769.index|log</p>
<p>第二步通过segment file查找message 通过第一步定位到segment file，当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和00000000000000368769.log的物理偏移地址，然后再通过00000000000000368769.log顺序查找直到offset=368776为止。</p>
<p>从上述图3可知这样做的优点，segment index file采取稀疏索引存储方式，它减少索引文件大小，通过mmap可以直接内存操作，稀疏索引为数据文件的每个对应message设置一个元数据指针,它比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间。</p>
<p>3 Kafka文件存储机制–实际运行效果<br>实验环境：</p>
<p>Kafka集群：由2台虚拟机组成<br>cpu：4核<br>物理内存：8GB<br>网卡：千兆网卡<br>jvm heap: 4GB<br>详细Kafka服务端配置及其优化请参考：kafka server.properties配置详解<br><img src="/images/2018/kafka_intro/kafka_read_disk.png" alt="Sample Image Added via Markdown"></p>
<p>从上述图5可以看出，Kafka运行时很少有大量读磁盘的操作，主要是定期批量写磁盘操作，因此操作磁盘很高效。这跟Kafka文件存储中读写message的设计是息息相关的。Kafka中读写message有如下特点:</p>
<h3 id="写message"><a href="#写message" class="headerlink" title="写message"></a>写message</h3><ul>
<li>消息从java堆转入page cache(即物理内存)。</li>
<li>由异步线程刷盘,消息从page cache刷入磁盘。</li>
</ul>
<h3 id="读message"><a href="#读message" class="headerlink" title="读message"></a>读message</h3><ul>
<li>消息直接从page cache转入socket发送出去。</li>
<li>当从page cache没有找到相应数据时，此时会产生磁盘IO,从磁 盘Load消息到page cache,然后直接从socket发出去</li>
</ul>
<h3 id="Kafka高效文件存储设计特点"><a href="#Kafka高效文件存储设计特点" class="headerlink" title="Kafka高效文件存储设计特点"></a>Kafka高效文件存储设计特点</h3><ul>
<li><p>Kafka把topic中一个partition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。</p>
</li>
<li><p>通过索引信息可以快速定位message和确定response的最大大小。</p>
</li>
<li>通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作。</li>
<li>通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。</li>
</ul>
<p>1.Linux Page Cache机制 </p>
<p>2.Kafka官方文档</p>
<h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>面试的时候经常问到Kafka怎么防止重复消费</p>
<ul>
<li><p>比如，你拿到这个消息做数据库的insert操作，那就容易了，给这个消息做一个唯一的主键，那么就算出现重复消费的情况，就会导致主键冲突，避免数据库出现脏数据。</p>
</li>
<li><p>再比如，你拿到这个消息做Redis的set的操作，那就容易了，不用解决，因为你无论set几次结果都是一样的，set操作本来就算幂等操作。</p>
</li>
<li><p>如果上面两种情况还不行，上大招。准备一个第三方介质，来做消费记录。以Redis为例，给消息分配一个全局id，只要消费过该消息，将<id,message>以K-V形式写入Redis. 那消费者开始消费前，先去Redis中查询有没有消费记录即可。</id,message></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Python Dict Internal</title>
    <url>/2018/python_dict.html</url>
    <content><![CDATA[<p>前言：最近在知乎上看到很多Python职位的面试都会问Python的数据结构。很多大公司，比如知乎，在面试python的工程师的时候都会很注重基础；<br>比如会问：python内置的list dict set等使用和原理。collections 模块里的deque，Counter，OrderDict等。 常用的排序算法和时间复杂度。</p>
<p>今天我就简单说说我学到的关于Python Dict 的见闻。</p>
<p>于是，首先我查看到《流畅的python》一书的第3章。</p>
<ol>
<li>首先我们看看Python是如何用散列表来实现dict类型。</li>
</ol>
<p>什么是散列表：<br>散列表其实是一个稀疏数组（总是有空白元素的数组称为稀疏数组）。<br>在一般的数据结构教材中，散列表里的单元通常叫作表元（bucket）。在 dict 的散列表当中，每个键值对都占用一个表元，每个表元都有两<br>个部分，一个是对键的引用，另一个是对值的引用。因为所有表元的大小一致，所以可以通过偏移量来读取某个表元。</p>
<p>散列算法：<br>1.为了获取 my_dict[search_key] 背后的值，Python 首先会调用hash(search_key) 来计算 search_key 的散列值，把这个值最低的几位数字当作偏移量(貌似是hash(key) 和 数组的长度-1 作 与运算，hash(key)&amp; (size-1)，在散列表里查找表元（具体取几位，得看当前散列表的大小）。</p>
<p>2.若找到的表元是空的，则抛出 KeyError 异常。若不是空的，则表元里会有一对 found_key:found_value。</p>
<p>3.这时候 Python 会检验 search_key == found_key 是否为真，如果它们相等的话，就会返回 found_value。</p>
<p>4.如果 search_key 和 found_key 不匹配的话，这种情况称为散列冲突。</p>
<p>5.Python 解决散列冲突的方法用的是开放地址法。<br>如下图所示：<br><img src="/images/2018/python_dict/python_dict1.png" alt="Sample Image Added via Markdown"></p>
<h3 id="Python-dict的特性："><a href="#Python-dict的特性：" class="headerlink" title="Python dict的特性："></a>Python dict的特性：</h3><p>1.字典在内存上的开销巨大。<br>由于字典使用了散列表，而散列表又必须是稀疏的，这导致它在空间上的效率低下。<br>2.键查询很快。<br>dict 的实现是典型的空间换时间：字典类型有着巨大的内存开销，但它们提供了无视数据量大小的快速访问——只要字典能被装在内存里。<br>3.往字典里添加新键可能会改变已有键的顺序。<br>无论何时往字典里添加新的键，Python 解释器都可能做出为字典扩容的决定。扩容导致的结果就是要新建一个更大的散列表，并把字典里已有的元素添加到新表里。</p>
<p>关于Python的开放地制法解决hash冲突的实现可以参考下<br><a href="https://harveyqing.gitbooks.io/python-read-and-write/content/python_advance/python_dict_implementation.html" target="_blank" rel="external">https://harveyqing.gitbooks.io/python-read-and-write/content/python_advance/python_dict_implementation.html</a></p>
<p><img src="/images/2018/python_dict/python_dict2.png" alt="Sample Image Added via Markdown"></p>
<p><em>若要了解详细的内部机制，推荐阅读</em><br><a href="http://python.jobbole.com/85040/" target="_blank" rel="external">http://python.jobbole.com/85040/</a></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>在Github开车需要需要了解的git技能</title>
    <url>/2018/git_github.html</url>
    <content><![CDATA[<h2 id="更新forked项目"><a href="#更新forked项目" class="headerlink" title="更新forked项目"></a>更新forked项目</h2><p>比如：我之前fork了Airflow这个仓库，隔了一段时间（官方的仓库已经有很多更新了）又想提交点PR，<br>那要不要重新删了这个repo再fork一次呢？<br>答案显然是：不用的。</p>
<p>首先我们添加下官方的remote，然后再fetch一下。<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">git remote add upstream https://github.com/whoever/whatever.git</div><div class="line"></div><div class="line"># Fetch all the branches of that remote into remote-tracking branches,</div><div class="line"># such as upstream/master:</div><div class="line"></div><div class="line">git fetch upstream</div></pre></td></tr></table></figure></p>
<p>然后：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">git pull --rebase upstream master</div><div class="line"># pull = fetch + merge</div><div class="line"># 如果rebase有conflict的话可以先--hard reset 前几个commit再执行rebase</div></pre></td></tr></table></figure></p>
<p>rebase的过程中可以把自己提交的不要的commit删掉。</p>
<p>最后再强推覆盖掉自己的repo<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">git push -f origin master</div></pre></td></tr></table></figure></p>
<p>更新后，如果本地的 代码想全部舍弃掉，重新把remote的 clone下来的话，可以用如下命令：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">git reset --hard origin/master</div></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>堆排序学习总结</title>
    <url>/2017/heap_sort.html</url>
    <content><![CDATA[<p>在学习《算法》一书中了解到：<br>优先队列是一种抽象的数据类型，优先队列最重要的操作就是删除最大元素和插入元素。<br>实现优先队列可以用数组，链表，最好的方式还是用堆。</p>
<p>我们可以把任意优先队列变成一种排序方法。将所有元素插入一个查找最小化元素的优先队列，然后再重复调用删除最小元素的操作将它们按顺序删去。用无序数组实现的优先队列这么做相当于进行一次选择排序。用基于堆得优先队列这样做等同于堆排序！</p>
<p>要了解堆首先得了解一下二叉树，在计算机科学中，二叉树是每个节点最多有两个子树的树结构。通常子树被称作“左子树”（left subtree）和“右子树”（right subtree）。</p>
<p>二叉树又分为满二叉树（full binary tree） 和 完全二叉树（complete binary tree）</p>
<p>满二叉树：一棵深度为 k，且有 2k - 1 个节点称之为满二叉树</p>
<p><img src="/images/2017/heap_sort/full_binary_tree.png" alt="Sample Image Added via Markdown"></p>
<p>完全二叉树则满足序号和满二叉树的完全对应。</p>
<p><img src="/images/2017/heap_sort/complete_binary_tree.png" alt="Sample Image Added via Markdown"></p>
<p>如下图，是一个堆和数组的相互关系<br><img src="/images/2017/heap_sort/heap_sort1.png" alt="Sample Image Added via Markdown"></p>
<p>对于给定的某个结点的下标 i，可以很容易的计算出这个结点的父结点、孩子结点的下标：</p>
<ul>
<li>Parent(i) = floor(i/2)，i 的父节点下标</li>
<li>Left(i) = 2i，i 的左子节点下标</li>
<li>Right(i) = 2i + 1，i 的右子节点下标</li>
</ul>
<p>堆排序原理</p>
<p>堆排序就是把最大堆堆顶的最大数取出，将剩余的堆继续调整为最大堆，再次将堆顶的最大数取出，这个过程持续到剩余数只有一个时结束。在堆中定义以下几种操作：</p>
<p>最大堆调整（Max-Heapify）：将堆的末端子节点作调整，使得子节点永远小于父节点(左子树不一定大于右子树)<br>创建最大堆（Build-Max-Heap）：将堆所有数据重新排序，使其成为最大堆<br>堆排序（Heap-Sort）：移除位在第一个数据的根节点，并做最大堆调整的递归运算</p>
<p>继续进行下面的讨论前，需要注意的一个问题是：数组都是 Zero-Based，这就意味着我们的堆数据结构模型要发生改变</p>
<p>相应的，几个计算公式也要作出相应调整：</p>
<p>Parent(i) = floor((i-1)/2)，i 的父节点下标<br>Left(i) = 2i + 1，i 的左子节点下标<br>Right(i) = 2(i + 1)，i 的右子节点下标</p>
<p>最大堆调整（MAX‐HEAPIFY）的作用是保持最大堆的性质，是创建最大堆的核心子程序，作用过程如图所示：</p>
<p><img src="/images/2017/heap_sort/heap_sort2.png" alt="Sample Image Added via Markdown"></p>
<p>源码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">heapsort</span><span class="params">(arr)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">max_heapify</span><span class="params">(arr, index, heap_size)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        # 最大堆调整（MAX‐HEAPIFY）的作用是保持最大堆的性质。 将堆的末端子节点作调整，使得子节点永远小于父节点</div><div class="line">        # 意思就是要让index这元素去到它该去的位置</div><div class="line">        《算法》P200</div><div class="line">        由上至下的对有序化（下沉）</div><div class="line">        如果我们把堆想象成一个严密的黑色会组织，每个子节点都表示一个下属（父节点则表示它的直接上级），name这些操作就可以得到很有趣的解释。</div><div class="line">        max_hepify这个方法就相当于一个领导，如果占着老大的位置，但如果自己下属有比自己厉害的就要退位让贤。自己去到该去的层次</div><div class="line"></div><div class="line">        注意的是：这方法是有惰性的。调用一次该方法仍然不行。如果该index的左右子树都小于自己的话，它就不会递归了。</div><div class="line">        这个方法只是让index尽量下沉下去。如果是堆有序的话，那么执行该方法后index位置就是该位置最大的值</div><div class="line">        """</div><div class="line"></div><div class="line">        <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">            imax = index</div><div class="line">            ileft = <span class="number">2</span> * index + <span class="number">1</span></div><div class="line">            iright = <span class="number">2</span> * index + <span class="number">2</span></div><div class="line"></div><div class="line">            <span class="keyword">if</span> ileft &lt; heap_size <span class="keyword">and</span> arr[index] &lt; arr[ileft]:</div><div class="line">                imax = ileft</div><div class="line"></div><div class="line">            <span class="keyword">if</span> iright &lt; heap_size <span class="keyword">and</span> arr[imax] &lt; arr[iright]:</div><div class="line">                imax = iright</div><div class="line"></div><div class="line">            <span class="keyword">if</span> imax != index:</div><div class="line">                arr[imax], arr[index] = arr[index], arr[imax]</div><div class="line">                index = imax</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                <span class="keyword">break</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_maxheap</span><span class="params">(arr)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        # 将堆所有数据重新排序，使其成为最大堆</div><div class="line"></div><div class="line">        创建最大堆（Build-Max-Heap）的作用是将一个数组改造成一个最大堆，接受数组和堆大小两个参数，Build-Max-Heap 将自下而上的调用 Max-Heapify 来改造数组，建立最大堆。</div><div class="line">        因为 Max-Heapify 能够保证下标 i 的结点之后结点都满足最大堆的性质，所以自下而上的调用 Max-Heapify 能够在改造过程中保持这一性质。</div><div class="line">        如果最大堆的数量元素是 n，那么 Build-Max-Heap 从 Parent(n) 开始，往上依次调用 Max-Heapify。</div><div class="line">        """</div><div class="line"></div><div class="line">        iparent = len(arr) // <span class="number">2</span> - <span class="number">1</span>   <span class="comment"># 最后一个节点的父节点</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(iparent, <span class="number">-1</span>, <span class="number">-1</span>):</div><div class="line">            max_heapify(arr, i, len(arr))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sort</span><span class="params">(arr)</span>:</span></div><div class="line">        build_maxheap(arr)  <span class="comment"># build后左子树 还是可能大于 右子树的.所以这并不是一个有序数组</span></div><div class="line">        print(arr)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</div><div class="line">            arr[<span class="number">0</span>], arr[i] = arr[i], arr[<span class="number">0</span>]</div><div class="line"></div><div class="line">            max_heapify(arr, <span class="number">0</span>, i)  <span class="comment"># 这里这个i 相当于比当前的length - 1；第i个元素为当前最大的元素，就被固定死了。不再访问它</span></div><div class="line">        <span class="keyword">return</span> arr</div><div class="line"></div><div class="line">    <span class="keyword">return</span> sort(arr)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line"></div><div class="line">    print(heapsort([<span class="number">5</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">3</span>,<span class="number">8</span>,<span class="number">2</span>,<span class="number">11</span>,<span class="number">15</span>,<span class="number">17</span>,<span class="number">16</span>,<span class="number">13</span>,<span class="number">10</span>,<span class="number">9</span>,<span class="number">12</span>,<span class="number">1</span>]))</div></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>TCP/IP 的一些学习</title>
    <url>/2017/tcp_knowledge.html</url>
    <content><![CDATA[<p>还记得两三年前，去一个大数据公司面试。首先要笔试，题目中有道题目就是填空TCP报文的表格。<br>首先我们看看TCP的报头：</p>
<p> <img src="/images/2017/tcp_knowledge/tcp_head.jpg" alt="Sample Image Added via Markdown"></p>
<p>当时觉得这是啥玩意呀。考这东西跟大数据有毛线关系。今天，才明白，这些都是计算机的基础。基本功来的，跑不了。</p>
<p>下面我们可以通过一张图就能比较清晰地展现TCP/IP建立连接的过程。也就是我们常说的3次握手，4次挥手。</p>
<p>我们先看3次握手的过程：<br><img src="/images/2017/tcp_knowledge/tcp_connection.png" alt="Sample Image Added via Markdown"></p>
<p>1.首先Server端是处在一个Listen的状态。经常你用netstat可以看到，比如打开了一个web 服务，就卡看到80端口处在listen状态<br>2.由客户端发送一个Segment(传输层的数据包单位)到服务的，主要内容有 SYN=1，seq=x(一个随机数)。发送后，客户端状态变为SYN-SENT<br>3.服务端收到客户端发送的Segment，状态变为SYN-REVD，并发送一个SYN=1,ACK=1,seq=y,ack=x+1 的Segment返还给客户端。表示自己收到了你的消息。<br>4.客户端收到服务端返还的内容后，状态就变成了ESTABLISHED。表示成功建立了连接。并且再次发送一个ACK=1,ack=y+1,seq=x+1 的Segment。<br>5.服务端收到客户端发来的Segment后，状态变为ESTABLISHED。成功完成3次握手。</p>
<p>建立一次连接，需要3次握手，也就是说发送3个数据包即可建立连接。发送的数据包内容中我们可以发现一点规律。ack的数值内容都是对方发送给自己的seq数值 +1  。</p>
<p>接下来我们看看4次挥手的过程。<br><img src="/images/2017/tcp_knowledge/tcp_finish.png" alt="Sample Image Added via Markdown"></p>
<p>1.首先由终端的提出方(假设是客户端)发送一个FIN=1,seq=u 的Segment到服务端。发送后客户端状态变为FIN-WAIT-1<br>2.服务端收到了客户端的Segment后状态变为CLOSE-WAIT，并反回一个ACK=1,seq=v,ack=u+1 的Segment给客户端。<br>3.客户端收到了服务端返回的消息后状态变为FIN-WAIT-2,此时服务端并没有关闭，意思是告诉客户端，请您耐心等待，我看看还有什么数据没有给你的，把剩下的数据发给你先。<br>4.过了片刻，服务端发送一个FIN=1,ACK=1,seq=w,ack=u+1 的Segment给客户端。然后服务端状态变为了LAST-ACK。<br>5.客户端收到了服务端发来的消息后，状态变为了TIME-WAIT。并发送ACK=1,seq=u+1,ack=w+1的Segment。表示我知道了，我再等等你(2MSL)还有什么话跟我说的。没有的话我就关了。<br>6.服务端收到客户端发送的消息后，状态就变为CLOSED了。</p>
]]></content>
      <categories>
        <category>study</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title>第一次去加州 （下）</title>
    <url>/2017/california_trip2.html</url>
    <content><![CDATA[<h2 id="加州游记-下"><a href="#加州游记-下" class="headerlink" title="加州游记 下"></a>加州游记 下</h2><p>一路沿海的路途。本来应该比较好的方案是从旧金山开到洛杉矶，这样就可以靠海看风景了。<br><img src="/images/2017/california_trip/IMG_0737.jpg" alt=""></p>
<h3 id="圣巴巴拉"><a href="#圣巴巴拉" class="headerlink" title="圣巴巴拉"></a>圣巴巴拉</h3><p>到圣巴巴拉时候，我们在Airbnb找了个当地一对年轻人的家。入住的时候，女房东发信息跟我说问我介不介意<br>房间里有很浓的大麻味。我说不介意。刚开始有点怕怕的，这到底是不是正经的人呀。出于好奇，我还是很想闻闻<br>大麻是什么味道的。我问我妈，你怕不怕，我妈说不怕，于是我们就定了这家<br><img src="/images/2017/california_trip/IMG_0750.jpg" alt=""><br><img src="/images/2017/california_trip/IMG_0752.jpg" alt=""></p>
<p>房东是两个小年轻，挺热情友好的，非常开朗，男的跟我介绍哪里有中餐馆，他说他有个好朋友是香港人。女的也很轻，<br>我们谈笑风生一个晚上。边聊变抽，笑嘻嘻的，很过瘾。我都看得我都很想抽一口。</p>
<p>在进旧金山之前，路上车非常少<br><img src="/images/2017/california_trip/IMG_0759.jpg" alt=""></p>
<p><img src="/images/2017/california_trip/IMG_0763.jpg" alt=""></p>
<p>都说国外的月亮比较远，记得那晚正是中秋节，我们来路途的一个医院附近休息赏月<br><img src="/images/2017/california_trip/IMG_0764.jpg" alt=""></p>
<h3 id="卡梅尔小镇-Carmel-by-the-sea"><a href="#卡梅尔小镇-Carmel-by-the-sea" class="headerlink" title="卡梅尔小镇 Carmel-by-the-sea"></a>卡梅尔小镇 Carmel-by-the-sea</h3><p>这里有非常不错的海景，吸引了不少游客。本来打算在这住一晚的，然后打开AirBnb一看，根本没有空房了，不知道得提前多久预定。<br>这里的房价贵的吓人。我们当时就在那里转了一个早上就走了。<br><img src="/images/2017/california_trip/IMG_0771.jpg" alt=""></p>
<h3 id="Santa-Jose"><a href="#Santa-Jose" class="headerlink" title="Santa Jose"></a>Santa Jose</h3><p>圣克拉拉大学<br>里面的设施非常新，貌似是一个贵族学校<br><img src="/images/2017/california_trip/IMG_0793.jpg" alt=""></p>
<h3 id="山景城"><a href="#山景城" class="headerlink" title="山景城"></a>山景城</h3><p>Google， 我最想去的公司，也是很多程序员最理想的工作地方吧<br><img src="/images/2017/california_trip/IMG_0818.jpg" alt=""></p>
<h3 id="San-Francisco"><a href="#San-Francisco" class="headerlink" title="San Francisco"></a>San Francisco</h3><p>Facebook 的停车场门上堆满了汽车<br><img src="/images/2017/california_trip/IMG_0840.jpg" alt=""></p>
<p>斯坦福大学，里面的草太大，太绿了，修这些草不知要花多少钱。</p>
<p><img src="/images/2017/california_trip/IMG_0848.jpg" alt=""></p>
<p><img src="/images/2017/california_trip/IMG_0869.jpg" alt=""></p>
<p>在美国问了几次路。发现最友好的是黑人。黑人总是给人一种很热情，乐观的感觉。还记得，在斯坦福校园里迷路的时候，一直问路人。<br>最友好的是一个混血小哥，像亨利的那种肤色。在小哥，锁单车准备进图书馆的时候，我走过去问他。<br>小哥非常热情地告诉我路怎么走。真是很感激，真想当场要个联系方式，也是这么牛逼的人了，还这么热情。</p>
<p>塞车的旧金山<br><img src="/images/2017/california_trip/IMG_0887.jpg" alt=""></p>
<p>感受一下旧金山的停车费<br><img src="/images/2017/california_trip/IMG_0890.jpg" alt=""></p>
<p>旧金山的中国城<br><img src="/images/2017/california_trip/IMG_0894.jpg" alt=""></p>
<p>在旧金山开车很有难度，一些性能较差的车可能在旧金山都开不了。因为有很多那种非常陡的路，下坡要踩很重的刹车，上坡就得考研车的动力了。<br>这条路倾斜调度应该有40度吧。<br><img src="/images/2017/california_trip/IMG_0901.jpg" alt=""></p>
<h3 id="在加州开车的感觉"><a href="#在加州开车的感觉" class="headerlink" title="在加州开车的感觉"></a>在加州开车的感觉</h3><p>加州人开车非常生猛。起步通常都是地板油起步。有一次我要掉头没走到最左侧的道路。在等着红灯。绿灯亮了时候我打灯后向左边偏转，后面突然冲出一辆车，差点撞到我。我吓尿了，半天没缓过神来。<br>在旧金山时候，很多斜坡上等红灯时候，他们起步都会让车轮发出吱吱的尖叫声，因为油门踩得太猛了。</p>
<p>最后一晚住在一个中国房东家里，非常不错的感觉，挺整齐温馨的。<br><img src="/images/2017/california_trip/IMG_0909.jpg" alt=""></p>
<h2 id="再见了加州"><a href="#再见了加州" class="headerlink" title="再见了加州"></a>再见了加州</h2><p>乘坐国泰航空的波音-777回来<br><img src="/images/2017/california_trip/IMG_0924.jpg" alt=""></p>
]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
        <tag>travel</tag>
      </tags>
  </entry>
  <entry>
    <title>第一次去加州 （上）</title>
    <url>/2017/california_trip.html</url>
    <content><![CDATA[<h2 id="加州游记-上"><a href="#加州游记-上" class="headerlink" title="加州游记 上"></a>加州游记 上</h2><h3 id="A380体验"><a href="#A380体验" class="headerlink" title="A380体验"></a>A380体验</h3><p>9点20分起飞。我们大概五点多一点到达了白云机场。国际航班一般都要提前3个小时到达机场</p>
<p>到达白云机场，在登机之前。完全不知道要有EVUS才能登机。还好有补救措施。可以到咨询台那里办理，大概每人300块钱吧。当时很担心会因为这个耽误了登机。<br>幸好，那工作人员姐姐，还是很快就办理完成了，大概只花了15分钟两个人。</p>
<p>在候机大厅里那里吃了面条，大概是70块钱这样吧。还不算很贵的。然后我们就坐在那儿等待起飞。<br>我们透过玻璃拍了一些A380的照片。那飞机特别大，有四个螺旋桨。</p>
<p>八点半多久开始登记了。因为这飞机人多，所以登机的时间也需要提前比较长，所有人登机完都要花费一个多小时。我们的位置是在第一层最靠后的中间位置，我旁边就是上二层的楼梯口。<br><img src="/images/2017/california_trip/IMG_0524.jpg" alt=""></p>
<p>晚上十一点的时候飞机的乘务人员就给我们准备了晚餐，确切说是夜宵吧。还不错，吃完就睡觉了。飞机的空调有点大，找空姐要了两张被子。<br>在飞机上睡觉还是比较不舒服的。因为座椅并不能打很低。<br><img src="/images/2017/california_trip/IMG_0535.jpg" alt=""><br>勉强能睡着，在飞机上我大概睡了5-6个小时吧，就醒来了，打开窗看看风景吧，只看到底下是一片太平洋。</p>
<h3 id="刚到美国"><a href="#刚到美国" class="headerlink" title="刚到美国"></a>刚到美国</h3><p>飞机抵达洛杉矶，很兴奋地去过关，我还对着海关人员说how are you。。边检冷冷地问我来美国干嘛的？我说旅游的。<br>自驾从1号公路到旧金山。我说一号公路，那海关人员说什么！？ 我再解释了一遍，我说租了一辆车走沿海的一号公路去旧金山。<br>– 噢！知道了，你早说嘛，走吧。<br>– Thank you sir.<br>就这么通过了。</p>
<p>进入到机场里面，机场比我想象中落后好多呀。人不多，而且感觉大门一点都不气派，像是国内一个二线城市机场的水平。<br><img src="/images/2017/california_trip/IMG_0552.jpg" alt=""><br>走出门口，手机连不上网，我不知道怎么打车。我就问问路边开车拉客（貌似是开黑车）的老黑，问他到那个地点要多少钱，他说要30美金，<br>我要这么多啊，那老黑很自信地说道：不信你问别的司机。我就去问不远处的的士司机，司机是一个亚洲面孔，顿时让我放心很多，<br>他看了看地图，查了下，说大概20美金。于是我们就打了这辆的士到达我在Airbnb提前预定的旅店。聊天中知道司机是个韩国人。</p>
<p>大概坐车20分钟，到了住所。见到了房东的老婆，一个菲律宾人，她非常友好热情地招呼了我们。在美国，<br>遇到亚洲面孔感觉都会比较亲切，比较放松，没那么紧张。</p>
<p>这是房东Matt家的厅<br><img src="/images/2017/california_trip/IMG_0572.jpg" alt=""><br>厨房<br><img src="/images/2017/california_trip/IMG_0574.jpg" alt=""></p>
<h3 id="提车"><a href="#提车" class="headerlink" title="提车"></a>提车</h3><p>第二天早上，起来就看到房东了，非常高大的白人，比起他老婆内敛很多。他也在机场上班就顺路开车载我们去机场拿车。<br>这就是我这7天的座驾了<br><img src="/images/2017/california_trip/IMG_0583.jpg" alt=""></p>
<p>动力还挺强劲的。</p>
<h3 id="Staples-Center"><a href="#Staples-Center" class="headerlink" title="Staples Center"></a>Staples Center</h3><p>拿到车，我们就在洛杉矶四处乱逛了。真的是乱逛，一点计划都没有的。<br>洛杉矶某个角落<br><img src="/images/2017/california_trip/IMG_0593.jpg" alt=""><br>下午2点回到旅店休息，本来打算睡个午觉的，没想时差还没有倒过来，睡了一大觉到晚上九点多了。就开车出去逛了。</p>
<p>湖人主场<br><img src="/images/2017/california_trip/IMG_0626.jpg" alt=""><br>湖人主场附近<br><img src="/images/2017/california_trip/IMG_0633.jpg" alt=""></p>
<p>湖人主场附近豪车多<br><img src="/images/2017/california_trip/IMG_0631.jpg" alt=""></p>
<h3 id="Beverly-Hill"><a href="#Beverly-Hill" class="headerlink" title="Beverly Hill"></a>Beverly Hill</h3><p>在美国走斑马线过马路。刚开始不知道白色的灯表示通过。有一次在比佛利山庄附近过马路时候，忘了是什么颜色的灯，本来是应该车通过的，结果我跑过马路，那些车都准备启动，我就这么跑过去了。<br>我不太确定是否可以过，然后就急急忙忙跑过去了。好尴尬。结果那些车还是让我了。</p>
<p>来富人区这看看<br><img src="/images/2017/california_trip/IMG_0643.jpg" alt=""></p>
<h3 id="Holly-Wood"><a href="#Holly-Wood" class="headerlink" title="Holly Wood"></a>Holly Wood</h3><p>走路来星光大道<br><img src="/images/2017/california_trip/IMG_0678.jpg" alt=""></p>
<h3 id="UCLA"><a href="#UCLA" class="headerlink" title="UCLA"></a>UCLA</h3><p>晚上又是倒时差，出来逛逛UCLA。UCLA挺豪华的感觉，设施都很新。<br>很大的停车场<br><img src="/images/2017/california_trip/IMG_0689.jpg" alt=""></p>
<p>教学楼<br><img src="/images/2017/california_trip/IMG_0694.jpg" alt=""></p>
<h3 id="圣莫妮卡"><a href="#圣莫妮卡" class="headerlink" title="圣莫妮卡"></a>圣莫妮卡</h3><p>还记得那天晚上接近凌晨2点钟的洛杉矶，我没有订到酒店，于是漫无目的地在洛杉矶街头上开着车，由于GTA5真人版。我一直往圣莫妮卡的方向前行。<br>走到快到海边附近，我尿急想附近找个地方解决，于是看到不远的地方有个麦当劳，于是我把车开到里，发现凌晨两点钟居然还有这么多人，大部分都是黑人，那个地方。<br>我走进麦当劳餐厅里面，我发现身边的都是黑人，白人都几乎没有，更没有黄种人了。我走进去他们都看着我，我很担心他们因为种族不同欺负我，我头不敢乱晃紧紧盯着前方，进去厕所后把尿撒完后赶紧就跑路了。<br>那里的人貌似都是homeless的，看起来都很穷。这么晚还不回家，应该就是无家可归的人吧。</p>
<p>仿佛来到了现场版的侠盗车手<br><img src="/images/2017/california_trip/IMG_0705.jpg" alt=""></p>
<p><img src="/images/2017/california_trip/IMG_0733.jpg" alt=""><br>圣莫妮卡的海边<br><img src="/images/2017/california_trip/IMG_0736.jpg" alt=""></p>
<p>洛杉矶之旅就告一段落了，接下来是一段沿海一号公路长途自驾游。 </p>
]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
        <tag>travel</tag>
      </tags>
  </entry>
  <entry>
    <title>怎么成为一个Apache Contributor贡献者？</title>
    <url>/2017/how_tobe_apache_contributor.html</url>
    <content><![CDATA[<h1 id="通往Apache-Contributor之路"><a href="#通往Apache-Contributor之路" class="headerlink" title="通往Apache Contributor之路"></a>通往Apache Contributor之路</h1><h2 id="根据个人贡献获得价值（Government-By-Merit）"><a href="#根据个人贡献获得价值（Government-By-Merit）" class="headerlink" title="根据个人贡献获得价值（Government By Merit）"></a>根据个人贡献获得价值（Government By Merit）</h2><p>回忆我刚参与ServiceComb项目，面对上万行的存量代码，总觉得无从下手，甚至认为开源社区高手如云，如果没有深厚且对口的技术功底，还是不要来掺和了。</p>
<p>在这个困难而关键的时候，社区导师给了我明确的指导——不要怕，从小事做起，不要“善小而不为”。于是我静下心来，在Jira上寻找最简单的任务，主动请缨的第一个任务是支持配置兼容，具体需求是cse.xxx配置项和servicecomb.xxx配置项要具备等同效果，经过一番努力，成功Merge PR ；之后我又接下另一个简单任务，增加一个Annotation用于支持Json String作为请求参数……</p>
<p>Apache Way非常看重个人贡献，没有贡献，一切无从谈起，与开源软件同行，不仅看你获得了多少，更要坚持长期贡献，这是它与商业软件最大的不同。贡献并不区分大小，无论是增加重大特性，还是小小的改进、Bug Fix和修订文档错误，这些同样是项目茁壮成长的关键。</p>
<p>实际上大多数开源爱好者都是从修订文档错误开始的，例如改正错别字、格式不正确以及订正描述等等，我对ServiceComb的理解也绝大多数来自这方面的工作；这样不但能够在阅读文档的过程中更快的了解技术细节，也比较容易入手做出贡献。</p>
<p>总之坚持下来，个人积累的贡献慢慢变厚，获得Apache的认可自然水到渠成。</p>
<h2 id="社区驱动（Community）"><a href="#社区驱动（Community）" class="headerlink" title="社区驱动（Community）"></a>社区驱动（Community）</h2><p>参与社区是技术成长最快的方式之一，Follow Apache社区的方式有订阅邮件列表和加入Gitter聊天室；从看大家讨论（讨论邮件一般会使用[Discussion]开头），到回答大家的问题（回复邮件和发送Gitter聊天），相信用不了多久你就能收获颇丰，并冒出自己的想法，迈出第一步提交第一个PR也就不难了。</p>
<p>ServiceComb作为一个微服务一站式解决方案，融合侵入式、非侵入式场景并支持多语言，解放开发者，帮助用户和开发者将企业应用轻松微服务化和上云；大家在这里讨论的话题、发起的投票、以及提交的代码，无不与微服务密切相关。在这个社区中我不但学习到了知识，完成了开源（也包含微服务）小白的蜕变，还进一步接受了开源的洗礼——前辈指导我进步，我将所学传递给新人；这个过程中我结识了很多新朋友，大家互通有无，不但极大的开阔了视野，也提高了自己的社交能力。</p>
<p>Apache开发者来自全球，社区大多都是用英文来交流。通过阅读英文资料，使用英文在Gitter上与开发人员直接交流，通过英文邮件来探讨问题，在不知不觉中自己的英文水平也大大提高了。</p>
<h2 id="协作开发（Collaborative-Development）"><a href="#协作开发（Collaborative-Development）" class="headerlink" title="协作开发（Collaborative Development）"></a>协作开发（Collaborative Development）</h2><p>这也是我极力推荐参与开源社区开发的重要原因之一，当你提交PR后，无论代码有多么烂，你总能收获宝贵的Comments，不花钱获得编程大师的指点——教你怎么写出优秀的代码，这是多么合算的买卖！</p>
<p>我在参与ServiceComb社区前，只知道Java基本语法，IDE不熟练（之前一直是用VS写C#，不使用Eclipse和IntelliJ IDEA），不会Git，不懂Maven，还能有更糟糕的起点吗：）</p>
<p>不用担心，社区会指导你。我前文提到的第一个简单的任务，花费了将近一周时间，被打回来了四五次后才被Merge；一个PR收获60+ Comments也是家常便饭。这个过程中我的Java水平成长得飞快，不久后就能独立承担新特性的设计和开发——Metrics，当然，完成这个新特性的过程中Committer和其他开发者给予了很多支持，所以，请大胆的提交你的第一个PR，成为一名Contributor吧。</p>
<h2 id="民主，开放和透明（Consensus-Open-and-Transparency）"><a href="#民主，开放和透明（Consensus-Open-and-Transparency）" class="headerlink" title="民主，开放和透明（Consensus, Open and Transparency）"></a>民主，开放和透明（Consensus, Open and Transparency）</h2><p>在Apache社区里投票至关重要，你可以感受到你的建议将被充分重视，我很喜欢这种参与感；大家的讨论和建议也完全公开透明，并且能够长时间通过邮件列表查询到，沟通效率非常高。所以大家多多参与，一定能收获惊喜，从万能的社区里寻找自己想要的答案，请记住，当你对某个问题产生困惑，即使是强大的StackOverflow也不会有原作者的答复更为准确。</p>
<p>写在最后，我想说从一名开源小白成长为Apache Committer并不是无比艰难又遥不可及的事情，只需要日积月累的在社区中投入一点业余时间，就能梦想成真。当然最好找到自己感兴趣的社区，这会让这段旅途更加愉快，也能交到更多志同道合的的朋友。</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://servicecomb.apache.org/cn/docs/how-to-grow-up-to-be-an-apache-committer/" target="_blank" rel="external">https://servicecomb.apache.org/cn/docs/how-to-grow-up-to-be-an-apache-committer/</a></p>
]]></content>
      <categories>
        <category>github</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka日常操作指南</title>
    <url>/2017/kafka_daily_command.html</url>
    <content><![CDATA[<h2 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h2><h3 id="查看topic"><a href="#查看topic" class="headerlink" title="查看topic"></a>查看topic</h3><p><code>kafka-topics --list --zookeeper device1:2181</code></p>
<h3 id="查看topic-partitions状态"><a href="#查看topic-partitions状态" class="headerlink" title="查看topic partitions状态"></a>查看topic partitions状态</h3><p><code>kafka-topics --describe --zookeeper device1:2181 --topic zhihu_comment</code></p>
<blockquote>
<p>这个命令很有用。有一次我的consumer全部设置正常，但无论如何都不能commit current offset。就想这是不会zookeeper的问题呢？<br>因为zookeeper是存放commit offset 信息的地方<br>经过仔细盘查，我发现在另外的topic却可以正常的commit，所以这是该topic的问题，于是要检查下这个<br>topic哪里出了问题，就用上了这个命令。结果发现有2个partition莫名的消失了，也不知道是为什么。但<br>总的来说还是把问题定位出来了。<br>因为，这个问题，我花了不少精力，时间不说，我还特意花了50美金去upwork上找人才解决的。</p>
</blockquote>
<p>后来又发现，因为我是用kafka-python来创建的topic，这样的创建导致了 committed offset 没有显示。<br>但用console创建的topic则不会。<br><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_topic</span><span class="params">(topic)</span>:</span></div><div class="line">    admin_client = KafkaAdminClient(bootstrap_servers=broker_list, client_id=<span class="string">'test'</span>)</div><div class="line">    topic_list = []</div><div class="line">    topic_list.append(NewTopic(name=topic, num_partitions=<span class="number">3</span>, replication_factor=<span class="number">1</span>))</div><div class="line">    admin_client.create_topics(new_topics=topic_list, validate_only=<span class="keyword">False</span>)</div></pre></td></tr></table></figure></p>
<h3 id="创建topic"><a href="#创建topic" class="headerlink" title="创建topic"></a>创建topic</h3><p>kafka-topics –create –zookeeper device1:2181 –replication-factor 1 –partitions 1 –topic test</p>
<h3 id="查看group每个partition-offset的状态"><a href="#查看group每个partition-offset的状态" class="headerlink" title="查看group每个partition offset的状态"></a>查看group每个partition offset的状态</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">kafka-consumer-groups --bootstrap-server device1:9092 --group test --describe</div></pre></td></tr></table></figure>
<h3 id="在console启动-producer-（一般可用来测试）"><a href="#在console启动-producer-（一般可用来测试）" class="headerlink" title="在console启动 producer （一般可用来测试）"></a>在console启动 producer （一般可用来测试）</h3><h4 id="删除某个topic的数据"><a href="#删除某个topic的数据" class="headerlink" title="删除某个topic的数据"></a>删除某个topic的数据</h4><blockquote>
<p>Solution1<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">kafka-topics.sh --delete --zookeeper device1:2181 --topic zhihu_test</div></pre></td></tr></table></figure></p>
<p>Solution2</p>
<p>#删除zhihu_profile topic 的所有数据<br>kafka-configs –zookeeper device1:2181 –entity-type topics –alter –entity-name zhihu_profile –add-config retention.ms=1000</p>
<h1 id="再把retention设置调回来"><a href="#再把retention设置调回来" class="headerlink" title="再把retention设置调回来"></a>再把retention设置调回来</h1><p>kafka-configs –zookeeper device1:2181 –entity-type topics –alter –entity-name zhihu_profile –add-config retention.ms=8640000000<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">但这种方法不能清除offset 和 commit的记录，因为我查了下某个group对应的offset如下：</div></pre></td></tr></table></figure></p>
</blockquote>
<p>topic: zhihu_profile partition: 0 committed: 2515555 last: 2515558 lag: 3<br>topic: zhihu_profile partition: 1 committed: 2519850 last: 2519853 lag: 3<br>topic: zhihu_profile partition: 2 committed: 2520594 last: 2520596 lag: 2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">查询某个group offset的代码：</div></pre></td></tr></table></figure>
<p>def get_current_offset(topic, group):</p>
<pre><code>consumer = KafkaConsumer(
        bootstrap_servers=broker_list,
        group_id=group,
        enable_auto_commit=False
    )   

for p in consumer.partitions_for_topic(topic):
    tp = TopicPartition(topic, p)
    consumer.assign([tp])
    committed = consumer.committed(tp)
    consumer.seek_to_end(tp)
    last_offset = consumer.position(tp)
    print(&quot;topic: %s partition: %s committed: %s last: %s lag: %s&quot; % (topic, p, committed, last_offset, (last_offset - committed)))

consumer.close(autocommit=False)
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">### reset offset</div></pre></td></tr></table></figure>
<p>kafka-consumer-groups –bootstrap-server device1:9092 \<br>–group my-group –reset-offsets –to-earliest –all-topics –execute<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"></div><div class="line">&gt; Reset offset of topic foo partition 0 to 1</div><div class="line"></div><div class="line">`--reset-offsets --group test.group --topic foo:0 --to-offset 1`</div><div class="line"></div><div class="line">&gt; Reset offset of topic foo partition 0,1,2 to earliest</div><div class="line"></div><div class="line">`--reset-offsets --group test.group --topic foo:0,1,2 --to-earliest`</div><div class="line"></div><div class="line">### 增加broker节点</div><div class="line">增加节点后，要重新新建Topic才能用到该broker。另外，kafka要依赖zookeeper，因为会把</div><div class="line">offset写入到zk。这样，如果zk的集群很大（zk节点多）的时候，会影响zk的写入速度。建议是用</div><div class="line">SSD配置给ZK（只给zk就行），然后这组zk专门服务于kafka，不要做其他用。</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"># TL;DR</div><div class="line">## 关于 auto.offset.reset 和 offset</div></pre></td></tr></table></figure></p>
<p>241</p>
<p>It is a bit more complex than you described. The auto.offset.reset config kicks in ONLY if your consumer group does not have a valid offset committed somewhere (2 supported offset storages now are Kafka and Zookeeper). And it also depends on what sort of consumer you use.</p>
<p>If you use a high-level java consumer then imagine following scenarios:</p>
<p>You have a consumer in a consumer group group1 that has consumed 5 messages and died. Next time you start this consumer it won’t even use that auto.offset.reset config and will continue from the place it died because it will just fetch the stored offset from the offset storage (Kafka or ZK as I mentioned).</p>
<p>You have messages in a topic (like you described) and you start a consumer in a new consumer group group2. There is no offset stored anywhere and this time the auto.offset.reset config will decide whether to start from the beginning of the topic (smallest) or from the end of the topic (largest)</p>
<p>One more thing that affects what offset value will correspond to smallest and largest configs is log retention policy. Imagine you have a topic with retention configured to 1 hour. You produce 5 messages, and then an hour later you post 5 more messages. The largest offset will still remain the same as in previous example but the smallest one won’t be able to be 0 because Kafka will already remove these messages and thus the smallest available offset will be 5.</p>
<p>Everything mentioned above is not related to SimpleConsumer and every time you run it, it will decide where to start from using the auto.offset.reset config.<br>```</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://stackoverflow.com/questions/29791268/how-to-change-start-offset-for-topic?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa" target="_blank" rel="external">https://stackoverflow.com/questions/29791268/how-to-change-start-offset-for-topic?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa</a></p>
]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>每周的总结与记录</title>
    <url>/2017/dennis_weekly.html</url>
    <content><![CDATA[<h3 id="20190924"><a href="#20190924" class="headerlink" title="20190924"></a>20190924</h3><p>早上喝了利动乳果糖口服溶液然后吃早餐，到公司九点多感觉肚子疼。于是拉肚子了。中午说拉肚子了就喝粥吧，就去红荔村吃皮蛋瘦肉粥，结果吃完后回来肚子不停地咕咕叫，叫的特别厉害。而且肚子很涨，越来越胀，然后开始涨疼了。我一直想忍过去，但一直都还会涨疼，还喝了矿泉水，冷的，更疼。下午三点多4楼看医生，说我不应该吃皮蛋瘦肉粥，他说腹泻后不能吃蛋白质类的食物，牛奶都不要喝。只能喝白粥。然后开了左氟沙星和奥美拉唑。他妈的，我以为肚子胀可以吃皮蛋瘦肉粥这些，因为这也算粥，但不知道是不能吃的。</p>
<p>晚上十点终于把皮蛋瘦肉粥拉出来了，感觉轻松一点。以后再也不好喝皮蛋瘦肉粥了。</p>
<h3 id="20191010"><a href="#20191010" class="headerlink" title="20191010"></a>20191010</h3><p>今天为了解决保存task result的task name花了很多时间看celery的源代码，也不断地尝试测试，收益很低，没有解决到问题。感觉看源码要有很强的专注力，而且要能记住很多各个不同调用的关系，不然看着就会发蒙，不知道自己要干什么。另外，Celery有些代码写着还是挺难懂的，比如说task 装饰品那部分代码。加上网络一直不稳定，尤其是连接国外的服务器。卡卡的很影响效率。然后tmux Pycharm这些工具用的不太熟练也对效率影响挺大的，要不停地去网上查文档。此外今天早上偶尔发现一个YOUTUBE的商机，偶尔看到了一个数据动态图，然后我觉得还是比较容易效仿的。最重要的是，收看率更好，而且貌似最近才兴起的，我看那些up主的历史记录都很短，最多也就半年多。</p>
<h3 id="20191013"><a href="#20191013" class="headerlink" title="20191013"></a>20191013</h3><p>今天发现爬虫的airflow调度部署很不顺利，比我想象中遇到很多麻烦。主要是执行命令时候环境变量没有起作用。Python用的是默认的解析器。反正陆陆续续弄了两三天还没弄好。<br>今天还有一件事情挺让我生气的。下午电信宽带突然就抽风了，停网了一个小时，迫于没完成Airflow事情的压力，现在又上不了网了，感觉今天什么事情都没干成。不过后来，让我有些感觉柳暗花明又一村。那电信师傅告诉我挺多的，首先他让我不要用那个房间的路由器了，因为功率低，而且有线网络达不到两百兆。然后，他让我直接用路由器来拨号，厅里那个光猫就充当中继器的作用。这样不不会把房间的路由器作为一个二级路由。有钱的话可以在淘宝买个有光纤接口的路由器。</p>
<h3 id="20191018"><a href="#20191018" class="headerlink" title="20191018"></a>20191018</h3><p>最近3天主要都是在搞Hive优化的工作，真是太难了，没有什么实质性进展，时间花费性价比很低。用了很多办法都没有实际性加快HQL运行的速度。主要是花了很多时间在Google搜索，然后各种查阅文档，然后尝试。包括在SO上发帖提问。<br>今天跟一面前同事霞琳吃饭饭聊天。收获还是挺多的，她说她已经收到了腾讯的Offer准备跳槽到。她是通过内推进去的。之前她也投过很多简历，很多也是石沉大海，也被不少公司面试后拒绝。有时候找工作真是要看运气。她和我不同的地方就是，她4月份就离开了一面，然后到9月份才开始工作。现在又拿到了腾讯的正式offer，真是苦尽甘来呀。不同的地方就是她大概经历了半年没有工作没有收入的压力，能够沉住气，这确实需要一定的勇气。而我找了一个月就来入职了。这点我还是非常佩服她的。<br>她说我可以参加一些分享会，认识一些行业内的人物。比如说她之前参加了TiDB的分享会，可以现场结识一些大牛。总结下来，就是找工作要运气，当然自身能力是前提。</p>
<h3 id="20191021"><a href="#20191021" class="headerlink" title="20191021"></a>20191021</h3><p>到现在这个数据量，接近亿级的数量，明显感觉到服务器很卡，运行时间单位都是上小时的。不知道从哪里入手优化任务。即使是用spark on yarn，依然很慢。看个主机Htop的资源利用率并不高，50%都没有用到。不知道集群内部在搞什么，完全是个黑箱操作，我只能默默地等待着。哎，优化之路很艰难。最根本的原因我觉得还是我的硬件资源不够。有钱的话还是再买一台服务器好点，或者先加一16G内存也好。</p>
<h3 id="20191212"><a href="#20191212" class="headerlink" title="20191212"></a>20191212</h3><p>这周花了些时间为Aiflow提交PR，也是没有什么进展，没有别被merge也没有被拒绝，有点心塞。其它的时间主要看了下新买的书<br>《Spark内核设计的艺术》</p>
<h3 id="20191220"><a href="#20191220" class="headerlink" title="20191220"></a>20191220</h3><p>这一个礼拜主要是忙着面试的准备。在看了Kyligence的职位后，感觉无论是公司和职位都是我非常青睐的职位，所以这周<br>我花了很多精力和时间在准备来面试这个岗位，甚至准备了一个面试的PPT，在经过两轮面试后，还是待定，比较失落。导致我<br>现在没有什么心思干别的事情了。</p>
<h3 id="20190105"><a href="#20190105" class="headerlink" title="20190105"></a>20190105</h3><p>这个礼拜主要忙于接收Offer，提交离职，准备交接工作。周五请同事吃饭，周六请客吃饭。周天打台球，<br>打羽毛球。</p>
<h3 id="20200126"><a href="#20200126" class="headerlink" title="20200126"></a>20200126</h3><p>本周主要回家过年了。大部分时间都上YouTube追踪最新的武汉肺炎的疫情，国内的信息真是太闭塞了。政府一味地去隐瞒<br>真相，呵呵，和HBO拍的《切尔诺贝利》一模一样啊。<br>然后花了点时间看了看Spark Internal这书，不过效率不是很高。</p>
<h3 id="20202016"><a href="#20202016" class="headerlink" title="20202016"></a>20202016</h3><p>回顾下昨天，昨天的一部分效率还可以，自己实现了一个gitlab backup程序。但是有一大段时间过得效率很低。<br>这是因为，要进行一个公司的在线技术测试。测试内容是Hadoop基础。首先要在内网上下载了一个很大的视频文件，<br>大概6G多。下载完后，让我感到失望的是，我在视频中收益与时间的比太低了，两段视频看了我3个多小时，都是一些<br>很入门我早就知道的内容，而且讲得很啰嗦。然后测试的题目质量也比较低，有的答案还是不对的。这让我很失望，<br>感觉让费我时间了。</p>
<p>上面这段经历只能说比较无奈吧，碰到这样也难以改变。下面，对这周做的事情，我还是有些东西可以总结的。首先，<br>整体上这周的效率都不错，入职第一周，全面大概地实操学习了KE，学到了很多kylin的思想和原理。</p>
<p>另外，在执行效率上，我发现了自己的一个问题。我不会平衡工作和休息，就是会很容易沉溺于寻思解决某个问题，<br>不能自拔。比如：在编写一段代码的时候，一直坐在电脑前，一坐就是几个小时，没解决完就不舒服。中途甚至憋着<br>尿也不愿意上趟厕所休息下。<br>还有一种例子就是：成功做完了某个事情，或说完成了当天的计划，但却不甘心，老是想着再来一点，再多做一些事情。<br>这种贪婪的心思导致让我又做在电脑前继续搞事情。<br>总结一下就是：不懂得如何达到平衡，而是一味地蛮干。不懂得休息，这很可能会对自己身体造成损伤，我后面几天<br>坐久了就又开始屁股痛了。前两天坐久了就休息下，换个姿势，点点眼药水，这对屁股眼镜都好。<br>其实，如果不懂得寻寻渐进，一味地沉溺于某件事不能自拔，这看似很努力，但很可能是丢了西瓜拣芝麻。并不是一种<br>良性循环。<br>解决问题的方案：我想尝试一下番茄工作法，比如每工作30分钟休息5分钟，看看效果。</p>
]]></content>
      <categories>
        <category>daily</category>
      </categories>
      <tags>
        <tag>daily</tag>
      </tags>
  </entry>
  <entry>
    <title>Kylin 基础介绍</title>
    <url>/2017/kylin_basic.html</url>
    <content><![CDATA[<h2 id="Kylin术语"><a href="#Kylin术语" class="headerlink" title="Kylin术语"></a>Kylin术语</h2><ul>
<li><p>维度(Dimension)<br>一组属性，提供结构化的标签信息，一般作为报表的坐标轴。</p>
</li>
<li><p>度量(Measure)<br>一类可以进行聚合分析的特殊维度，聚合后的结果称为指标。</p>
</li>
<li><p>事实表(Fact Table)<br>数据仓库中的中央表，用于描述业务内特定事件的数据。</p>
</li>
<li><p>维度表(Lookup Table)<br>维度属性的集合，人们观察数据的特定角度。</p>
</li>
<li><p>基度（Cardinality）<br>指数据表中某一列数据去重后的元素个数。</p>
</li>
<li><p>星型模型(Star Schema)<br>一种多维的数据关系，由一张事实表和一组维度表组成。</p>
</li>
<li><p>cube<br>一个cube就是一个Hive表的数据按照指定维度与指标计算出的所有组合结果。</p>
</li>
<li><p>cuboid<br>某一维度组合下，度量聚合后的结果集合。有个特殊的cuboid叫 base cuboid，比如维度有ABCD，（A,B,C,D）称为Base cuboid</p>
</li>
<li><p>数据立方体(cube)<br>一组用于分析数据的相关度量值和维度，是所有cuboid的集合，作为存储和分析的基本单位</p>
</li>
<li><p>segment<br>Cube Segment 是指针对源数据中的某一个片段，计算出来的 Cube 数据。<br>通常数据仓库中的数据数量会随着时间的增长而增长，而 Cube Segment 也是按时间顺序来构建的。</p>
</li>
<li><p>衍生维度<br>维度表上的某一列通过PK可推的，换句话是可查询到的 比如由身份证可以推导性别，所以能通过PK推导的维度都可以设置为衍生维度。<br>衍生维度是放在snapshot中</p>
</li>
</ul>
<h3 id="Kylin核心技术：Cube与计算"><a href="#Kylin核心技术：Cube与计算" class="headerlink" title="Kylin核心技术：Cube与计算"></a>Kylin核心技术：Cube与计算</h3><p>比如我们有10多亿中国人口数据，要统计男女比例这么个需求。如果直接从硬盘里提取数据来统计肯定会很慢，磁盘速度大概是几百兆/秒。<br>即使把数据放到内存中也最多提升100倍的速度。</p>
<p>所以，可以用与计算来统计男女比例</p>
<h3 id="维度表和事实表"><a href="#维度表和事实表" class="headerlink" title="维度表和事实表"></a>维度表和事实表</h3><p>我学到了一个数据仓库的概念，维度表（Fact Table）和事实表（Lookup Table）。<br>通过下面例子就让我明白了：<br>学过数据库的童鞋应该都知道星型模型，星型模型在数据仓库的设计中可以为是一种典型的维度模型。<br>我们在进行维度建模的时候会建一张事实表，这个事实表就是星型模型的中心，然后会有一堆维度表，这些维度表就是向外发散的星星。<br>那么什么是事实表、什么又是维度表吗，下面会专门来解释。</p>
<p><img src="/images/2017/kylin_basic/367a4515.png" alt=""></p>
<p>中间的chat表就是事实表。其它周边的表就是维度表。</p>
<h3 id="维度设计"><a href="#维度设计" class="headerlink" title="维度设计"></a>维度设计</h3><p>只有普通维度会影响Cuboid的数量和存储膨胀率，衍生维度并不参与Cuboid计算，而是有衍生维度对应的外键FK参与计算Cuboid。<br>在查询时，对衍生维度的查询会首先转换为对外键所在维度的查询，因此会牺牲少量查询性能。</p>
<p>Kylin暂时不支持星座模型</p>
<p>默认情况下，系统将会为所有的维度表创建快照。<br>设计新的模型，将大维度表改为事实表</p>
<p>设计规则如下：<br>经常出现在规律条件中的维度应当设为普通维度，以达到更好的查询性能。</p>
<p>Derived 衍生类型的前提是要放到快照表里的</p>
<ul>
<li>For normal dimensions, Kyligence has max number restriction of 62</li>
<li>If a dimension table is used in cube design, its corresponding fact table foreign key will be added as normal dimension even it’s not used.</li>
</ul>
<h3 id="维度优化"><a href="#维度优化" class="headerlink" title="维度优化"></a>维度优化</h3><p>Mandatory Dimension: 一定group by的字段<br>Hierarchy Dimension: 国家、省、市<br>Joint Dimension: 要么一起出现，要么不出现，比如：A、B、C绑在了一起 （很少有这样的场景）</p>
<h3 id="Rowkey"><a href="#Rowkey" class="headerlink" title="Rowkey"></a>Rowkey</h3><p>UHC不要用Dict作为encoding;<br>常用的字段比如日期字段要放在Rowkey的前面<br>高基维放置在低基维前面<br>过滤的维度放置在非过滤前面</p>
<p>max length of FixedLength(N) for UHC of varchar/char type will be 256</p>
<h3 id="Cube-Build"><a href="#Cube-Build" class="headerlink" title="Cube Build"></a>Cube Build</h3><p>Supported Cube Build Types:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Full Build</div><div class="line">By Date/time</div><div class="line">Streaming(by offset)</div><div class="line">By File(Beta)</div><div class="line">Customize(Beta)</div></pre></td></tr></table></figure></p>
<p>尽量使用增量加载<br>计划合并你的增量segment<br>用增量加载的时候要考虑下SCD</p>
<h3 id="星型模型和雪花模型"><a href="#星型模型和雪花模型" class="headerlink" title="星型模型和雪花模型"></a>星型模型和雪花模型</h3><p>fact table 外围一圈 dimension table 就是星型模型<br>dimension table 外再围一圈 dimension table就是雪花模型</p>
<h3 id="System-Configuration"><a href="#System-Configuration" class="headerlink" title="System Configuration"></a>System Configuration</h3><p>Important configurations:<br>Job retry<br>Pushdown：对于没有cube能查得结果的sql，Kylin支持将这类查询通过JDBC下压至备用查询引擎如Hive, SparkSQL, Impala等来查得结果<br>Query cache(process level/shared)<br>Compression</p>
<p>Configuration override<br>System<br>Project<br>Cube</p>
<h3 id="Authorization-Management"><a href="#Authorization-Management" class="headerlink" title="Authorization Management"></a>Authorization Management</h3><p>project: Query, Operation , Managment, Admin<br>table: Table query, Row query, Column query</p>
<p>Caution: column ACL is a blacklist.</p>
<h3 id="Resource-Isolation"><a href="#Resource-Isolation" class="headerlink" title="Resource Isolation"></a>Resource Isolation</h3><p>KE can submit cube build jobs to specified Yarn queues per: system level;project level;cube level</p>
<h3 id="应用发布流程"><a href="#应用发布流程" class="headerlink" title="应用发布流程"></a>应用发布流程</h3><p>DEV -&gt; QA -&gt;(sign off) PROD</p>
<h3 id="Daily-Operation"><a href="#Daily-Operation" class="headerlink" title="Daily Operation"></a>Daily Operation</h3><ol>
<li><p>Garbage cleanup<br>e.g.: segment merged, cube purged</p>
</li>
<li><p>System upgrade<br>Minor version upgrade<br>Major version upgrade</p>
</li>
<li><p>System backup/restore<br>System level<br>Project level</p>
</li>
</ol>
<h3 id="Rest-API"><a href="#Rest-API" class="headerlink" title="Rest API"></a>Rest API</h3><p>Authorization 这个Header的内容是 “ADMIN:yourpassword”的base64 encode</p>
<h3 id="Hadoop生态和和MPP数据库的区别"><a href="#Hadoop生态和和MPP数据库的区别" class="headerlink" title="Hadoop生态和和MPP数据库的区别"></a>Hadoop生态和和MPP数据库的区别</h3><p>Hadoop跟MPP的存储模型不一样。<br>Hadoop生态的存储用HDFS，HDFS的扩展是通过元数据来做的，它有中心节点用来存元数据，在加入新的节点的时候，<br>只需要修改元数据就可以，所以HDFS可扩展能力是收到管理元数据那台机器的性能限制的。一般来说可以到10K这个<br>规模，再向上就不行了。</p>
<p>而MPP要自己做切分，自己做切分就带来动态调整的问题。MPP通常采用的是没有中心节点的存储模式，比如hash，<br>每增加节点的时候，都需要rehash，这样当规模到了几百台的时候，扩展能力就下来了。</p>
<p>Hive在内存管理上方式不大一样。<br>MPP内存管理比较精细，他主要的想法是在每个机器上放个数据库，传统数据库的内存管理比较复杂，主要是内外存<br>交互的东西，这样的架构决定了MPP在小数据量的时候，latency可以做的比较小，但是数据量大的时候，<br>throughput做不上去了。<br>而Hive的内存管理非常粗放，它后来就是MapReduce的job，MR的job是没有太多精细的内存管理，就是拼命地<br>scan，完了就是spill，这样的架构导致throughput很大，但是latency很高。</p>
<p>failover机制，<br>Hive的failover就是MR的failover，job挂掉了重新换机器跑就完了。</p>
<p>MPP它的计算是要attach到数据节点上去的，如果你规模上去，那么fail的可能性就上去了，这样如果你每次计算<br>都有台机器挂了，一挂，别人就要等你，而不是换台机器继续跑。可以用木桶效应类比。</p>
<h3 id="Cube设计"><a href="#Cube设计" class="headerlink" title="Cube设计"></a>Cube设计</h3><p>Rowkey的具体格式是cuboid id + 具体的维度值（最新的Rowkey中为了并发查询还加入了ShardKey）<br><img src="/images/2017/kylin_basic/b447988e.png" alt=""><br>前面4个框着的作为Rowkey，比如，2015-10-1的维度序号为1，BJ的序号为0，item-A序号为0，则最终Rowkey为 111+100</p>
<p>Rowkey的设计原则：</p>
<ol>
<li>结合业务场景特点，按照查询频次来放置字段顺序</li>
<li>通过设计的Rowkey尽可能的将数据打散到整个集群中，负载均衡</li>
<li>设计的Rowkey尽量简短</li>
</ol>
<p>有3个方面要考虑：</p>
<ul>
<li><p>Encoding<br>我们的编码方式能够大大减少每行Cube数据的体积。<br>而Cube中可能存在数以亿计的行数，使用编码节约的空间累加起来将是一个非常巨大的数字。</p>
</li>
<li><p>排序<br>高概率用于过滤的列<br>基数高的列（用它进行过滤时，返回的结果集小）<br>排序评分=过滤概率*过滤强度<br>经常使用到的维度提前—在需要进行后聚合的场景中效率会更高。</p>
</li>
<li><p>分片<br>按维度分片（Shard by Dimension）提供了一种更加高效的分片策略，那就是按照某个特定维度进行分片。<br>简单地说，如果Cuboid中某两个行的Shard by Dimension的值相同，<br>那么无论这个Cuboid最终会被划分成多少个分片，这两行数据必然会被分配到同一个分片中。</p>
</li>
</ul>
<p>Shard by维度要求<br>仅支持单一维度<br>基数足够高（几十万）<br>分布均匀<br>经常进行group by和filter</p>
<p>更多的优化原理要参考Rowkey的设计</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://blog.bcmeng.com/post/kylin-cube.html#cuboid-%E7%9A%84%E7%BB%B4%E5%BA%A6%E5%92%8C%E6%8C%87%E6%A0%87%E5%A6%82%E4%BD%95%E8%BD%AC%E6%8D%A2%E4%B8%BAhbase%E7%9A%84kv%E7%BB%93%E6%9E%84" target="_blank" rel="external">https://blog.bcmeng.com/post/kylin-cube.html#cuboid-%E7%9A%84%E7%BB%B4%E5%BA%A6%E5%92%8C%E6%8C%87%E6%A0%87%E5%A6%82%E4%BD%95%E8%BD%AC%E6%8D%A2%E4%B8%BAhbase%E7%9A%84kv%E7%BB%93%E6%9E%84</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kylin</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase 基础介绍</title>
    <url>/2016/hbase_intro.html</url>
    <content><![CDATA[<h1 id="Hbase的由来"><a href="#Hbase的由来" class="headerlink" title="Hbase的由来"></a>Hbase的由来</h1><p>Hbase是Bigtable的开源山寨版本。</p>
<p>是建立的HDFS之上，提供高可靠性、高性能、列存储、可伸缩、实时读写的数据库系统。</p>
<p>它介于NoSQL和RDBMS之间，仅能通过主键(row key)和主键的range来检索数据，仅支持单行事务(可通过hive支持来实现多表join等复杂操作)。<br>主要用来存储非结构化和半结构化的松散数据。</p>
<p>与Hadoop一样，Hbase目标主要依靠横向扩展，通过不断增加廉价的商用服务器，来增加计算和存储能力。</p>
<p>大：一个表可以有上亿行，上百万列</p>
<p>面向列: 面向列(族)的存储和权限控制，列(族)独立检索。</p>
<p>稀疏:对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。</p>
<h1 id="逻辑视图"><a href="#逻辑视图" class="headerlink" title="逻辑视图"></a>逻辑视图</h1><p>HBase以表的形式存储数据。表有行和列组成。列划分为若干个列族(row family)</p>
<h2 id="1-Hbase的检索"><a href="#1-Hbase的检索" class="headerlink" title="1.Hbase的检索"></a>1.Hbase的检索</h2><p>与NoSQL数据库们一样, Row key 是用来检索记录的主键。访问Hbase table中的行，只有三种方式：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1. 通过单个row key访问</div><div class="line"></div><div class="line">2. 通过row key的range</div><div class="line"></div><div class="line">3. 全表扫描</div></pre></td></tr></table></figure></p>
<h2 id="2-Row-key的大小"><a href="#2-Row-key的大小" class="headerlink" title="2. Row key的大小"></a>2. Row key的大小</h2><p>Row key行键 (Row key)可以是任意字符串(最大长度是 64KB，实际应用中长度一般为 10-100bytes)，在hbase内部，row key保存为字节数组。</p>
<ol>
<li>Row key的内部存储方式<br>存储时，数据按照Row key的字典序(byte order)排序存储。设计key时，要充分利用排序存储这个特性，将经常一起读取的行存储放到一起。(位置相关性)</li>
</ol>
<p>注意：字典序对int排序的结果是：</p>
<p>1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…,9,91,92,93,94,95,96,97,98,99。</p>
<p>要保持整形的自然序，行键必须用0作左填充。</p>
<ol>
<li>原子性<br>行的一次读写是原子操作 (不论一次读写多少列)。这个设计决策能够使用户很容易的理解程序在对同一个行进行并发更新操作时的行为。</li>
</ol>
<p>列族</p>
<ol>
<li>列蔟<br>hbase表中的每个列，都归属与某个列族。列族是表的schema的一部分(而列不是)，必须在使用表之前定义。<br>列名都以列族作为前缀。例如courses:history，courses:math都属于courses 这个列族。</li>
</ol>
<p>访问控制、磁盘和内存的使用统计都是在列族层面进行的。<br>实际应用中，列族上的控制权限能帮助我们管理不同类型的应用：我们允许一些应用可以添加新的基本数据,<br>一些应用可以读取基本数据并创建继承的列族、一些应用则只允许浏览数据（甚至可能因为隐私的原因不能浏览所有数据）。</p>
<ol>
<li>时间戳<br>HBase中通过row和columns确定的为一个存贮单元称为cell。<br>每个 cell都保存着同一份数据的多个版本。版本通过时间戳来索引。<br>时间戳的类型是 64位整型。时间戳可以由hbase(在数据写入时自动 )赋值，此时时间戳是精确到毫秒的当前系统时间。<br>时间戳也可以由客户显式赋值。如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳。<br>每个 cell中，不同版本的数据按照时间倒序排序，即最新的数据排在最前面。</li>
</ol>
<p>为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担，Hbase提供了两种数据版本回收方式。<br>一是保存数据的最后n个版本，二是保存最近一段时间内的版本（比如最近七天）。用户可以针对每个列族进行设置。</p>
<p>7.Cell<br>由{row key, column(=<family> + <label>), version} 唯一确定的单元。cell中的数据是没有类型的，全部是字节码形式存贮。</label></family></p>
<h1 id="物理存储"><a href="#物理存储" class="headerlink" title="物理存储"></a>物理存储</h1><p>1． 已经提到过，Table中的所有行都按照row key的字典序排列。</p>
<p>2． Table 在行的方向上分割为多个Hregion。<br><img src="/images/2016/hbase_intro/1e8cda78.png" alt=""></p>
<p>3． region按大小分割的，每个表一开始只有一个region，随着数据不断插入表，region不断增大，当增大到一个阀值的时候，Hregion就会等分会两个新的Hregion。<br>当table中的行不断增多，就会有越来越多的Hregion。<br><img src="/images/2016/hbase_intro/b9c752bf.png" alt=""></p>
<p>4． HRegion是Hbase中分布式存储和负载均衡的最小单元。<br>最小单元就表示不同的Hregion可以分布在不同的HRegion server上。但一个Hregion是不会拆分到多个server上的。</p>
<p>5． HRegion虽然是分布式存储的最小单元，但并不是存储的最小单元。<br>事实上，HRegion由一个或者多个Store组成，每个store保存一个columns family。每个Store又由一个memStore和0至多个StoreFile组成。<br>如图：StoreFile以HFile格式保存在HDFS上。<br><img src="/images/2016/hbase_intro/38134f95.png" alt=""></p>
<ol>
<li>HFile</li>
</ol>
<p>HFile分为六个部分：</p>
<ul>
<li><p>Data Block 段：保存表中的数据，这部分可以被压缩</p>
</li>
<li><p>Meta Block 段： (可选的)–保存用户自定义的kv对，可以被压缩。</p>
</li>
<li><p>File Info 段： Hfile的元信息，不被压缩，用户也可以在这一部分添加自己的元信息。</p>
</li>
<li><p>Data Block Index 段：Data Block的索引。每条索引的key是被索引的block的第一条记录的key。</p>
</li>
<li><p>Meta Block Index段： (可选的)–Meta Block的索引。</p>
</li>
<li><p>Traile：这一段是定长的。保存了每一段的偏移量，读取一个HFile时，会首先读取Trailer，Trailer保存了每个段的起始位置(段的Magic Number用来做安全check).<br>然后，DataBlock Index会被读取到内存中，这样，当检索某个key时，不需要扫描整个HFile，而只需从内存中找到key所在的block，<br>通过一次磁盘io将整个block读取到内存中，再找到需要的key。DataBlock Index采用LRU机制淘汰。</p>
</li>
</ul>
<p>HFile的Data Block，Meta Block通常采用压缩方式存储，压缩之后可以大大减少网络IO和磁盘IO，随之而来的开销当然是需要花费cpu进行压缩和解压缩。<br>目前HFile的压缩支持两种方式：Gzip，Lzo。</p>
<ol>
<li>HLog(WAL log)</li>
</ol>
<p>WAL 意为Write ahead log(<a href="http://en.wikipedia.org/wiki/Write-ahead_logging)。" target="_blank" rel="external">http://en.wikipedia.org/wiki/Write-ahead_logging)。</a><br>类似mysql中的binlog,用来做灾难恢复之用，Hlog记录数据的所有变更,一旦数据修改，就可以从log中进行恢复。</p>
<p>每个Region Server维护一个Hlog,而不是每个Region一个。<br>这样不同region(来自不同table)的日志会混在一起，这样做的目的是不断追加单个文件相对于同时写多个文件而言，可以减少磁盘寻址次数.<br>因此可以提高对table的写性能。带来的麻烦是，如果一台region server下线，为了恢复其上的region，<br>需要将region server上的log进行拆分，然后分发到其它region server上进行恢复。</p>
<p>HLog文件就是一个普通的Hadoop Sequence File，Sequence File 的Key是HLogKey对象，HLogKey中记录了写入数据的归属信息。<br>除了table和region名字外，同时还包括 sequence number和timestamp，timestamp是”写入时间”，<br>sequence number的起始值为0，或者是最近一次存入文件系统中sequence number。<br>HLog Sequece File的Value是HBase的KeyValue对象，即对应HFile中的KeyValue，可参见上文描述。</p>
<h1 id="四、系统架构"><a href="#四、系统架构" class="headerlink" title="四、系统架构"></a>四、系统架构</h1><ul>
<li>Client</li>
</ul>
<p>包含访问Hbase的接口，client维护着一些cache来加快对Hbase的访问，比如Region的位置信息。</p>
<ul>
<li><p>Zookeeper</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1． 保证任何时候，集群中只有一个master</div><div class="line">2． 存贮所有Region的寻址入口。</div><div class="line">3． 实时监控Region Server的状态，将Region server的上线和下线信息实时通知给Master</div><div class="line">4. 存储Hbase的schema,包括有哪些table，每个table有哪些column family</div></pre></td></tr></table></figure>
</li>
<li><p>Master</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1. 为Region server分配region</div><div class="line">2. 负责region server的负载均衡</div><div class="line">3. 发现失效的region server并重新分配其上的region上的垃圾文件回收</div><div class="line">4. 处理schema更新请求</div></pre></td></tr></table></figure>
</li>
<li><p>Region Server</p>
</li>
</ul>
<p>维护Master分配给它的region，处理对这些region的IO请求</p>
<p>负责切分在运行过程中变得过大的region</p>
<p>可以看到，client访问Hbase上数据的过程并不需要master参与（寻址访问zookeeper和region server，数据读写访问region server），<br>master仅仅维护者table和region的元数据信息，负载很低。</p>
<h1 id="五、关键算法-流程"><a href="#五、关键算法-流程" class="headerlink" title="五、关键算法/流程"></a>五、关键算法/流程</h1><h3 id="1-region定位"><a href="#1-region定位" class="headerlink" title="1. region定位"></a>1. region定位</h3><p>系统如何找到某个row key (或者某个 row key range)所在的region</p>
<p>bigtable 使用三层类似B+树的结构来保存region位置。</p>
<p>第一层：保存zookeeper里面的文件，它持有root region的位置。</p>
<p>第二层：root region是.META.表的第一个region其中保存了.META.z表其它region的位置。<br>通过root region，我们就可以访问.META.表的数据。</p>
<p>第三层：.META.，它是一个特殊的表，保存了hbase中所有数据表的region 位置信息。</p>
<p>说明：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1. root region永远不会被split，保证了最需要三次跳转，就能定位到任意region 。</div><div class="line"></div><div class="line">2. META.表每行保存一个region的位置信息，row key 采用表名+表的最后一样编码而成。</div><div class="line"></div><div class="line">3. 为了加快访问，.META.表的全部region都保存在内存中。</div><div class="line"></div><div class="line">假设，.META.表的一行在内存中大约占用1KB。并且每个region限制为128MB。</div><div class="line"></div><div class="line">那么上面的三层结构可以保存的region数目为：(128MB/1KB) * (128MB/1KB) = = 2(34)个region</div><div class="line"></div><div class="line">4. client会将查询过的位置信息保存缓存起来，缓存不会主动失效，</div><div class="line">因此如果client上的缓存全部失效，则需要进行6次网络来回，才能定位到正确的region(其中三次用来发现缓存失效，另外三次用来获取位置信息)。</div></pre></td></tr></table></figure></p>
<h3 id="2-读过程"><a href="#2-读过程" class="headerlink" title="2. 读过程"></a>2. 读过程</h3><p>上文提到，HBase使用MemStore和StoreFile存储对表的更新。</p>
<p>数据在更新时首先写入Log(WAL log)和内存(MemStore)中，MemStore中的数据是排序的，<br>当MemStore累计到一定阈值时，就会创建一个新的MemStore，并且将老的MemStore添加到flush队列，由单独的线程flush到磁盘上，成为一个StoreFile。<br>与此同时，系统会在zookeeper中记录一个redo point，表示这个时刻之前的变更已经持久化了(minor compact)。<br>当系统出现意外时，可能导致内存(MemStore)中的数据丢失，此时使用Log(WAL log)来恢复checkpoint之后的数据。</p>
<p>前面提到过StoreFile是只读的，一旦创建后就不可以再修改。因此Hbase的更新其实是不断追加的操作。<br>当一个Store中的StoreFile达到一定的阈值后，就会进行一次合并(major compact),将对同一个key的修改合并到一起，形成一个大的StoreFile，<br>当StoreFile的大小达到一定阈值后，又会对StoreFile进行split，等分为两个StoreFile。</p>
<p>由于对表的更新是不断追加的，处理读请求时，需要访问Store中全部的StoreFile和MemStore，将他们的按照row key进行合并，<br>由于StoreFile和MemStore都是经过排序的，并且StoreFile带有内存中索引，合并的过程还是比较快。</p>
<h3 id="3-写过程"><a href="#3-写过程" class="headerlink" title="3.写过程"></a>3.写过程</h3><ol>
<li><p>client向region server提交写请求</p>
</li>
<li><p>region server找到目标region</p>
</li>
<li><p>region检查数据是否与schema一致</p>
</li>
<li><p>如果客户端没有指定版本，则获取当前系统时间作为数据版本</p>
</li>
<li><p>将更新写入WAL log</p>
</li>
<li><p>将更新写入Memstore</p>
</li>
<li><p>判断Memstore的是否需要flush为Store文件。</p>
</li>
</ol>
<h3 id="4-region分配"><a href="#4-region分配" class="headerlink" title="4.region分配"></a>4.region分配</h3><p>任何时刻，一个region只能分配给一个region server。<br>master记录了当前有哪些可用的region server。以及当前哪些region分配给了哪些region server，哪些region还没有分配。<br>当存在未分配的region，并且有一个region server上有可用空间时，master就给这个region server发送一个装载请求，把region分配给这个region server。<br>region server得到请求后，就开始对此region提供服务。</p>
<h3 id="5-region-server上线"><a href="#5-region-server上线" class="headerlink" title="5. region server上线"></a>5. region server上线</h3><p>master使用zookeeper来跟踪region server状态。<br>当某个region server启动时，会首先在zookeeper上的server目录下建立代表自己的文件，并获得该文件的独占锁。<br>由于master订阅了server目录上的变更消息，当server目录下的文件出现新增或删除操作时，master可以得到来自zookeeper的实时通知。<br>因此一旦region server上线，master能马上得到消息。</p>
<h3 id="6-region-server下线"><a href="#6-region-server下线" class="headerlink" title="6.region server下线"></a>6.region server下线</h3><p>当region server下线时，它和zookeeper的会话断开，zookeeper而自动释放代表这台server的文件上的独占锁。<br>而master不断轮询server目录下文件的锁状态。如果master发现某个region server丢失了它自己的独占锁，<br>(或者master连续几次和region server通信都无法成功),master就是尝试去获取代表这个region server的读写锁，<br>一旦获取成功，就可以确定：</p>
<ol>
<li><p>region server和zookeeper之间的网络断开了。</p>
</li>
<li><p>region server挂了。</p>
</li>
</ol>
<p>上面的其中一种情况发生了，无论哪种情况，region server都无法继续为它的region提供服务了，<br>此时master会删除server目录下代表这台region server的文件，并将这台region server的region分配给其它还活着的同志。</p>
<p>如果网络短暂出现问题导致region server丢失了它的锁，那么region server重新连接到zookeeper之后，<br>只要代表它的文件还在，它就会不断尝试获取这个文件上的锁，一旦获取到了，就可以继续提供服务。</p>
<h3 id="7-master上线"><a href="#7-master上线" class="headerlink" title="7.master上线"></a>7.master上线</h3><p>master启动进行以下步骤:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1. 从zookeeper上获取唯一一个代码master的锁，用来阻止其它master成为master。</div><div class="line">2. 扫描zookeeper上的server目录，获得当前可用的region server列表。</div><div class="line">3. 和2中的每个region server通信，获得当前已分配的region和region server的对应关系。</div><div class="line">4. 扫描.META.region的集合，计算得到当前还未分配的region，将他们放入待分配region列表。</div></pre></td></tr></table></figure></p>
<h3 id="8-master下线"><a href="#8-master下线" class="headerlink" title="8. master下线"></a>8. master下线</h3><p>由于master只维护表和region的元数据，而不参与表数据IO的过程，<br>master下线仅导致所有元数据的修改被冻结(无法创建删除表，无法修改表的schema，无法进行region的负载均衡，<br>无法处理region上下线，无法进行region的合并，唯一例外的是region的split可以正常进行，因为只有region server参与)，表的数据读写还可以正常进行。<br>因此master下线短时间内对整个HBase集群没有影响。从上线过程可以看到，master保存的信息全是可以冗余信息（都可以从系统其它地方收集到或者计算出来），<br>因此，一般HBase集群中总是有一个master在提供服务，还有一个以上的’master’在等待时机抢占它的位置。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>怎么成为Apache Spark贡献者</title>
    <url>/2017/tobe_spark_contributor.html</url>
    <content><![CDATA[<p>You should start with:</p>
<p>Read API Docs: Spark 2.4.3 Scala Doc<br>Join the mailing list: Community | Apache Spark<br>Now you should take a look at Jira (list of known bugs / issues): - ASF JIRA</p>
<h2 id="Jira"><a href="#Jira" class="headerlink" title="Jira"></a>Jira</h2><p>首先要到Spark 的 Jira里找找有没有什么自己可以解决的Issue。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>从网上找到自己的答案</title>
    <url>/2017/find_your_answer.html</url>
    <content><![CDATA[<h1 id="Use-the-Right-Search-Engine"><a href="#Use-the-Right-Search-Engine" class="headerlink" title="Use the Right Search Engine"></a>Use the Right Search Engine</h1><p>成为一个程序员也有一段时间了。回过头来想想看以前做的事情，只想说：非常low！</p>
<p>我记得东哥那时候推荐我用bing或者翻墙上谷歌。我当时可是一脸懵逼，感觉用习惯了，也觉得没什么不好的地方。</p>
<p>不说其他的，就说程序这块吧。当你运行代码碰到一个报错信息时候，直接用google搜的话很快就能找到解决方案。比如Python的话google返回的很有可能是stackoverflow这些链接。linux的是askubuntu的这些链接。</p>
<p>而百度返回的很有可能是Oschina或者Csdn等等。然而很多好的解答都是在stackoverflow那些网站。</p>
<h1 id="English-is-Important"><a href="#English-is-Important" class="headerlink" title="English is Important"></a>English is Important</h1><p>有些比较少罕见的问题，国内搜索引擎压根就找不到，国内压根就没有这方面答案。你必须从国外的网站获取解答。比如 Github issue 、Quora或者官方文档。</p>
<a id="more"></a>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>多用Google搜索。多到英文环境下找想要的答案。</p>
]]></content>
      <categories>
        <category>daily</category>
      </categories>
      <tags>
        <tag>daily</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark repartition和coalesce的区别</title>
    <url>/2017/spark_repartition_coalesce.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>有些时候，在很多partition的时候，我们想减少点partition的数量，不然写到HDFS上的文件数量也会很多很多。<br>我们使用reparation呢，还是coalesce。所以我们得了解这两个算子的内在区别。</p>
<h2 id="正题"><a href="#正题" class="headerlink" title="正题"></a>正题</h2><p>要知道，repartition是一个消耗比较昂贵的操作算子，Spark出了一个优化版的repartition叫做coalesce，它可以尽量避免数据迁移，<br>但是你只能减少RDD的partition.</p>
<p>举个例子，有如下数据节点分布：</p>
<p>Node 1 = 1,2,3<br>Node 2 = 4,5,6<br>Node 3 = 7,8,9<br>Node 4 = 10,11,12</p>
<p>用coalesce，将partition减少到2个：</p>
<p>Node 1 = 1,2,3 + (10,11,12)<br>Node 3 = 7,8,9 + (4,5,6)</p>
<p>注意，Node1 和 Node3 不需要移动原始的数据</p>
<p>The repartition algorithm does a full shuffle and creates new partitions with data that’s distributed evenly.<br>Let’s create a DataFrame with the numbers from 1 to 12.</p>
<p>repartition 算法会做一个full shuffle然后均匀分布地创建新的partition。我们创建一个1-12数字的DataFrame测试一下。<br>刚开始数据是这样分布的：</p>
<p>Partition 00000: 1, 2, 3<br>Partition 00001: 4, 5, 6<br>Partition 00002: 7, 8, 9<br>Partition 00003: 10, 11, 12</p>
<p>我们做一个full shuffle，将其repartition为2个。</p>
<p>这是在我机器上数据分布的情况：<br>Partition A: 1, 3, 4, 6, 7, 9, 10, 12<br>Partition B: 2, 5, 8, 11</p>
<p>The repartition method makes new partitions and evenly distributes the data in the new partitions (the data distribution is more even for larger data sets).<br>repartition方法让新的partition均匀地分布了数据（数据量大的情况下其实会更均匀）</p>
<h3 id="coalesce-和-repartition-的区别"><a href="#coalesce-和-repartition-的区别" class="headerlink" title="coalesce 和 repartition 的区别"></a>coalesce 和 repartition 的区别</h3><p>coalesce用已有的partition去尽量减少数据shuffle。<br>repartition创建新的partition并且使用 full shuffle。<br>coalesce会使得每个partition不同数量的数据分布（有些时候各个partition会有不同的size）<br>然而，repartition使得每个partition的数据大小都粗略地相等。</p>
<h3 id="coalesce-会比-repartition-快速吗？"><a href="#coalesce-会比-repartition-快速吗？" class="headerlink" title="coalesce 会比 repartition 快速吗？"></a>coalesce 会比 repartition 快速吗？</h3><p>coalesce可能会比repartition更快，但是，在partition大小不相等的时候， 总体上会比repartition慢一些。<br>通常，在过滤掉大数据集后，你需要用repartition一下。<br>我发现repartition总体上会快一些，因为Spark一般都是用相同大小的partition。</p>
<h2 id="Referrence"><a href="#Referrence" class="headerlink" title="Referrence"></a>Referrence</h2><p><a href="https://stackoverflow.com/questions/31610971/spark-repartition-vs-coalesce" target="_blank" rel="external">https://stackoverflow.com/questions/31610971/spark-repartition-vs-coalesce</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 的宽依赖和窄依赖</title>
    <url>/2016/spark_dependency.html</url>
    <content><![CDATA[<h1 id="说说Spark-的宽依赖和窄依赖"><a href="#说说Spark-的宽依赖和窄依赖" class="headerlink" title="说说Spark 的宽依赖和窄依赖"></a>说说Spark 的宽依赖和窄依赖</h1><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>　　众所周知，在Spark中可以对RDD进行多种转换，父RDD转换之后会得到子RDD，于是便可以说子RDD的诞生需要依赖父RDD。在Spark中一共有两种依赖方式，分别是宽依赖和窄依赖。</p>
<p>rdd 的 toDebugString 可以查看RDD的谱系</p>
<a id="more"></a>
<h3 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a>窄依赖</h3><p>如果父RDD的每个分区至多只对应一个子RDD的分区，则这种依赖关系称为窄依赖。</p>
<p>常见的窄依赖算子有：map、filter、union。</p>
<p>对窄依赖RDD进行Lineage恢复时，如果子RDD某个分区坏了，通过父RDD指定分区重算时，将不会重算其他正常的分区，避免了冗余计算，提高了性能。</p>
<h3 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a>宽依赖</h3><p>如果父RDD中至少有一个分区对应子RDD的多个分区(至少两个分区)，则这种依赖关系称为宽依赖。</p>
<p>常见的宽依赖算子有：groupByKey、sortByKey。</p>
<p>对宽依赖RDD进行Lineage恢复时，如果子RDD某个分区坏了，通过父RDD指定分区重算时，有可能会重算一些其他正常的分区，会有冗余计算，性能开销也会比窄依赖大很多。</p>
<p>宽依赖通常是Spark拆分Stage的边界，在同一个Stage内均为窄依赖。</p>
<p><img src="/images/2016/spark_dependency/spark-dependency.jpg" alt="Sample Image Added via Markdown"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>我们可以不用去背宽窄依赖的概念和对应关系之类的。我们应该知道为什么要区分宽窄依赖，它的目的是什么。其实目的就是提醒我们尽量地使用窄依赖，因为这样会减少风险，<br>当子RDD的某个partition损坏的时候是否要重新计算很多父RDD的多个partition呢。这个才是宽窄依赖的重点。<br>另一个划分看宽窄依赖的目的是：划分stage。至于划分stage的目的，就是要等待其它节点的所有partition全部计算完成才能作下一个阶段的计算。</p>
<p>在网上查了很多资料貌似有很多不同的定义。在看了很多解释后了解到，依赖的问题在论文里和在Spark的实现中的定义是不一样的。</p>
<ul>
<li><p>在论文中，是叫narrow dependency和wide dependency。<br>如果父RDD的每个partition只被子RDD的一个partition所依赖，就叫Narrow dependency，否则叫Wide dependency。</p>
</li>
<li><p>在Spark中，是叫narrow dependency（也叫完全依赖）和shuffle dependency（也叫部分依赖）。<br>如果父RDD的每个partition（并且是partition里的部分数据）都被子RDD的每个partition所依赖才叫ShuffleDependency，其余情况都是NarrowDependency。</p>
</li>
</ul>
<h3 id="SparkInternals-dependency图解"><a href="#SparkInternals-dependency图解" class="headerlink" title="SparkInternals dependency图解"></a>SparkInternals dependency图解</h3><p><img src="/images/2016/spark_dependency/spark_internals_dependency.png" alt="Sample Image Added via Markdown"></p>
<p>前三个是完全依赖，RDD x 中的 partition 与 parent RDD 中的 partition/partitions 完全相关。最后一个是部分依赖，RDD x 中的 partition 只与 parent RDD 中的 partition 一部分数据相关，另一部分数据与 RDD x 中的其他 partition 相关。</p>
<p>在 Spark 中，完全依赖被称为 NarrowDependency，部分依赖被称为 ShuffleDependency。其实 ShuffleDependency 跟 MapReduce 中 shuffle 的数据依赖相同（mapper 将其 output 进行 partition，然后每个 reducer 会将所有 mapper 输出中属于自己的 partition 通过 HTTP fetch 得到）。</p>
<ul>
<li>第一种 1:1 的情况被称为 OneToOneDependency。</li>
<li>第二种 N:1 的情况被称为 N:1 NarrowDependency。</li>
<li>第三种 N:N 的情况被称为 N:N NarrowDependency。不属于前两种情况的完全依赖都属于这个类别。</li>
<li>第四种被称为 ShuffleDependency。</li>
</ul>
<p>N:N NarrowDependency 的几个很经典的情况是：coalesce 和 cartesian<br><img src="/images/2016/spark_dependency/coalesce.png" alt="Sample Image Added via Markdown"></p>
<p><img src="/images/2016/spark_dependency/cartesian.png" alt="Sample Image Added via Markdown"></p>
<p>以上两种情况它们仍然可以在一个stage里的pipeline以一个task计算。不用等所有的task计算完再计算。因为它不是shuffle操作。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://github.com/JerryLead/SparkInternals/blob/master/markdown/2-JobLogicalPlan.md" target="_blank" rel="external">https://github.com/JerryLead/SparkInternals/blob/master/markdown/2-JobLogicalPlan.md</a><br><a href="https://www.zhihu.com/question/37137360/answer/715150822" target="_blank" rel="external">https://www.zhihu.com/question/37137360/answer/715150822</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 数据倾斜调优</title>
    <url>/2016/spark_bias.html</url>
    <content><![CDATA[<h1 id="关于Spark-的数据倾斜的问题"><a href="#关于Spark-的数据倾斜的问题" class="headerlink" title="关于Spark 的数据倾斜的问题"></a>关于Spark 的数据倾斜的问题</h1><h3 id="数据倾斜的现象"><a href="#数据倾斜的现象" class="headerlink" title="数据倾斜的现象"></a>数据倾斜的现象</h3><p>　　1、绝大多数task执行得都非常快，但个别task执行极慢。比如，总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时。这种情况很常见。</p>
<p>　　2、原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。</p>
<h3 id="数据倾斜的原理"><a href="#数据倾斜的原理" class="headerlink" title="数据倾斜的原理"></a>数据倾斜的原理</h3><p>　　数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。</p>
<p>　　因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。</p>
<a id="more"></a>
<p>　　下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而整个stage的运行速度也由运行最慢的那个task所决定。</p>
<p><img src="/images/2016/spark_bias/spark-bias1.jpg" alt="Sample Image Added via Markdown"></p>
<h3 id="如何定位导致数据倾斜的代码"><a href="#如何定位导致数据倾斜的代码" class="headerlink" title="如何定位导致数据倾斜的代码"></a>如何定位导致数据倾斜的代码</h3><p>　　数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。</p>
<ul>
<li>某个task执行特别慢的情况</li>
</ul>
<p>　　首先要看的，就是数据倾斜发生在第几个stage中。</p>
<p>　　如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以在Spark Web UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。</p>
<p>　　比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。</p>
<p><img src="/images/2016/spark_bias/spark-bias2.jpg" alt="Sample Image Added via Markdown"></p>
<p>　　知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。</p>
<p>　　这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。</p>
<p>　　1、stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。</p>
<p>　　2、stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。</p>
<!-- scala -->
<pre><code>val conf = new SparkConf()
val sc = new SparkContext(conf)

val lines = sc.textFile(&quot;hdfs://...&quot;)
val words = lines.flatMap(_.split(&quot; &quot;))
val pairs = words.map((_, 1))
val wordCounts = pairs.reduceByKey(_ + _)

wordCounts.collect().foreach(println(_))
</code></pre><p>　　通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由reduceByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。</p>
<ul>
<li>某个task莫名其妙内存溢出的情况</li>
</ul>
<p>　　这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。</p>
<p>　　但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。</p>
<h3 id="查看导致数据倾斜的key的数据分布情况"><a href="#查看导致数据倾斜的key的数据分布情况" class="headerlink" title="查看导致数据倾斜的key的数据分布情况"></a>查看导致数据倾斜的key的数据分布情况</h3><p>　　知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。</p>
<p>　　此时根据你执行操作的情况不同，可以有很多种查看key分布的方式：</p>
<p>　　1、如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。</p>
<p>　　2、如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。</p>
<p>　　举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。</p>
<!-- scala -->
<pre><code>val sampledPairs = pairs.sample(false, 0.1)
val sampledWordCounts = sampledPairs.countByKey()
sampledWordCounts.foreach(println(_))
</code></pre><h3 id="数据倾斜的解决方案"><a href="#数据倾斜的解决方案" class="headerlink" title="数据倾斜的解决方案"></a>数据倾斜的解决方案</h3><ul>
<li>解决方案一：使用Hive ETL预处理数据</li>
</ul>
<p>方案适用场景：<br>导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。</p>
<p>方案实现思路：<br>此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。</p>
<p>方案实现原理：<br>这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。</p>
<p>方案优点：<br>实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p>
<p>方案缺点：<br>治标不治本，Hive ETL中还是会发生数据倾斜。</p>
<p>方案实践经验：<br>在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p>
<p>项目实践经验：<br>在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。</p>
<ul>
<li>解决方案二：过滤少数导致倾斜的key</li>
</ul>
<p>方案适用场景：<br>如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p>
<p>方案实现思路：<br>如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。</p>
<p>方案实现原理：<br>将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。</p>
<p>方案优点：<br>实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p>
<p>方案缺点：<br>适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p>
<p>方案实践经验：<br>在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p>
<ul>
<li>解决方案三：提高shuffle操作的并行度</li>
</ul>
<p>方案适用场景：<br>如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。</p>
<p>方案实现思路：<br>在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p>
<p>方案实现原理：<br>增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。</p>
<p>方案优点：<br>实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p>
<p>方案缺点：<br>只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p>
<p>方案实践经验：<br>该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p>
<p><img src="/images/2016/spark_bias/spark-bias3.jpg" alt="Sample Image Added via Markdown"></p>
<ul>
<li>解决方案四：两阶段聚合（局部聚合+全局聚合）</li>
</ul>
<p>方案适用场景：<br>对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。</p>
<p>方案实现思路：<br>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</p>
<p>方案实现原理：<br>将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p>
<p>方案优点：<br>对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p>
<p>方案缺点：<br>仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</p>
<p><img src="/images/2016/spark_bias/spark-bias4.jpg" alt="Sample Image Added via Markdown"></p>
<!-- scala -->
<pre><code>// 第一步，给RDD中的每个key都打上一个随机前缀。
JavaPairRDD&lt;String, Long&gt; randomPrefixRdd = rdd.mapToPair(
        new PairFunction&lt;Tuple2&lt;Long,Long&gt;, String, Long&gt;() {
            private static final long serialVersionUID = 1L;
            @Override
            public Tuple2&lt;String, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple)
                    throws Exception {
                Random random = new Random();
                int prefix = random.nextInt(10);
                return new Tuple2&lt;String, Long&gt;(prefix + &quot;_&quot; + tuple._1, tuple._2);
            }
        });

// 第二步，对打上随机前缀的key进行局部聚合。
JavaPairRDD&lt;String, Long&gt; localAggrRdd = randomPrefixRdd.reduceByKey(
        new Function2&lt;Long, Long, Long&gt;() {
            private static final long serialVersionUID = 1L;
            @Override
            public Long call(Long v1, Long v2) throws Exception {
                return v1 + v2;
            }
        });

    // 第三步，去除RDD中每个key的随机前缀。
    JavaPairRDD&lt;Long, Long&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair(
            new PairFunction&lt;Tuple2&lt;String,Long&gt;, Long, Long&gt;() {
                private static final long serialVersionUID = 1L;
                @Override
                public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;String, Long&gt; tuple)
                        throws Exception {
                    long originalKey = Long.valueOf(tuple._1.split(&quot;_&quot;)[1]);
                    return new Tuple2&lt;Long, Long&gt;(originalKey, tuple._2);
                }
            });

    // 第四步，对去除了随机前缀的RDD进行全局聚合。
    JavaPairRDD&lt;Long, Long&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey(
            new Function2&lt;Long, Long, Long&gt;() {
                private static final long serialVersionUID = 1L;
                @Override
                public Long call(Long v1, Long v2) throws Exception {
                    return v1 + v2;
                }
            });
</code></pre><ul>
<li>解决方案五：将reduce join转为map join</li>
</ul>
<p>方案适用场景：<br>在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p>
<p>方案实现思路：<br>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p>
<p>方案实现原理：<br>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。</p>
<p>方案优点：<br>对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p>
<p>方案缺点：<br>适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p>
<p><img src="/images/2016/spark_bias/spark-bias5.jpg" alt="Sample Image Added via Markdown"></p>
<!-- scala -->
<pre><code>// 首先将数据量比较小的RDD的数据，collect到Driver中来。
List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect()
// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。
// 可以尽可能节省内存空间，并且减少网络传输性能开销。
final Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data);

// 对另外一个RDD执行map类操作，而不再是join类操作。
JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd = rdd2.mapToPair(
        new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() {
            private static final long serialVersionUID = 1L;
            @Override
            public Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, String&gt; tuple)
                    throws Exception {
                // 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。
                List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value();
                // 可以将rdd1的数据转换为一个Map，便于后面进行join操作。
                Map&lt;Long, Row&gt; rdd1DataMap = new HashMap&lt;Long, Row&gt;();
                for(Tuple2&lt;Long, Row&gt; data : rdd1Data) {
                    rdd1DataMap.put(data._1, data._2);
                }
                // 获取当前RDD数据的key以及value。
                String key = tuple._1;
                String value = tuple._2;
                // 从rdd1数据Map中，根据key获取到可以join到的数据。
                Row rdd1Value = rdd1DataMap.get(key);
                return new Tuple2&lt;String, String&gt;(key, new Tuple2&lt;String, Row&gt;(value, rdd1Value));
            }
        });

// 这里得提示一下。
// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。
// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。
// rdd2中每条数据都可能会返回多条join后的数据。
</code></pre><ul>
<li>解决方案六：采样倾斜key并分拆join操作</li>
</ul>
<p>方案适用场景：<br>两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。</p>
<p>方案实现思路：<br>　　1、对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。</p>
<p>　　2、然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。</p>
<p>　　3、接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。</p>
<p>　　4、再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。　　</p>
<p>　　5、而另外两个普通的RDD就照常join即可。</p>
<p>　　6、最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。</p>
<p>方案实现原理：<br>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。</p>
<p>方案优点：<br>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。</p>
<p>方案缺点：<br>如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。</p>
<p><img src="/images/2016/spark_bias/spark-bias6.jpg" alt="Sample Image Added via Markdown"></p>
<!-- scala -->
<pre><code>// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。
JavaPairRDD&lt;String, Row&gt; expandedRDD = rdd1.flatMapToPair(
        new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() {
            private static final long serialVersionUID = 1L;
            @Override
            public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, Row&gt; tuple)
                    throws Exception {
                List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;();
                for(int i = 0; i &lt; 100; i++) {
                    list.add(new Tuple2&lt;String, Row&gt;(0 + &quot;_&quot; + tuple._1, tuple._2));
                }
                return list;
            }
        });

// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。
JavaPairRDD&lt;String, String&gt; mappedRDD = rdd2.mapToPair(
        new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() {
            private static final long serialVersionUID = 1L;
            @Override
            public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple)
                    throws Exception {
                Random random = new Random();
                int prefix = random.nextInt(100);
                return new Tuple2&lt;String, String&gt;(prefix + &quot;_&quot; + tuple._1, tuple._2);
            }
        });

// 将两个处理后的RDD进行join即可。
JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD);
</code></pre>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>浅谈SPARK 的 广播变量</title>
    <url>/2016/spark_broadcast.html</url>
    <content><![CDATA[<h1 id="简单谈谈SPARK-广播变量的用法"><a href="#简单谈谈SPARK-广播变量的用法" class="headerlink" title="简单谈谈SPARK 广播变量的用法"></a>简单谈谈SPARK 广播变量的用法</h1><h3 id="首先我们看看官网的文档对Broadcast的描述："><a href="#首先我们看看官网的文档对Broadcast的描述：" class="headerlink" title="首先我们看看官网的文档对Broadcast的描述："></a>首先我们看看官网的文档对Broadcast的描述：</h3><ul>
<li><p>Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks.</p>
</li>
<li><p>意思就是说广播变量让你可以在每个节点机器上缓存一个只读的变量，而减少了在每个任务中复制一份的繁琐。</p>
</li>
<li>Spark还尝试使用高效地广播算法来分发变量，进而减少通信的开销。</li>
<li>Spark的动作通过一系列的步骤执行，这些步骤由分布式的洗牌操作分开。Spark自动地广播每个步骤每个任务需要的通用数据。这些广播数据被序列化地缓存，在运行任务之前被反序列化出来。这意味着当我们需要在多个阶段的任务之间使用相同的数据，或者以反序列化形式缓存数据是十分重要的时候，显式地创建广播变量才有用。</li>
</ul>
<p>创建的方法很简单<br><a id="more"></a></p>
<!-- python -->
<pre><code>import pandas as pd
table=pd.read_sql(&quot;select goods_id,goods_model,brand_id from goods&quot;,engine)
#比如我们要先读取一个比较大的mysql表以供map、filter这些变型中查询数据。我们可以用广播变量将这个公共的变量在每个节点中都保留一份

table_broadcast=sc.broadcast(table)

#调用的时候很简单，只需要在对象.value就可以

def parse(tup):
    result=table_broadcast.value[table_broadcast.value[&apos;goods_id]==100][&apos;brand_id&apos;]
    return result

rdd.map(parse)
</code></pre><p>在创建了广播变量之后，在集群上的所有函数中应该使用它来替代使用v.这样v就不会不止一次地在节点之间传输了。另外，为了确保所有的节点获得相同的变量，对象v在被广播之后就不应该再修改。</p>
<p>如果不用broadcast的话，table会从主节点为每个任务发送一个这样的数据，就会代价很大，而且再调用table的时候，还需要向每个节点再发送一遍。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>浅谈SPARK 的 combineByKey 算子</title>
    <url>/2016/spark_combinebykey.html</url>
    <content><![CDATA[<h1 id="理解comebineByKey的原理"><a href="#理解comebineByKey的原理" class="headerlink" title="理解comebineByKey的原理"></a>理解comebineByKey的原理</h1><h3 id="SPARK的combineByKey算子和aggregate类似。首先我们看看官网的文档："><a href="#SPARK的combineByKey算子和aggregate类似。首先我们看看官网的文档：" class="headerlink" title="SPARK的combineByKey算子和aggregate类似。首先我们看看官网的文档："></a>SPARK的combineByKey算子和aggregate类似。首先我们看看官网的文档：</h3><ul>
<li><p>combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<function portable_hash="" at="" 0x7ff5681b9d70="">)</function></p>
</li>
<li><p>Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a “combined type” C. Note that V and C can be different – for example, one might group an RDD of type (Int, Int) into an RDD of type (Int, List[Int]).</p>
</li>
<li><p>Users provide three functions:<br>createCombiner, which turns a V into a C (e.g., creates a one-element list)<br>mergeValue, to merge a V into a C (e.g., adds it to the end of a list)<br>mergeCombiners, to combine two C’s into a single one.</p>
</li>
</ul>
<p>都是将类型为RDD[(K,V)]的数据处理为RDD[(K,C)]。这里的V和C可以是相同类型，也可以是不同类型。</p>
<p>combineByKey函数主要接受了三个函数作为参数，分别为createCombiner、mergeValue、mergeCombiners。</p>
<p>这三个函数足以说明它究竟做了什么。理解了这三个函数，就可以很好地理解combineByKey。</p>
<a id="more"></a>
<p>1.createCombiner ,让V变为C<br>V –&gt; C<br>2.mergeValue，将V合并为C<br>C, V –&gt; C<br>3.mergeCombiners，合并所有C为一个<br>C, C –&gt; C</p>
<p>我们在命令行里输入IPYTHON=1 pyspark --master local[<em>]启动Spark 的Python shell。这里加了–master local[</em>] 是为了可以以local 模式运行，这样可以便于print 打印调试。</p>
<p>下面是一个求平均值的例子<br><!-- python --></p>
<pre><code>data = [
        (&apos;A&apos;, 2.), (&apos;A&apos;, 4.), (&apos;A&apos;, 9.), 
        (&apos;B&apos;, 10.), (&apos;B&apos;, 20.), 
        (&apos;Z&apos;, 3.), (&apos;Z&apos;, 5.), (&apos;Z&apos;, 8.), (&apos;Z&apos;, 12.) 
       ]
rdd = sc.parallelize( data )

def mergeValue(x,value):
    print &apos;what is the x&apos;,x
    return x[0]+value,x[1]+1

sumCount = rdd.combineByKey(lambda value: (value, 1),
                            mergeValue,
                            lambda x, y: (x[0] + y[0], x[1] + y[1])
    )

sumCount.collect()
#[(&apos;A&apos;, (15.0, 3)), (&apos;B&apos;, (30.0, 2)), (&apos;Z&apos;, (28.0, 4))]

averageByKey = sumCount.map(lambda (key, (totalSum, count)): (key, totalSum / count))

averageByKey.collectAsMap()
#{A: 5.0, B: 15.0,Z: 7.0}
</code></pre><p>我们按步骤解读一下：<br>1.Create a Combiner<br>lambda value:(value,1)<br>这个步骤定义了C的数据结构，也就是(sum,count)。</p>
<p>如果是一个新的元素，此时使用createCombiner()来创建那个键对应的累加器的初始值。（注意：这个过程会在每个分区第一次出现各个键时发生，而不是在整个RDD中第一次出现一个键时发生。）</p>
<p>2.Merge a Value<br>lambda x, value: (x[0] + value, x[1] + 1)<br>这个方法告诉combineByKey当给到一个新value的时候要做什么。方法的参数是一个combiner和一个新的value。combiner的数据结构在上一个方法定义了(sum,count)。</p>
<p>3.Merge two Combiners<br>lambda x, y: (x[0] + y[0], x[1] + y[1])<br>这个方法告诉combineByKey怎么合并两个combiners</p>
<p>内部流程如下：</p>
<!-- python -->
<pre><code>data = [
        (&quot;A&quot;, 2.), (&quot;A&quot;, 4.), (&quot;A&quot;, 9.), 
        (&quot;B&quot;, 10.), (&quot;B&quot;, 20.), 
        (&quot;Z&quot;, 3.), (&quot;Z&quot;, 5.), (&quot;Z&quot;, 8.), (&quot;Z&quot;, 12.) 
       ]

Partition 1: (&quot;A&quot;, 2.), (&quot;A&quot;, 4.), (&quot;A&quot;, 9.), (&quot;B&quot;, 10.)
Partition 2: (&quot;B&quot;, 20.), (&quot;Z&quot;, 3.), (&quot;Z&quot;, 5.), (&quot;Z&quot;, 8.), (&quot;Z&quot;, 12.) 


Partition 1 
(&quot;A&quot;, 2.), (&quot;A&quot;, 4.), (&quot;A&quot;, 9.), (&quot;B&quot;, 10.)

A=2. --&gt; createCombiner(2.) ==&gt; accumulator[A] = (2., 1)
A=4. --&gt; mergeValue(accumulator[A], 4.) ==&gt; accumulator[A] = (2. + 4., 1 + 1) = (6., 2)
A=9. --&gt; mergeValue(accumulator[A], 9.) ==&gt; accumulator[A] = (6. + 9., 2 + 1) = (15., 3)
B=10. --&gt; createCombiner(10.) ==&gt; accumulator[B] = (10., 1)

Partition 2
(&quot;B&quot;, 20.), (&quot;Z&quot;, 3.), (&quot;Z&quot;, 5.), (&quot;Z&quot;, 8.), (&quot;Z&quot;, 12.) 

B=20. --&gt; createCombiner(20.) ==&gt; accumulator[B] = (20., 1)
Z=3. --&gt; createCombiner(3.) ==&gt; accumulator[Z] = (3., 1)
Z=5. --&gt; mergeValue(accumulator[Z], 5.) ==&gt; accumulator[Z] = (3. + 5., 1 + 1) = (8., 2)
Z=8. --&gt; mergeValue(accumulator[Z], 8.) ==&gt; accumulator[Z] = (8. + 8., 2 + 1) = (16., 3)
Z=12. --&gt; mergeValue(accumulator[Z], 12.) ==&gt; accumulator[Z] = (16. + 12., 3 + 1) = (28., 4)

Merge partitions together
A ==&gt; (15., 3)
B ==&gt; mergeCombiner((10., 1), (20., 1)) ==&gt; (10. + 20., 1 + 1) = (30., 2)
Z ==&gt; (28., 4)

最终结果为：
Array( [A, (15., 3)], [B, (30., 2)], [Z, (28., 4)])
</code></pre>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>浅谈SPARK 的 aggregate 算子</title>
    <url>/2016/spark_aggregate.html</url>
    <content><![CDATA[<h1 id="理解aggregate的原理"><a href="#理解aggregate的原理" class="headerlink" title="理解aggregate的原理"></a>理解aggregate的原理</h1><p>刚开始我觉得SPARK的aggregate算子比较难理解的。首先我们看看官网的example</p>
<!-- python -->
<pre><code>&gt;&gt;&gt; seqOp = (lambda x, y: (x[0] + y, x[1] + 1))
&gt;&gt;&gt; combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))
&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)
(10, 4)
&gt;&gt;&gt; sc.parallelize([]).aggregate((0, 0), seqOp, combOp)
(0, 0)    
</code></pre><p>看了后，我就一脸懵逼了，两个lambda函数中的x,y各自代表什么东西呢？</p>
<p>我们先看看官网的aggregate 用法说明：</p>
<p>Aggregate the elements of each partition, and then the results for all the partitions, using a given combine functions and a neutral “zero value.”</p>
<p>The functions op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object allocation; however, it should not modify t2.</p>
<p>The first function (seqOp) can return a different result type, U, than the type of this RDD. Thus, we need one operation for merging a T into an U and one operation for merging two U</p>
<a id="more"></a>
<p>zero value：初始值，就是你要结果的类型。上面例子的zero value是(0,0)</p>
<p>seqOp:对RDD里每个partition里要实施的操作。</p>
<p>combOp:对所有的partition汇总的操作</p>
<p>我们再进一步解读官网的那个例子，仔细说说那些x，y分别代表什么<br>这个例子的意图是算出一个列表里所有的元素的和，还有列表的长度。</p>
<p>在pyspark里，我们执行以下代码</p>
<!-- python -->
<pre><code>#我们创建一个有4个元素的list，并且分为2个partition
listRDD = sc.parallelize([1,2,3,4], 2)

#然后我们定义seqOp
seqOp = (lambda local_result, list_element: (local_result[0] + list_element, local_result[1] + 1) )
#上面你可以更直观的看到x，y分别代表什么了。local_result也就是初始值（0，0）；list_element就是每个元素1,2,3,4..

#然后是combOp：
combOp = (lambda some_local_result, another_local_result: (some_local_result[0] + another_local_result[0], some_local_result[1] + another_local_result[1]) )
#从变量的名字你应该可以直观的看出x，y分别代表什么了吧

#然后aggregate一下：
listRDD.aggregate( (0, 0), seqOp, combOp)
Out[8]: (10, 4)
</code></pre><p>第一个分区的子列表是[1,2]，我们实施seqOp的时候会对这个子列表产生一个本地（可以认为这是一个节点服务器上）的result，result也就是（sum,length)。第一个分区的local result 是(3,2)。</p>
<p>是这么算的：<br>(0+1,0+1)=&gt; (1,1)<br>(1+2,1+1)=&gt; (3,2)</p>
<p>第二个分区的子列表是[3,4]<br>同理也可以算出第二个分区的local result是(7,2)</p>
<p>然后是到combOp将两个本地结果汇总。计算过程是：<br>(3+7,2+2) =&gt;(10,4)</p>
<h3 id="这里有个特别注意的地方"><a href="#这里有个特别注意的地方" class="headerlink" title="这里有个特别注意的地方"></a>这里有个特别注意的地方</h3><p>如果你的zero value不是(0,0)，而是(1,0)结果会有点出乎意料，这个例子中，结果并不是(12,4)，而是(13,4)。如果你的partition为3的话结果是(14,4)。这应该是aggregate 会根据分区数在多运算几次</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>aggregate 可以用来先计算每个partition的本地结果，然后再汇总。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>pyspark</tag>
      </tags>
  </entry>
  <entry>
    <title>Python的yield 问题</title>
    <url>/2016/python_yield.html</url>
    <content><![CDATA[<h2 id="Python-yield示范"><a href="#Python-yield示范" class="headerlink" title="Python yield示范"></a>Python yield示范</h2><p>yield 关键词在python中不算常用。以前我只知道它的作用和return相似，就是返回一个值。并不知道它具体的用途和用法。</p>
<p>要理解yield，你必须知道generators（生成器）。而要知道生成器，需要了解iterables（可迭代对象）</p>
<p>生成器（Generators）就是迭代器的一种特殊形式，只不过它只能迭代一次。这是因为生成器不是将所有的数值存储在内存里，而是，立即生成数值。<br>和迭代器相似，我们可以通过使用next()来从generator中获取下一个值</p>
<p>顺便说说：集合数据类型，如list、tuple、dict、set、str等，这些可以用for循环的对象都是可迭代对象（Iterable）。<br>但list、dict、str虽然是Iterable，却不是迭代器（Iterator）。把list、dict、str等Iterable变成Iterator可以使用iter()函数。</p>
<p>迭代器对象可以被next()函数调用并不断返回下一个数据，直到没有数据时抛出StopIteration错误。</p>
<p>Python的for循环本质上就是通过不断调用next()函数实现的</p>
<p>我们看看下面例子：</p>
<a id="more"></a>
<p>测试环境是ubuntu;<br>Python版本为2.7<br><!-- python --></p>
<pre><code>def yield_test1():
    print &apos;one&apos;
    yield 1
    print &apos;two&apos;
    yield 2
    print &apos;three&apos;
    yield 3
    print &apos;is any thing?&apos;

&gt;y=yield_test()
&gt;y.next()   #第一次调用next 返回的是第一个yield，返回的是1
&gt;one
&gt;[out]1
&gt;y.next()   #第二次调用next 返回的是第二个yield，返回的是2
&gt;two
&gt;[out]2
&gt;y.next()   #第三次调用next 返回的是第三个yield，返回的是3
&gt;three
&gt;[out]3
&gt;y.next()   #第四次调用next，就不一样了，会报错StopIteration。因为已经没有东西可以yield了。但是注意，还是可以print &apos;is any thing&apos;的。
&gt;is any thing?
&gt;StopIteration                             Traceback (most recent call last)
</code></pre><p>我的理解：yield和return的却别在于，yield是有记忆的返回东西。因为它记忆了上一次yield的值。</p>
<p>当一个生成器函数调用yield，生成器函数的“状态”会被冻结，所有的变量的值会被保留下来，下一行要执行的代码的位置也会被记录，直到再次调用next()。一旦next()再次被调用，生成器函数会从它上次离开的地方开始。如果永远不调用next()，yield保存的状态就被无视了。</p>
<p>接下来，我们运用yield在做个实例：</p>
<p>求出所有小于2000000的质数的和1+2+3+5+7+11+13+….<br>通常，我们的思路是这样for i in [所有小于2000000的质数]<br>sum+=i</p>
<!-- python -->
<pre><code>#首先，我们定义 判断质数的方法
def is_prime(number):
if number &gt; 1:
    if number == 2:
        return True
    if number % 2 == 0:
        return False
    for current in range(3, int(math.sqrt(number) + 1), 2):
        if number % current == 0:
            return False
    return True
return False

all_prime=[]#用一个list来装载所有的质数
for i in range(1,2000000):
    if is_prime(i):
        all_prime.append（i）

len(all_prime) #len看了下大概有66W个质数
sum=0
for i in all_prime:
    sum+=i
</code></pre><p>这种方法也可以。只不过非常消耗资源。因为要把这么大一堆质数装载到一个容器里，这占了很多空间。</p>
<p>我们换成另一种方法：<br><!-- python --></p>
<pre><code>def sum_prime(maxnum):
    total = 0
    for next_prime in get_primes(2):#质数从2开始
        if next_prime &lt; int(maxnum):
            total += next_prime
        else:
            print(total)
            return

def get_primes(number):
    while True:
        if is_prime(number):
            yield number
        number += 1 

sum_prime(2000000)
</code></pre><p>这样就节省了很多资源，中间也不用管它到底是有哪些质数，我只要得出总的和，结果 就够了。</p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>聊聊python的unicode 问题</title>
    <url>/2016/python_unicode.html</url>
    <content><![CDATA[<h2 id="Python的中文字符处理"><a href="#Python的中文字符处理" class="headerlink" title="Python的中文字符处理"></a>Python的中文字符处理</h2><p>都说python对中文不太友好。</p>
<p>我们看看下面两个例子：<br>测试环境是ubuntu;<br>python版本为2.7<br><!-- python --></p>
<pre><code>》a = &apos;呵呵&apos;
》a 
》&apos;\xe5\x91\xb5\xe5\x91\xb5&apos;
》print a
》呵呵
</code></pre><p>为什么print 和 直接输入变量回车显示的东西不一样呢？<br>\xe5\x91\xb5\xe5\x91\xb5这些是什么东西啊。一定很奇怪。其实bytes字符编码，是8进制的东西。</p>
<a id="more"></a>
<p>首先’呵呵‘ 字符串赋值给a。然而python 内部是以’\xe5\x91\xb5\xe5\x91\xb5’保存的，这应该是utf-8（因为ubuntu默认为uhf-8）的unicode 对应的8进制字符编码</p>
<p>我们再看看：<br><!-- python --><br>    》a=u’呵呵’  #带上u表示将这字符串转为unicode<br>    》a<br>    》u’\u5475\u5475’<br>    》print a<br>    》呵呵</p>
<p>我们发现print真是厉害，不管是字符串 还是 unicode 它都能显示为中文的字符。而回车显示的u’\u5475\u5475’是什么呢？这是’呵呵’对应的uncode编码</p>
<p>我们知道unicode可以 通过encode 编码来转为str</p>
<!-- python -->
<pre><code>》a.encode(&apos;utf-8&apos;)
》&apos;\xe5\x91\xb5\xe5\x91\xb5&apos;
》print a.encode(&apos;utf-8&apos;)
》呵呵
</code></pre><p>我们看到encode后显示的str和我们最开始显示的’\xe5\x91\xb5\xe5\x91\xb5’一样。这说吗中文的字符串在python里就是用\x这种字符串编码保存的</p>
<p>print 可以把这种编码自动地解码并且显示为对应的中文字符</p>
<p>另外，有些时候会碰到这种情况，请看下面：</p>
<!-- python -->
<pre><code>》a=u&apos;呵呵&apos;
》l=[a]
》s=str(l)
》s
》&quot;[u&apos;\\u5475\\u5475&apos;]&quot;
</code></pre><p>哟，怎么多了个\<br>有时候，python会以这种编码保存，这不知道是什么编码，多了一个斜杠的<br>遇到这种情况我们可以用如下办法<br><!-- python --></p>
<pre><code>》s.decode(&apos;unicode_escape&apos;)
》s
》u&quot;[u&apos;\u5475\u5475&apos;]&quot;
</code></pre><p>这样就可以把它变回来了。此外我们可以print s一下看看会出现什么呢？<br><!-- python --></p>
<pre><code>》print s
》[u&apos;\u5475\u5475&apos;]
</code></pre><p>哈，其实print 就是把不好读的编码优化为比这种编码更好读的一种形式</p>
<h3 id="总结：print就是让显示出来的东西尽量友好。没那么多乱七八糟"><a href="#总结：print就是让显示出来的东西尽量友好。没那么多乱七八糟" class="headerlink" title="总结：print就是让显示出来的东西尽量友好。没那么多乱七八糟"></a>总结：print就是让显示出来的东西尽量友好。没那么多乱七八糟</h3>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>聊聊python的变量问题</title>
    <url>/2016/python_func_params.html</url>
    <content><![CDATA[<h2 id="Python的参数作用范围"><a href="#Python的参数作用范围" class="headerlink" title="Python的参数作用范围"></a>Python的参数作用范围</h2><p>我们看看下面两个例子：</p>
<!-- python -->
<pre><code>a = &apos;hello&apos;
def fun(a):
    a = &apos;lol&apos;
fun(a)
print a  # &apos;hello&apos;

a=[]
def fun(a):
    a.append(1)
fun(a)
print a  # [1]
</code></pre><p>为什么第一个例子print出来的是’hello’而不是’lol 呢?<br>为什么第二个例子print出来却是[1] 呢?</p>
<a id="more"></a>
<p>我们通过id() 来查看内存的引用：</p>
<!-- python -->
<pre><code>a = &apos;hello&apos;
def fun(a):
    print &quot;point&quot;,id(a)   # point 213224723696912
    a = 2
    print &quot;re-point&quot;,id(a), id(2)   # re-point 2132244845844160 2132244845844160
print &quot;point&quot;,id(a), id(1)  # point 213224723696912 213224723696912
fun(a)
print a  # &apos;hello&apos;
</code></pre><p>可以看到，在执行完a=’hello’之后，a的引用中保存的值，即内存地址发生变化，由原来’hello’对象的所在的地址变成了’lol这个实体对象的内存地址。</p>
<p>而第2个例子a引用保存的内存值就不会发生变化：<br><!-- python --></p>
<pre><code>a = []
def fun(a):
    print &quot;point&quot;,id(a)  # point 53629256
    a.append(1)
print &quot;point the same&quot;,id(a)     # point the same 53629256
fun(a)
print a  # [1]
</code></pre><p>这里记住的是类型是属于对象的，而不是变量。而对象有两种,“可更改”（mutable）与“不可更改”（immutable）对象。在python中，strings, tuples, 和numbers是不可更改的对象，而list,dict等则是可以修改的对象。</p>
<p>当一个引用传递给函数的时候,函数自动复制一份引用,这个函数里的引用和外边的引用没有半毛关系了.所以第一个例子里函数把引用指向了一个不可变对象,当函数返回的时候,外面的引用没半毛感觉.<br>而第二个例子就不一样了,函数内的引用指向的是可变对象,对它的操作就和定位了指针地址一样,在内存里进行修改.</p>
<p>在类变量和实例变量中也有类似的例子：<br><!-- python --></p>
<pre><code>class Person:
    name=&quot;aaa&quot;

p1=Person()
p2=Person()
p1.name=&quot;bbb&quot;
print p1.name  # bbb
print p2.name  # aaa
print Person.name  # aaa
</code></pre><p>类变量就是供类使用的变量,实例变量就是供实例使用的.</p>
<p>这里p1.name=”bbb”是实例调用了类变量,这其实和上面第一个问题一样,就是函数传参的问题,p1.name一开始是指向的类变量name=”aaa”,<br>但是在实例的作用域里把类变量的引用改变了,就变成了一个实例变量,self.name不再引用Person的类变量name了.</p>
<p>可以看看下面的例子:<br><!-- python --></p>
<pre><code>class Person:
    name=[]

p1=Person()
p2=Person()
p1.name.append(1)
print p1.name  # [1]
print p2.name  # [1]
print Person.name  # [1]
</code></pre>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark简易入门</title>
    <url>/2016/spark_guide.html</url>
    <content><![CDATA[<h1 id="Spark简易入门"><a href="#Spark简易入门" class="headerlink" title="Spark简易入门"></a>Spark简易入门</h1><p>Spark，搞大数据的人都应该用过吧。spark作为主流的大数据处理框架，到底为什么这么多人用呢？我们少扯淡，直接动手写大数据界的HelloWorld：WordCount。</p>
<p>先贴上代码（Scala版本):</p>
<!-- java -->
<pre><code>import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
object WordCount 
  def main(args: Array[String]) {
    val conf = new SparkConf()
    conf.setAppName(&quot;WorkCount&quot;)
    val sc = new SparkContext(conf)

    val file = &quot;hdfs://127.0.0.1:9000/file.txt&quot;
    val lines = sc.textFile(file)
    val words = lines.flatMap(_.split(&quot;\\s+&quot;))
    val wordCount = words.countByValue()
    println(wordCount)
}
</code></pre><p>短短10多行代码，就已经完成了，比大家想象的要简单，完全看不出Spark背后做了什么处理。分布式，容错处理，这就是Spark给我们带来的福利。</p>
<a id="more"></a>
<h3 id="接下来我们了解下Spark的核心："><a href="#接下来我们了解下Spark的核心：" class="headerlink" title="接下来我们了解下Spark的核心："></a>接下来我们了解下Spark的核心：</h3><ul>
<li>Spark上下文</li>
</ul>
<p>Spark集群的执行单位是<strong>Application</strong>，任何提交的任务都会产生一个Application。一个Application只会关联上一个Spark上下文，也就是SparkContext。构建SparkContext时可以传入Spark相关配置，也就是SparkConf，它可以用来指定Application的名称，任务需要集群的CPU核数/内存大小，调优需要的配置等等。</p>
<p><code>val conf = new SparkConf()</code><br><br><code>conf.setAppName(&quot;WorkCount&quot;)</code><br><br><code>val sc = new SparkContext(conf)</code><br></p>
<p>这三行语句创建了一个Spark上下文，并且运行时这个Application的名字就叫WordCount。</p>
<ul>
<li>弹性分布式数据集RDD</li>
</ul>
<p>Spark中最主要的编程概念就是弹性分布式数据集 (resilient distributed dataset,RDD)，它是元素的集合，划分到集群的各个节点上，可以被并行操作。RDD的创建可以从HDFS(或者任意其他支持Hadoop文件系统) 上的一个文件开始，或者通过转换Master中已存在的Scala集合而来。</p>
<p><code>val file = &quot;hdfs://127.0.0.1:9000/file.txt&quot;</code><br><br><code>val lines = sc.textFile(file)</code><br></p>
<p>这两行语句从HDFS文件中创建了叫lines的RDD，它的每个元素就对应文件中的每一行，有了RDD我们就可以通过它提供的各种API来完成需要的业务功能。</p>
<p>RDD提供的API分为两类：转换（Transformation）和动作（Action）。</p>
<ul>
<li>转换</li>
</ul>
<p>顾名思义，转换就是把一个RDD转换成另一个RDD。当然，光是拷贝产生一个新的RDD出来是没有太大意义的，这里的转换实际上是RDD中元素的映射和转换。有一点必须要注意的是，RDD是只读的，一旦执行转换，一定会生成一个新的RDD。<br></p>
<p><code>val words = lines.flatMap(_.split(&quot;\\s+&quot;))</code><br></p>
<p>flatMap是RDD众多转换中的一种，它的功能是把源RDD中的元素映射成目的RDD中的0个或者多个元素。上面语句把以文本行为元素的RDD转换成了以单个单词为元素的RDD。</p>
<ul>
<li>动作</li>
</ul>
<p>“动作”就不好望文生义了，可以简单地理解成想要获得结果时调用的API。</p>
<p><code>val wordCount = words.countByValue()</code><br></p>
<p>countByValue就是一个“动作”，它的功能是统计RDD中每个元素出现的次数，得到一个元素及其出现次数的Map。</p>
<p><strong>提示：返回结果为RDD的API是转换，返回结果不为RDD的API是动作。</strong></p>
<ul>
<li>运行</li>
</ul>
<p>要运行Spark任务，首先要把代码打成JAR包，额。。。这个不需要多言。</p>
<p>打包后，就只需在Spark集群上以命令行的方式用spark-submit提交就OK。</p>
<p><code>spark-submit --class &quot;demo.WordCount&quot; SparkDemo-1.0-SNAPSHOT.jar</code><br></p>
<p>其中demo.WordCount是main函数所在的ojbect，而SparkDemo-1.0-SNAPSHOT.jar就是打出来的jar包。</p>
<p>Spark 最简单的流程，就这样Finish了。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Streaming Dstream ForeachRDD的理解</title>
    <url>/2016/spark_foreachRDD.html</url>
    <content><![CDATA[<h1 id="通俗理解Spark-Streaming-Dstream-的ForeachRDD"><a href="#通俗理解Spark-Streaming-Dstream-的ForeachRDD" class="headerlink" title="通俗理解Spark Streaming Dstream 的ForeachRDD"></a>通俗理解Spark Streaming Dstream 的ForeachRDD</h1><p>在SparkStreaming中，数据中的每个batch都是只有一个RDD，为什么我们还要用ForeachRDD 在每一个RDD呢？ 不是只有一个RDD吗？</p>
<p>Dstream也就是离散stream，就是把连续的数据分成一小团一小团。我们用专业术语“microbatching”来描述。每个microbatch 变成一个RDD以便Spark的后续处理。在每一个batch interval中，每个DStream有且仅有一个RDD。</p>
<p><img src="/images/2016/spark_foreachRDD/spark-streaming.png" alt="Sample Image Added via Markdown"></p>
<p>然而RDD是什么呢，RDD是一个分布式数据集合。你可以认为它是一个告诉你实际数据在集群中具体什么地方的指南者（pointer）。</p>
<a id="more"></a>
<p>Dstream.foreachRDD在SparkStreaming中是一个“output operator”，它让你直接基于Dstream的RDDs去处理数据。比如说，用foreachRDD方法将数据写入数据库</p>
<p>这里有个容易疑惑的地方，DStream是和时间有关的集合。我们来对比一下传统的集合。我们用一群客户（users）来举个例子：</p>
<!-- scala -->
<pre><code>val userDStream：DStream[User]=???
userDStream.foreachRDD{usersRDD =&gt;
    usersRDD.foreach{user =&gt; serveCoffee(user)}
}
</code></pre><p>注意：</p>
<ul>
<li>DStream.foreachRDD返回给你的是RDD[user],不是单单一个user。回到上面咖啡的这个例子，这个客户集合 是指某个时间区间(比如下午一点到两点)的集合</li>
<li>为了处理单个集合的元素，需要进一步操作(operate) RDD。在这个案例中，我们用RDD.foreach来对每个客户serve coffee</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>怎么变得更有效率 ————Aaron Swarts</title>
    <url>/2016/be_more_productive.html</url>
    <content><![CDATA[<h1 id="How-to-be-More-Productive–Aaron-Swartz"><a href="#How-to-be-More-Productive–Aaron-Swartz" class="headerlink" title="How to be More Productive–Aaron Swartz"></a>How to be More Productive–Aaron Swartz</h1><p>原文在 <a href="http://www.aaronsw.com/weblog/productivity" target="_blank" rel="external">http://www.aaronsw.com/weblog/productivity</a> </p>
<p>有人跟我说：“你花在看电视上的时间足够用来写本书了。”毫无疑问，把时间花在写书上花在看电视上更好。但这里隐含了一个假设，即时间是“可互换的”。也就是说，看电视的时间可以轻松地用来写书。但悲催的是事实并非如此。</p>
<p>不同的时间有不同的质量等级。如果我正走向地铁站而且忘带笔记本了，我就很难写什么文章。同样，如果你不停地被打断，也很难集中注意力。另外还有些心理和情感上的因素：有时候我心情不错，就愿意去主动做一些事；也有些时候我心情郁闷，就只能看看电视了。</p>
<p>如果你想变的更加有效，你必须意识到这个事实，并很好的处理它。首先，你得利用好不同类型的时间。其次，你得提高时间的质量。</p>
<h2 id="1-更有效的利用你的时间"><a href="#1-更有效的利用你的时间" class="headerlink" title="1. 更有效的利用你的时间"></a>1. 更有效的利用你的时间</h2><p>1.1 选择好问题</p>
<a id="more"></a>
<p>生命是如此的短暂（别人这么告诉我），为什么浪费时间去做那些没有意义的事呢？做一些让你感到舒适的事是很容易的，但你应该不断地问自己为什么要做这些事呢？有没有更重要的事情等着你去做呢？为什么你不去做那些事呢？这些问题很难回答（如果你遵循这个规则，慢慢地你就不得不问自已为什么没去做那些世界上最重要的事了），但是每一个小小的进步都会让你更加有效。</p>
<p>这不是说你所有的时间都必须用来做那些最重要的事。我的时间肯定就不是这样的（毕竟，我现在还在写这篇文章呢）。但这是我衡量自己生活的明确标准。</p>
<p>1.2  收集很多问题</p>
<p>另一个公开的秘密是：如果你认准一件事并集中精力只做这一件事，你的效率就是最高的。但我发现这是很不现实的。就以现在为例吧，我正在调整坐姿，锻炼身体，喝水，清理桌面，和我弟弟聊天，同时还在写这篇文章。今天一整天，我写了现在这篇文章，读了本书，吃了点东西，回了几封邮件，和一些朋友聊了聊天，买了点东西，改了改其他几篇文章，备份了硬盘，还整理了一下图书列表。在过去的一周里，我做了好几个不同的软件项目，读了好几本书，学习了好几种不同的编程语言，等等。</p>
<p>有很多不同的项目能让我能在不同质量的时间下做不同的工作。而且，在你卡壳或是厌烦的时候有其他的一些事可以做（你可以给你的大脑一些时间来放松）。</p>
<p>同时这会让你变得更有创造力。创造力就是把你自己从其他地方学到的东西用到你正在做的工作中。如果你同时做许多不同方向的工作，那你就会得到更多的想法和创意。</p>
<p>1.3  列出清单</p>
<p>找一些不同的事同时做并不困难，大部分人都有很多很多事要做。但是如果你想把它们全部记在脑袋里的话，它们很快就会消失的。想记住所有这些事给你带来的心理压力会把你逼疯的。解决办法很简单：把它们写下来。<br>一旦你把要做的事列成清单，你就可以更好地进行分类组织了。例如，我的清单包括：编程、写作、思考、跑腿、阅读，聆听、观看等。</p>
<p>大部分项目都包括很多不同的任务。以写这篇文章为例，除了纯粹的文字写作外，还包含阅读其他关于拖延的文章，构思文章的结构，润色语句，写邮件向别人请教问题等。每一项任务都属于清单的不同部分，所以你可以在合适的时间再去做。</p>
<p>1.4  把清单和生活结合起来</p>
<p>一旦你有了这个清单，你就要经常记得看它。记得看它的最好方法是把它放在你能看到的地方。例如，我总是在桌子上堆一摞书，最上面的那一本就是我最近在读的。当我想要读书的时候，我就直接把最上面的那一本抓过来。</p>
<p>我看电视和电影时也这么做。如果想看某部电影，我就会把它放在电脑中一个专门的文件夹里。当我想休息一下看看电影的时候，我就会打开那个文件夹。</p>
<p>我也想过一些更强制性的方法，比如说我想查看博客，会弹开一个页面，列出我“待读”文件夹里的文章。或者当我不小心犯了错时，就弹开一个窗口，提出工作建议。</p>
<h2 id="2-提高你时间的质量"><a href="#2-提高你时间的质量" class="headerlink" title="2 提高你时间的质量"></a>2 提高你时间的质量</h2><p>像上面那样最大限度的利用时间还远远不够，更重要的是提高你自己的时间的质量。大多数人的时间都被上学、工作之类的事情吃掉了。如果你属于其中之一，你必须停下来。但你还能做什么呢？</p>
<p>2.1 减轻身体上的约束</p>
<p>2.1.1 携带纸和笔</p>
<p>我认识的很多人都有随身携带笔记本之类东西的的习惯。纸和笔在很多时候都是非常有用的，你可以给某人写点什么东西、针对什么做点记录、写下自己的想法等。我甚至在地铁上写过一整篇文章。（我以前是这样的，但我现在只用带智能手机。它不用让我给人物理信息，但可以一直给我提供读的东西，我可以把笔记直接写在收件箱里）</p>
<p>2.1.2  避免被打扰</p>
<p>对于那些需要集中注意力的任务，你应该避免被打扰。一个很简单的方法是去一个没人能打扰你的地方，另一个方法是告诉周围的人“关门的时候不要打扰”或“我戴耳机时给我发消息”（然后你在有空的时候再看消息）<br>这一点不要做过了。当你浪费时间的时候你反倒应该被打扰一下，帮助别人解决问题肯定比坐在那里看新闻更好的利用了时间。所以可以达成一个专门的协议：当你没有集中精力的时候你可以被打扰。</p>
<p>2.2  减轻心理上的约束</p>
<p>2.2.1  吃、睡和锻炼</p>
<p>当你感到很饿、很累、很焦躁的时候，你的时间的质量会很低。解决这个问题很简单，就是：吃、睡和锻炼。但我有时候做得不好，虽然觉得很饿了，但我还是一直工作而不想吃东西，结果最后实在太累了都没法吃东西了。</p>
<p>对自己说“虽然我很累了，但我不能休息，因为我必须要工作”会让你觉得自己很努力，但事实上休息之后你的效率会更高。既然你迟早都要睡觉，还不如先休息好，再来提高剩余时间内的效率。</p>
<p>我锻炼其实不多，所以不好给出建议，但我仍尽力做好。我躺着读书时，我就做仰卧起坐。我要步行去什么地方时，我就跑步。</p>
<p>2.2.2 与快乐的人谈话</p>
<p>减轻精神负担是很难的，与快乐的人做朋友可起到帮助。比如，我在和Palu Graham或Dan Connolly交谈后总是更乐于工作，他们总是释放正能量。也许有人愿意关在屋子里埋头苦干，不与其他人接触，他们觉得这样时间才没有被“浪费”，但事实上这会让他们变得情绪低落，工作效率也会大大下降。</p>
<p>2.2.3 分担压力</p>
<p>即便你的朋友不能给你带来快乐，和其他人一起做事也会让难题变简单。一方面，精神上的压力大家可以互相分担，另一方面，和其他人在一起可以让你专注于工作而不是时常分心。</p>
<p>2.3  拖延和精神力场</p>
<p>上面所说的那些并不是问题的核心，关于效率大家最大的问题还是“拖延”。虽然很多人不承认，但是几乎所有人都会或多或少拖延，不只是你。但这不意味着你不必避免它。</p>
<p>拖延是什么？从旁观者来看，你在玩（如玩游戏，看新闻）而不是在做事，这让别人以为你很懒、很糟糕。但问题的关键是：为什么会这样呢？你的脑子里究竟是怎么想的？</p>
<p>我花了很多时间来研究这件事，我能给出的最好解释是你的大脑赋予每项任务一种“精神力场”。你玩过两块磁铁相互作用吗？如果你让它们异极相对，他们就会相互排斥，你会感到他们之间的磁场力。你越是想要把它们合在一起，越会感到它们之间的排斥。</p>
<p>精神上也是类似。你看不见摸不着它，但你却可以感受到它的存在。你越是想要接近它，它会离你越远。</p>
<p>你不可能通过蛮力来克服两个场之间的排斥力，你一不用力了它们就会转过来。我也从来没有通过纯粹的自制力来克服这种精神力场。其实，你不应该强制，你应该悄悄地调转方向。</p>
<p>那又是什么产生了这种精神力场呢？似乎有两个主要原因：任务是否艰巨，任务是否是被指派的。</p>
<p>2.3.1 艰巨的任务</p>
<p>2.3.1.1 把任务细分</p>
<p>一个任务很艰巨的原因之一是这个任务很庞大。比如说你想要做一个菜谱构造程序，没有人能坐下来就完成一个菜谱构造器。这是一个目标，不是一项任务。任务是使你能够朝向目标更迈进的具体步骤。一个好的任务应该类似于“画出展示菜谱的屏幕的模型”，这是你能够立即做的。</p>
<p>当你完成了一个任务后，下一步就会变得更加清晰。你将会考虑一个菜谱由什么构成，你需要什么样的搜索机制，如何构建菜谱的数据库，等等。这样你就构建了一个引擎，每一个任务都会通向下一个任务。</p>
<p>对于每一个大项目，我都会考虑我需要完成一系列什么样的任务，并且将这些任务加入到我分类的待办事项列表中去。同样，当我做完一些任务之后，我会把接下来需要完成的任务再加入任务列表中去。</p>
<p>2.3.1.2  简化任务</p>
<p>另一个让任务变得艰巨的原因就是它太复杂了。写书这个任务会放你无所适从，那么就先从写文章开始吧。如果一篇文章也觉得太复杂了，那么就先写一个段落的概要吧。最重要的是真正做了一些工作，真正的有进展。</p>
<p>一旦你明确了你的任务之后，你就可以更清楚的判断它，更容易的理解它。完善现有的东西比从头创建东西更容易。如果你的一个段落写好了，那么一点一点积累，它会变成一篇文章，最终变成一本书。</p>
<p>2.3.1.3  认真考虑它</p>
<p>通常来说解决一个难题需要些灵感。如果你对那个领域并不熟悉，你应该从研究这个领域开始，借鉴一下其他人的经验，慢慢的理解这个领域，并且做一些小的尝试看看你能否搞定这个领域。</p>
<p>2.3.2  被指派的任务</p>
<p>被指派的任务是那些别人要求你做的任务。很多心理学实验都表明：当你“刺激”其他人做什么事的时候，他们反倒不容易做好那个事。奖励，惩罚等外部刺激会扼杀“内在动机”——你对于某个问题发自内心的兴趣。（这是社会心理学中最完全重复的发现之一了——70多项研究发现奖励会减弱任务的兴趣） 。人类的大脑对于被要求做的事有先天的抗拒力 。</p>
<p>奇怪的是这种现象不仅局限于其他人要求你做的事，当你向自己分配任务时仍然会出现这种现象。如果你对自己说“我应该好好做X工作了，这是我现在最重要的事”，之后你就会感到X突然变成了世界上最困难的事情了。然而一旦当Y变成了“最重要的事”，原来的那个X又变得简单了。</p>
<p>2.3.2.1  虚构一个任务</p>
<p>如果你要完成X，那就告诉自己做Y。然而不幸的是，这样欺骗自己也很难，因为你清楚你究竟要做什么。所以你必须悄悄地做。</p>
<p>一个方法是让别人给你分配点什么事情。最著名的例子就是毕业生必须要写篇论文才能毕业，这是一项很难的任务，为了不做这件事，研究生结果做了很多其他很难的工作。</p>
<p>这项工作必须看上去非常重要（不做就不能毕业）也非常艰巨（你最好的工作写上百页），但实际上没那么重要，放下来也不会成为什么灾难。</p>
<p>2.3.2.2  不要自己给自己布置任务</p>
<p>给自己布置任务看起来很诱人，比如对自己说“好吧，我要把这些放在一边，坐下来将这篇文章写完”。更糟糕的是给自己一些奖励，比如“如果我写完这篇文章，我就吃点糖果。”最糟糕的是让别人假装布置给你一些任务。</p>
<p>这些方式都很诱人——我自己完成了——但是这些都会让你变得更没有效。在这三种情况下，你还是在给自己布置任务，你的大脑只会去逃避它。</p>
<p>2.3.2.3 把事情变得有趣</p>
<p>困难的工作听起来不会令人感到愉悦，但事实上这可能就是最能让我感到高兴的事。一个困难的问题不但能让你集中全部注意力，而且当你完成它的时候你会感到非常棒，非常有成就感。</p>
<p>所以帮助自己完成一件事的秘密不是说服自己必须完成它，而是说服自己这件事确实非常有意思。如果一件事没有意思的话，你需要做的就是让它变得有意思。</p>
<p>我最开始认真考虑这这个问题是我在大学与论文的时候。写论文不是一个特别难的任务，但的确是被分配的。谁愿意去写什么两本毫无关系的书有什么关系呢。所以我开始将论文编到我自己的小把戏中。比如，我决定每一段都写出自己的小风格，尽力去模仿各种形式的演讲。</p>
<p>让事情更有趣的另一个方法是解决元问题。不要去构造一个WEB应用，而是构造一个WEB应用框架，将之作为一个示范应用。这种任务不但更有趣，而且更有用。</p>
<h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h2><p>高效有很多神话——什么时间是可替换的、集中精力是好的、奖励你自己是好的、艰巨的工作是不爽的，拖延是不自然的——但它们都有一个共同的主题：真实的工作违反了你内心的倾向。</p>
<p>对大多数人在大多数工作中，这也许成立。你没有理由写无聊的文章或归档无意义的纪要，如果社会强迫你做，你需要学会关闭你头脑中让你停止的声音。</p>
<p>但如果你正在做一些有意义的有创造性的事情，关闭你的大脑就是错误的。效率的真正秘密在于“聆听自己”，在你饿的时候吃饭，在你疲惫的时候睡觉，当你厌烦的时候休息一下，做那些有趣好玩的项目。</p>
<p>这看起来太容易了。它不包含任何花哨的缩写、或自制力、或个人的成功经验。但是社会上的一些观念正在把我们向相反的方向引导。要想变得更加有效，我们需要做的就是转过头来聆听自己。</p>
<p>嗯。</p>
]]></content>
      <categories>
        <category>study</category>
      </categories>
      <tags>
        <tag>daily</tag>
      </tags>
  </entry>
</search>
