<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Tmux简明教程</title>
    <url>/2019/tmux_guide.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>你经常可能会遇到这样的情况：你在vim编辑你在远程服务器上面的代码，然后你要新开一个窗口再次ssh到<br>这个远程服务器来测试运行你的代码。此外，如果你的WIFI断线了，你的所有session都会挂掉，GG。<br>然后你又要麻烦地重新开两个session。</p>
<p>其实，上述问题都可以用tmux解决，一个可以提供如下功能的命令行工具：</p>
<ol>
<li>在一个terminal window内开启多个windows，panes。</li>
<li>在一个session内(这个session会一直保持着，即使断网的时候也会保持着)保持windows和panes。</li>
<li>能够共享session(这功能对结对编程来说真是太妙了！)，就是说，你在session上做得所有操作，<br>在另一台电脑上连接到同一个session，会看到同样的输入和输出。</li>
</ol>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>Ubuntu用户只需要执行<code>sudo apt-get install tmux</code><br>Mac 用户只需要执行<code>brew install tmux</code></p>
<p>安装完后，执行如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">tmux</div></pre></td></tr></table></figure></p>
<p><img src="/images/2019/tmux_guide/1dec3830.png" alt=""><br>这看起来和普通的终端差不多，除了底下有条绿色的状态bar。在这个bar上，我们可以运行tmux命令去控制管理终端windows和sessions</p>
<h3 id="Tmux基本命令："><a href="#Tmux基本命令：" class="headerlink" title="Tmux基本命令："></a>Tmux基本命令：</h3><p>当你进入到tmux后，你可以通过先运行一个prefix key来执行各种命令。默认来说，tmux的prefix key是 ctrl + b。也就是说在执行<br>其他任何命令之前你都要先同时按下ctrl + b。 </p>
<p>tmux的命令有很多很多，但首先我们了解下基本的就好了，如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;prefix&gt; c: 创建一个新的window（在status bar中显示）</div><div class="line">&lt;prefix&gt; 0: 切换到 window 0</div><div class="line">&lt;prefix&gt; 1: 切换到 window 1</div><div class="line">&lt;prefix&gt; 2: 切换到 window 2 (etc.)</div><div class="line">&lt;prefix&gt; x: Kill 当前的window </div><div class="line">&lt;prefix&gt; w: 切换 window</div><div class="line">&lt;prefix&gt; d: detach 到tmux（也就是说当你离开后，再次回到session中）</div></pre></td></tr></table></figure></p>
<p>如果你把在tmux session的所有的window都kill后，它将会kill整个session并且回到普通的终端上。<br>如果你用<prefix> d 去 detach到tmux，你将会回到你的tmux session，而且你可以看到你之前运行的东西已然在那里。<br>要查看所有的tmux sessions，你可以使用：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">tmux ls</div></pre></td></tr></table></figure></prefix></p>
<p>要attach到最后一个使用的session，你可以用：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">tmux a</div></pre></td></tr></table></figure></p>
<p>要attach到指定的某个session，你可以用：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">tmux a -t &lt;session-name&gt;</div></pre></td></tr></table></figure></p>
<p>这里补充说明下 session 和 window之间的关系，我们看看这个图，<br>一个window相当于你的显示器能看到的所有东西，然后一个window上可以分成一块块的拼图，也就是各个panes：<br><img src="/images/2019/tmux_guide/5a861515.png" alt=""></p>
<p>这就这么一些。现在你可以通过多个终端window和永久的sessions来在你的远程电脑上工作了。<br>你甚至可以通过让两个人attach到同一个session来进行结对编程！</p>
<h3 id="下一步"><a href="#下一步" class="headerlink" title="下一步"></a>下一步</h3><p>上面的命令对掌握tmux真的非常有用。通常，我尽量少开窗口，一般是一个window用于vim，另一个window做开发，第3个窗口去运行命令。<br>大多数人倾向于用panes来在同一个屏幕上展现多个内容。</p>
<p>Panes是个特别炫酷的东西而且很值得学习，但是推荐你们先放一放，不要急着学习太多panes的东西，因为它会增加你的tmux学习曲线。<br>当你用panes时候，你要记住水平和垂直创建panes的命令，还有切换不同panes的命令，还有改变panes大小，关闭panes的命令，等等等等。<br>所以我建议先把其它的玩熟练再去用panes这些花哨的东西。</p>
<p>你也可以通过在home目录下添加<code>.tmux.conf</code>文件来指定tmux的一些配置。<br>比如通常可以在里面改变 prefix的快捷键（很多人会使用ctrl+a 作为prefix）。<br>初始window数设置为1，而不是0;然后还可以设置下颜色。<br>如果你打算重新绑定prefix，很有可能这将会为你以后形成肌肉记忆了，哈哈。</p>
<p>下面是一个精简版的.tmux.conf, 只改变了window数和 prefix:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># remap prefix from &apos;C-b&apos; to &apos;C-a&apos;</div><div class="line">unbind C-b</div><div class="line">set-option -g prefix C-a</div><div class="line">bind-key C-a send-prefix</div><div class="line"># Start window numbering at 1</div><div class="line">set -g base-index 1</div><div class="line"></div><div class="line"># 建议加上这个，用鼠标会有很大惊喜，呵呵。在多个panes的时候切换用鼠标点一下就好了！</div><div class="line"># 同时也解决了scroll的问题，可以轻松地用鼠标在不同的panes内滚动！</div><div class="line">set -g mouse on</div></pre></td></tr></table></figure></p>
<p>tmux有非常多的可选配置，但是你得由浅到深，刚开始尽可能精简。当你想尝试一些更高级的功能时，不要在网上复制粘贴。<br>你必须清楚<code>.tmux.conf</code>的每一个配置的作用是什么。</p>
<p>一旦你感觉你掌握了基础，将会有很多好玩的高级tmux游戏等着你。</p>
]]></content>
      <categories>
        <category>tmux</category>
      </categories>
      <tags>
        <tag>tmux</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo博客SEO优化，添加robots.txt</title>
    <url>/2019/hexo_seo.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近一时兴起，想提高自己博客的点击率，就尝试做了一些SEO优化，并且加入了Google Adsense广告。呵呵，<br>写博客这么辛苦，赚点钱是应该的嘛。</p>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>废话不说了，直接进入主题。都知道要想在百度搜索，Google搜索结果页中排得靠前，你得让人家的爬虫搜你嘛，<br>所以我们得通过一个叫<code>robots.txt</code>的文件放在根目录上。这文件的目的，就是告诉搜索引擎应该搜索我这网站的那些内容。<br>我们当然希望是搜索我们文章内容本身，不要去搜那些JavaScript和CSS代码。</p>
<h3 id="配置-robots-txt"><a href="#配置-robots-txt" class="headerlink" title="配置 robots.txt"></a>配置 robots.txt</h3><p>我们在hexo 根目录下的 <code>public</code> 目录下新建一个<code>robots.txt</code>文件，内容如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">User-agent: *</div><div class="line">Allow: /</div><div class="line">Allow: /archives/</div><div class="line">Allow: /categories/</div><div class="line">Allow: /tags/</div><div class="line">Allow: /about/</div><div class="line"></div><div class="line">Disallow: /vendors/</div><div class="line">Disallow: /js/</div><div class="line">Disallow: /css/</div><div class="line">Disallow: /fonts/</div><div class="line">Disallow: /fancybox/</div><div class="line"></div><div class="line">Sitemap: https://mikolaje.github.io/sitemap.xml</div><div class="line">Sitemap: https://mikolaje.github.io/baidu_sitemap.xml</div></pre></td></tr></table></figure></p>
<p>最后面两行是site-map</p>
<p>这里要注意的是如果js和fonts这些加了disallow的话，会出现谷歌抓取问题。<br><img src="/images/2019/hexo_seo/google_spyder.png" alt=""><br>因为现在（2019-09以后）Google Search默认是用智能手机引擎来抓取，<br>所以如果js和css这样被disallow的话会有问题，建议还是把上面的disallow去掉。</p>
<h3 id="Sitemap即网站地图"><a href="#Sitemap即网站地图" class="headerlink" title="Sitemap即网站地图"></a>Sitemap即网站地图</h3><p>它的作用在于便于搜索引擎更加智能地抓取网站。<br>最简单和常见的sitemap形式，是XML文件，在其中列出网站中的网址以及关于每个网址的其他元数据（上次更新时间、更新的频率及相对其他网址重要程度等）。</p>
<p>要使用<code>sitemap</code>我们需要安装两个hexo的插件：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">npm install hexo-generator-sitemap --save</div><div class="line">npm install hexo-generator-baidu-sitemap --save</div></pre></td></tr></table></figure></p>
<p>然后，我们要在根目录下的<code>_config.yml</code> 的最后面添加如下内容：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><div class="line"><span class="attr">sitemap:</span></div><div class="line"><span class="attr">  path:</span> sitemap.xml</div><div class="line"><span class="attr">baidusitemap:</span></div><div class="line"><span class="attr">  path:</span> baidusitemap.xml</div></pre></td></tr></table></figure>
<h3 id="配置-google-analytics"><a href="#配置-google-analytics" class="headerlink" title="配置 google analytics"></a>配置 google analytics</h3><p>在theme/next/_config.yml文件下添加如下配置：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">google_analytics: UA-146421499-1</div></pre></td></tr></table></figure></p>
<p>Track ID要到你自己的GA的页面里找</p>
<h3 id="配置ads-txt"><a href="#配置ads-txt" class="headerlink" title="配置ads.txt"></a>配置ads.txt</h3><blockquote>
<p>ads.txt是干什么用的？</p>
<p>授权数字卖方 (ads.txt) 是一项 IAB 计划，可帮助确保您的数字广告资源只通过您认定为已获得授权的卖家（如 AdSense）进行销售。创建自己的 ads.txt 文件后，您可以更好地掌控允许谁在您的网站上销售广告，并可防止向广告客户展示仿冒广告资源。</p>
</blockquote>
<p>在Google Adsense找到相应的页面下载 ads.txt，然后同样放在根目录的<code>public</code>目录下面。<br><img src="/images/2019/hexo_seo/adsense.png" alt=""></p>
<h3 id="修改博文链接"><a href="#修改博文链接" class="headerlink" title="修改博文链接"></a>修改博文链接</h3><p>HEXO默认的文章链接形式为<code>domain/year/month/day/postname</code>，默认就是四级url，并且可能造成url过长，对搜索引擎是不太不友好，<br>我们可以改成domain/postname 的形式。编辑站点的_config.yml文件，<br>修改其中的permalink字段改为<code>permalink: :title.html</code>即可。</p>
<blockquote>
<p>配置完成后，重新部署hexo：<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code></p>
</blockquote>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>SEO</tag>
      </tags>
  </entry>
  <entry>
    <title>PySpark 会比Scala或Java慢吗（译）？</title>
    <url>/2019/pyspark_slower.html</url>
    <content><![CDATA[<p>首先，你必须知道不同类型的API（RDD API，MLlib 等），有它们不同的性能考虑。</p>
<h2 id="RDD-API"><a href="#RDD-API" class="headerlink" title="RDD API"></a>RDD API</h2><p>（带JVM编排的Python结构）</p>
<p>这是一个会被Python代码性能和PySpark实施影响最大的组件。虽然Python性能很可能不会是个问题，至少有几个因素你要考虑下：</p>
<ul>
<li><p>JVM 通信的额外开销。所有进出Python executor的数据必须通过一个socket和一个JVM worker. 尽管这过程相当高效，以为走的都是本地通信，<br>但多少依然还是要付出点代价。</p>
</li>
<li><p>基于进程的Python executor 对比基于线程（单JVM多线程）的 Scala executors。每个Python executor在它独自的进程里运行。<br>它的副作用是，虽然它有着比JVM更强的隔离性，并且对executor生命周期的一些控制也比JVM更强，但是潜在地会比JVM的executor消耗更多的内存。<br>比如：</p>
<ul>
<li>解析器内存footprint</li>
<li>加载模块的footprint</li>
<li>更低效的广播（因为每个进程需要独自的广播复制）</li>
</ul>
</li>
<li><p>Python本身的性能。总的来说Scala会比Python更快，但不同的task有有所不同。此外，你有其它的选项包括JITs<br>比如Numba，C扩展Cython或者其它专业的lib比如Theano。最后，可以考虑用PyPy作为解析器。</p>
</li>
<li><p>PySpark configuration提供<code>spark.python.worker.reuse</code>参数， 这可以用来对每个task在 forking Python进程和复用已有的进程中作出选择。<br>后者似乎在避免昂贵的垃圾回收方面上更有用（这更多的是一个印象而不是系统测试的结果）</p>
</li>
<li><p>在CPython里首选的垃圾回收方法，引用计数法，和典型的Spark 作业（比如流式处理，没有引用循环）结合得挺好，并且减少了长时间垃圾回收等待的风险。</p>
</li>
</ul>
<h2 id="MLlib"><a href="#MLlib" class="headerlink" title="MLlib"></a>MLlib</h2><p>（结合Python和JVM执行）</p>
<p>基本上要考虑的和前面说的那些差不多，这里再补充一些。尽管MLlib所用的基础架构是Python RDD，所有的算法都是直接用Scala来执行的。这意味着需要额外的开销来将Python 对象转为Scala对象，<br>增长的内存使用率和一些其它的限制我们将来再说。</p>
<p>现在的Spark2.x，基于RDD的API是以一个维护模式存在，Spark3.0计划会移除RDD API。</p>
<h2 id="DataFrame-API-和-Spark-ML"><a href="#DataFrame-API-和-Spark-ML" class="headerlink" title="DataFrame API 和 Spark ML"></a>DataFrame API 和 Spark ML</h2><p>（限制在driver的用Python代码的JVM执行）<br>这些可能是对标准数据处理task最好的选择。因为Python代码在driver端大多被限制在高层次的逻辑操作，在这方面上Scala和Python基本上没有什么区别。<br>有个例外是，按行的Python UDF相对来说会比Scala慢很多。尽管有很多改进的机会（在Spark2.0有着大量的改进），最大的限制还是JVM和Python解析器之间数据传送。</p>
<p>尽量习惯于用Spark内置的一些函数比如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">from pyspark.sql.functions import col, lower, trim</div><div class="line"></div><div class="line">exprs = [</div><div class="line">    lower(trim(col(c))).alias(c) if t == &quot;string&quot; else col(c) </div><div class="line">    for (c, t) in df.dtypes</div><div class="line">]</div><div class="line"></div><div class="line">df.select(*exprs)</div></pre></td></tr></table></figure></p>
<p>应该用Spark的lower而不是Python String的lower，这样做有几个好处：</p>
<ul>
<li>这操作直接将数据到JVM而不用到Python解析器</li>
<li>只需要投影一次，而不用对字段的每个字符串进行投影</li>
</ul>
<p>对了，避免在DataFrame和RDD之间的转换，因为这需要耗费很大的序列化和反序列化工作，更别说JVM和Python之间的数据传输了。</p>
<p>值得注意的是，调用Py4J会有非常高的延迟。这包括这样的调用：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">from pyspark.sql.functions import col</div><div class="line"></div><div class="line">col(&quot;foo&quot;)</div></pre></td></tr></table></figure></p>
<p>通常，这不应该是个问题（overhead是固定的，不取决于数据量，但假如是实时程序，你可能考虑对 Java wrapper进行 缓存/复用 。</p>
<h2 id="GraphX-和-Spark-DataSets"><a href="#GraphX-和-Spark-DataSets" class="headerlink" title="GraphX 和 Spark DataSets"></a>GraphX 和 Spark DataSets</h2><p>对于 Spark 1.6 和 2.1，GraphX和Spark DataSets都不提供Python接口，所以你可以说PySpark比Scala差多了。</p>
<h3 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h3><p>实践里，GraphX开发几乎完全停滞了，项目目前在维护模式，在JIRA上一些tickets都已经关掉了，不再fix。GraphFrames库提供了Python结合，你可以选它作为一个 graph处理的办法。</p>
<h3 id="DataSets"><a href="#DataSets" class="headerlink" title="DataSets"></a>DataSets</h3><p>主观来说，Python在统计类型的DataSets没有什么空间，即使现有的Scala实施过于简单，并且不提供和DataFrame一样的性能优势。</p>
<h2 id="Streaming"><a href="#Streaming" class="headerlink" title="Streaming"></a>Streaming</h2><p>从我之前说来看，我都会强烈推荐Scala，而不是Python。未来如果PySpark在structured streams上得到支持的话，可能会改变，但是现在来说，还是为时过早。再者，基于RDD的API<br>在Databricks文档里（ 2017-03-03）已经被定为“streaming遗产”，所以，可以期许下在未来进行统一。</p>
<h2 id="非性能考虑"><a href="#非性能考虑" class="headerlink" title="非性能考虑"></a>非性能考虑</h2><h3 id="功能平等"><a href="#功能平等" class="headerlink" title="功能平等"></a>功能平等</h3><p>不是所有的Spark特性、功能在PySpark上都有。需要确保下你需要的那部分已经实现了，并且尝试了解可能的限制。</p>
<p>有点特别重要的是，当你使用MLlib，和其它类似的混合Context（比如在task里调用Java/Scala 方法)。公平来讲，一些PySpark API，比如mllib.linalg，提供比Scala更加复杂的方法。</p>
<h3 id="API设计"><a href="#API设计" class="headerlink" title="API设计"></a>API设计</h3><p>PySpark API的设计和Scala类似，并不那么Pythonic。 这意味着很容易地可以在两种语言之间切换，但同时，Python可能会变得难以理解</p>
<h3 id="架构的复杂性"><a href="#架构的复杂性" class="headerlink" title="架构的复杂性"></a>架构的复杂性</h3><p>PySpark数据处理流程相当复杂比起纯粹的JVM执行来说。PySpark程序非常难去debug或找出出错原因。此外，至少在基本对Scala和JVM总体的理解上是必须要有的。</p>
<h3 id="Spark2-0-及以后"><a href="#Spark2-0-及以后" class="headerlink" title="Spark2.0 及以后"></a>Spark2.0 及以后</h3><p>随着RDD API被冻结，正在进行迁移到DataSet API对Python用户同时带来机会和挑战。尽管高级层次部分的API用Python包装会容易很多，但更高级的直接被使用的可能性很低。</p>
<p>此外，在SQL的世界里，原生Python function依然是二等公民。但值得期待的是，在将来伴随着Apache Arrow序列化，Python的地位会提高（目前侧重仍然是数据收集，UDF序列化以及反序列化仍然是个长远的目标）。<br>对于那些Python代码依赖性很强的项目，还可以选择纯Python的框架，比如Dask或Ray等等，也挺有意思的。</p>
<h2 id="不必和其它比较"><a href="#不必和其它比较" class="headerlink" title="不必和其它比较"></a>不必和其它比较</h2><p>Spark DataFrame（SQL，DataSets）API提供了一个在PySpark程序里整合Java/Scala代码优雅的方式。<br>你可以用<code>DataFrames</code> 去输送数据给原生JVM代码，然后返回结果。<br>我已经在其它地方解释了我的看法 <a href="https://stackoverflow.com/q/31684842" target="_blank" rel="external">这里</a> ，你可以在这 <a href="https://stackoverflow.com/q/36023860/1560062" target="_blank" rel="external">https://stackoverflow.com/q/36023860/1560062</a> 找到一个Python-Scala的工作案例 。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>pyspark</tag>
      </tags>
  </entry>
  <entry>
    <title>自建家用服务器集群，打造一个私有云</title>
    <url>/2019/private_IDC.html</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="我的战神"><a href="#我的战神" class="headerlink" title="我的战神"></a>我的战神</h3><p>还记得17年的时候，那时深度学习刚火起来，幼稚的我居然打算往这个领域去研究（根本没有意识到这领域需要的数学功底要多深）。俗话说，工欲善其事，必先利其器，于是乎，<br>一股脑地跑到广州的岗顶那买了台神舟战神 超级本（GTX-1060）。当时，经常都听到东哥在提到GPU，CUDA，GTX1080提到这些关键词。哈，为什当时会选择买一台笔记本，而不是台式呢？</p>
<p>其实，当时页考虑过买台式的，但是，考虑到当时在外面租房子，台式机搬家不方便。于是就想搞一台显卡性能好的笔记本，但是，完全不知道台式机的1060和笔记本的1060完全不是一个概念，<br>虽然都要同一个型号。至于为什么要买神舟，而不是华硕，或者其它，很简单，—-因为它性价比高（其实不高，买回来不久就感觉上当了，买回来玩FIFA OL会经常卡卡的，小毛病很多，而且<br>偶尔还会蓝屏。。）</p>
<p>那时候7400左右的价格在岗顶那买回来，真正用它就用了一年的时间吧。后来，在今年5月份，我以3000的价格在闲鱼上低价卖了。<br><img src="/images/2019/private_IDC/IMG_3434.jpg" alt="Sample Image Added via Markdown"></p>
<h3 id="第一台组装台式机"><a href="#第一台组装台式机" class="headerlink" title="第一台组装台式机"></a>第一台组装台式机</h3><p>前面算是有点扯远了，不过也算是记录一下我早期的一些想法吧，正是有了曾经的各种因素，导致了我今天打算搭建私有集群的IDEA。在尝到了神舟笔记本的坑后，我意识到，其实买一台台式机的必要性。<br>台式机在散热，硬件稳定性上都明显强于笔记本，而且，性价比也挺高的。对了，对于购买台式机的一个很重要的因素是，17年到18年这段时间里，我在云服务上花了不少钱，刚开始在阿里云，<br>然后在Google Cloud上。陆陆续续大概花了有六七千块钱吧，也将近买一台机器的钱了。<br>18年年初，我跑去深圳华强北，组装了自己第一台组装机（华硕主板，16G内存，1T硬盘），不加显示器，一共5000多人民币。背回来立马装了ubuntu系统，顿时感到有了自己的服务器，那感觉就像<br>在外面租房久了，终于买了属于自己的房子，再也不要担心服务器没续费，登陆不上；再也不用担心资源不够用，要去计算云服务的成本和开支了。自己的机子想怎么扩展都行。</p>
<p>其实刚开始，由于技术原因，我并没有把它当做一台服务器在用，只是把它当成一台PC安装了一些自己熟悉的开发环境和软件。在配置了Ngrok后，基本上把它当成一台云服务器了，24小时运行着。<br>关于ngrok的配置我前面有一篇专门的文章有讲得很详细。</p>
<p>最近发现我的台式机风扇声音有点大（比起Dell T30来说），从长远来看，毕竟是要7*24 running的，考虑到耗电量和稳定性，还是服务器好啊。我打算将再购置<br>2台服务器。其实，服务器完全可以当PC来用，但PC不能当服务器来用。</p>
<h3 id="第一台真正的服务器"><a href="#第一台真正的服务器" class="headerlink" title="第一台真正的服务器"></a>第一台真正的服务器</h3><p>虽然我的台式机现在配置有32G内存，在安装了一些软件（Cloudera Manager， Elasticsearch）后，内存就不太够用了。我是做数据这一块的，平时也要用分布式的框架，像Hadoop生态圈的<br>所以组件。我非常需要有一个真正的分布式平台去练手，于是就有了我的第一台服务器————戴尔T30<br><img src="/images/2019/private_IDC/t30.jpg" alt="Sample Image Added via Markdown"></p>
<p>也是在华强北的赛格广场，3200买的。配置是E3-1225，16G DDR4，2T*2 带阵列。T30 我没理解错的话应该是戴尔服务器的入门版。我对服务器不太理解，刚入门，挺好奇的，只知道一般PC的CPU<br>是分I7，I5等等，服务器的CPU是E开头的。然后，服务器用的内存和PC用的也不一样，是不能共用的。</p>
<p>购机的时候和那些卖电脑的老板闲聊也学到了好多专业知识。别看他们穿着没有写字楼的白领干净，没有程序员工资高，<br>但他们对计算机硬件知识还是很有经验的。比如，老板跟我说，你知道磁盘是怎么运行的吗？他说并不是说高转速的磁盘就一定好，如果突然断电的话，<br>转速高的磁盘容易被划伤。并介绍了下SAS，SATA，SSD之间的一些区别。他说SAS这种算是比较老的格式了。现在基本都用SATA。</p>
<p>那天我看了两种服务器，一种是机架式服务器，另一种是踏实。机架式的风扇声比较大（虽然单路的会比双路的小一些，但比起塔式服务器会大挺多的。）</p>
<h4 id="坑人的阵列卡"><a href="#坑人的阵列卡" class="headerlink" title="坑人的阵列卡"></a>坑人的阵列卡</h4><p>作为一个服务器装机菜鸟，我从网上下好的ubuntu服务器放在U盘里，然后改好U盘启动引导，开始安装。前几步没有啥问题，但到了选择安装磁盘路径的时候，找不到任何磁盘，<br>从而无法点击下一步继续。后来在老板的指导下，我才得知，原来是因为有阵列卡，BIOS没有设置对还是啥的，所以才读不到磁盘。于是，我在老板的远程指导下成功地拆卸掉阵列卡，<br>并重新接线好两块磁盘。之后，便正常地顺利正常地安装好ubuntu server了。</p>
<p><img src="/images/2019/private_IDC/raid.jpg" alt="Sample Image Added via Markdown"></p>
<h4 id="交换机"><a href="#交换机" class="headerlink" title="交换机"></a>交换机</h4><p>哈哈，没钱买千兆网卡，千兆交换机，只能用路由器充当了。</p>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>玩硬件组装也是个超级无底洞，各个部件都可以扩展起来，内存，硬盘，CPU，可以水平扩展，也可以横向扩展。以后准备好好赚钱升级设备吧。如果比较缺钱的话可以上淘宝或闲鱼买点二手的配件，<br>比如内存条，硬盘这些。最好是可以和个人卖家交易，因为个人的话经常可以买到一些物美价廉的二手物品。另外，V2EX网站也有专门二手转让的版块，可以去看看。</p>
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title>SQLAchemy的多进程实践</title>
    <url>/2019/sqlalchemy_with_multiprocess.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近上头说我写的ETL工具从MySQL导出为CSV的速度太慢了，需要性能优化。的确，有部分数据因为在MySQL里面做了<br>分表分库，而我目前的导出实现是一个一个对小表进行导出。其实，这一步完全是可以并发多个表同时导出的。<br>理论上如果网络IO没有瓶颈的话，多个表同时从MySQL里dump可以大大提升效率。</p>
<h2 id="正题"><a href="#正题" class="headerlink" title="正题"></a>正题</h2><h3 id="查阅文档"><a href="#查阅文档" class="headerlink" title="查阅文档"></a>查阅文档</h3><p>为了实现并发地使用sqlalchemy我花了不少时间在网上找资料，也在StackOverflow寻求帮助。<br>其实刚开始我是想用threading结合SQLAlchemy来实现多线程导出的。去SQLAlchemy官网一看，没有找到相关的实现文档，<br>却找到了multiprocessing与SQLAlchemy的实践:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Using Connection Pools with Multiprocessing</div><div class="line">It’s critical that when using a connection pool, and by extension when using an Engine created via create_engine(), that the pooled connections are not shared to a forked process. TCP connections are represented as file descriptors, which usually work across process boundaries, meaning this will cause concurrent access to the file descriptor on behalf of two or more entirely independent Python interpreter states.</div><div class="line"></div><div class="line">There are two approaches to dealing with this.</div><div class="line"></div><div class="line">The first is, either create a new Engine within the child process, or upon an existing Engine, call Engine.dispose() before the child process uses any connections. This will remove all existing connections from the pool so that it makes all new ones. Below is a simple version using multiprocessing.Process, but this idea should be adapted to the style of forking in use:</div></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line">engine = create_engine(<span class="string">"..."</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_in_process</span><span class="params">()</span>:</span></div><div class="line">  engine.dispose()</div><div class="line"></div><div class="line">  <span class="keyword">with</span> engine.connect() <span class="keyword">as</span> conn:</div><div class="line">      conn.execute(<span class="string">"..."</span>)</div><div class="line"></div><div class="line">p = Process(target=run_in_process)</div></pre></td></tr></table></figure>
<h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><p>于是，我就打算用multiprocessing试试了。multiprocessing有个比较坑爹的地方就是它会用pickle来序列化一些数据，<br>因为要把数据复制到新spawn出来的进程。</p>
<h4 id="关于pickle"><a href="#关于pickle" class="headerlink" title="关于pickle"></a>关于pickle</h4><p>首先我们要了解下pickle，虽然pickle挺坑的。哪些内容可以pickle呢，官网上说：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">What can be pickled and unpickled?</div><div class="line">The following types can be pickled:</div><div class="line"></div><div class="line">None, True, and False</div><div class="line"></div><div class="line">integers, floating point numbers, complex numbers</div><div class="line"></div><div class="line">strings, bytes, bytearrays</div><div class="line"></div><div class="line">tuples, lists, sets, and dictionaries containing only picklable objects</div><div class="line"></div><div class="line">functions defined at the top level of a module (using def, not lambda)</div><div class="line"></div><div class="line">built-in functions defined at the top level of a module</div><div class="line"></div><div class="line">classes that are defined at the top level of a module</div><div class="line"></div><div class="line">instances of such classes whose __dict__ or the result of calling __getstate__() is picklable (see section Pickling Class Instances for details).</div></pre></td></tr></table></figure></p>
<p>比如, 我们举个简单的例子：<br><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> pymysql</div><div class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</div><div class="line"><span class="comment">#engine = create_engine(f'mysql+pymysql://root:ignorance@localhost:3306/', server_side_cursors=True, pool_size=20)</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Client</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment">#self.engine = create_engine(f'mysql+pymysql://root:ignorance@localhost:3306/', server_side_cursors=True, pool_size=20)</span></div><div class="line">        self.pool = Pool(<span class="number">5</span>)</div><div class="line">        self.connection = pymysql.connect(host=<span class="string">'localhost'</span>,</div><div class="line">                             user=<span class="string">'root'</span>,</div><div class="line">                             password=<span class="string">'ignorance'</span>,</div><div class="line">                             port=<span class="number">3306</span>,</div><div class="line">                             charset=<span class="string">'utf8mb4'</span>,</div><div class="line">                             cursorclass=pymysql.cursors.DictCursor)</div><div class="line">        self.engine = create_engine(f<span class="string">'mysql+pymysql://root:ignorance@localhost:3306/'</span>, server_side_cursors=<span class="keyword">True</span>, pool_size=<span class="number">20</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run_in_process</span><span class="params">(self, x, y=<span class="string">'hehe'</span>)</span>:</span></div><div class="line">        print(<span class="string">'run in process'</span>)</div><div class="line">        self.engine.dispose()</div><div class="line">        conn = self.engine.connect()</div><div class="line">        res = conn.execute(<span class="string">'select count(1) from zhihu.zhihu_answer_meta limit 10'</span>)</div><div class="line">        print(res.fetchall())</div><div class="line">        time.sleep(<span class="number">5</span>)</div><div class="line">        print(conn)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></div><div class="line">        x = <span class="string">'x'</span> </div><div class="line">        res_list = []</div><div class="line">        res = self.pool.apply_async(self.run_in_process, args=(x,), kwds=&#123;<span class="string">'y'</span>:<span class="string">'shit'</span>&#125;)</div><div class="line">        res_list.append(res)</div><div class="line">        <span class="comment">#[each.get(3) for each in res_list]</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run_pool</span><span class="params">(self)</span>:</span></div><div class="line">        self.pool.close()</div><div class="line">        self.pool.join()</div><div class="line"></div><div class="line">client = Client()</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    client.run()</div><div class="line"></div><div class="line">client.run_pool()</div></pre></td></tr></table></figure></p>
<p>这段代码我的目的是想用多个进程多个connector连接到MySQL，然后各自进程同时去查询，这样便可以实现并行处理，<br>提升效率。然而，实际上这段代码不能正常地执行，而且没有任何报错提示。这是为何呢？</p>
<p>有些时候可能会看到这样的报错:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">TypeError: can&apos;t pickle _thread._local objects</div></pre></td></tr></table></figure></p>
<p><a href="https://stackoverflow.com/questions/58022926/cant-pickle-the-sqlalchemy-engine-in-the-class" target="_blank" rel="external">https://stackoverflow.com/questions/58022926/cant-pickle-the-sqlalchemy-engine-in-the-class</a></p>
<p>下面我把上面的代码修改下：<br><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> pymysql</div><div class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Client</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.pool = Pool(<span class="number">5</span>)</div><div class="line">        self.connection = pymysql.connect(host=<span class="string">'localhost'</span>,</div><div class="line">                             user=<span class="string">'root'</span>,</div><div class="line">                             password=<span class="string">'ignorance'</span>,</div><div class="line">                             port=<span class="number">3306</span>,</div><div class="line">                             charset=<span class="string">'utf8mb4'</span>,</div><div class="line">                             cursorclass=pymysql.cursors.DictCursor)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run_in_process</span><span class="params">(self, x, y=<span class="string">'hehe'</span>)</span>:</span></div><div class="line">        <span class="comment"># 这里我们把engine改为了方法的内部变量，而不是实例变量。这样就不报错了</span></div><div class="line">        engine = create_engine(f<span class="string">'mysql+pymysql://root:ignorance@localhost:3306/'</span>, server_side_cursors=<span class="keyword">True</span>, pool_size=<span class="number">20</span>)</div><div class="line">        engine.dispose()</div><div class="line">        conn = engine.connect()</div><div class="line">        res = conn.execute(<span class="string">'select count(1) from zhihu.zhihu_answer_meta limit 10'</span>)</div><div class="line">        print(res.fetchall())</div><div class="line">        time.sleep(<span class="number">5</span>)</div><div class="line">        print(conn)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></div><div class="line">        x = <span class="string">'x'</span></div><div class="line">        res_list = []</div><div class="line">        res = self.pool.apply_async(self.run_in_process, args=(x,), kwds=&#123;<span class="string">'y'</span>:<span class="string">'shit'</span>&#125;)</div><div class="line">        res_list.append(res)</div><div class="line">        <span class="comment">#[each.get(3) for each in res_list]</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run_pool</span><span class="params">(self)</span>:</span></div><div class="line">        self.pool.close()</div><div class="line">        self.pool.join()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getstate__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># 这里我增加了这个魔术方法</span></div><div class="line">        self_dict = self.__dict__.copy()</div><div class="line">        <span class="keyword">del</span> self_dict[<span class="string">'pool'</span>]</div><div class="line">        <span class="keyword">del</span> self_dict[<span class="string">'connection'</span>]  <span class="comment"># if conenction is not deleted, it would be silent without any errors</span></div><div class="line">        <span class="keyword">return</span> self_dict</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span><span class="params">(self, state)</span>:</span></div><div class="line">        <span class="comment"># 这里我增加了这个魔术方法</span></div><div class="line">        self.__dict__.update(state)</div><div class="line"></div><div class="line"></div><div class="line">client = Client()</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    client.run()</div><div class="line"></div><div class="line">client.run_pool()</div></pre></td></tr></table></figure></p>
<h4 id="我们看到，这段代码有一些变化："><a href="#我们看到，这段代码有一些变化：" class="headerlink" title="我们看到，这段代码有一些变化："></a>我们看到，这段代码有一些变化：</h4><blockquote>
<p>增加了<strong>getstate</strong>， <strong>getstate</strong>这两个魔术方法。为什么要加呢？<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">首先，我们 self.pool.apply_async(self.run_in_process) 可以看出apply_async调用的是实例的方法，所以Python需要pickle整个Client对象，</div><div class="line">包括它的所有实例变量。在第一个代码片段中我们可以看到它的实例变量有pool, connection, engine等等。然而这些对象都是不可以被pickle的，</div><div class="line">所以代码执行的时候会有问题。所以就有了__getstate__， __getstate__ 这两个东西。</div></pre></td></tr></table></figure></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">__getstate__ 总是在对象pickle之前调用， 同时，它让你可以指定你想pickle的对象状态。然后unpickle的时候，</div><div class="line">如果__setstate__被实现了，则__setstate__(state)会被调用。如果没有被实现的话，__getstate__返回的dict将会被</div><div class="line">unpickle的实例使用。在上面例子中，__setstate__ 其实没有实际效果，不写也可以。</div></pre></td></tr></table></figure>
<h3 id="multiprocessing的一些细节"><a href="#multiprocessing的一些细节" class="headerlink" title="multiprocessing的一些细节"></a>multiprocessing的一些细节</h3><h4 id="传多个参数在target函数"><a href="#传多个参数在target函数" class="headerlink" title="传多个参数在target函数"></a>传多个参数在target函数</h4><p>有时候当我们想在apply_async 的 target函数上传指定参数的时候, 可以用kwds传进去,比如：<br><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_in_process</span><span class="params">(self, x, y=<span class="string">'hehe'</span>)</span>:</span></div><div class="line">    <span class="comment"># 这里我们把engine改为了方法的内部变量，而不是实例变量。</span></div><div class="line">    print(x)</div><div class="line">    print(y)</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></div><div class="line">    x = <span class="string">'x'</span></div><div class="line">    res = self.pool.apply_async(self.run_in_process, args=(x,), kwds=&#123;<span class="string">'y'</span>:<span class="string">'shit'</span>&#125;)</div></pre></td></tr></table></figure></p>
<h4 id="map和apply的区别"><a href="#map和apply的区别" class="headerlink" title="map和apply的区别"></a>map和apply的区别</h4><p>map执行的顺序是和参数的顺序一致的，apply_async的顺序是随机的<br>map一般用来切分参数执行在同一个方法上。而apply_async可以调用不同的方法。</p>
<p>此外,<br>map相当于 map_async().get()<br>apply相当于 apply_async().get()</p>
<h5 id="子进程报错没有提示"><a href="#子进程报错没有提示" class="headerlink" title="子进程报错没有提示"></a>子进程报错没有提示</h5><p>有个很坑的地方，有些时候逻辑明明没有执行，但又没有任何报错！<br><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment">#@staticmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">work</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"error"</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    pool = mp.Pool()</div><div class="line">    foo = Foo()</div><div class="line">    res = pool.apply_async(foo.work)</div><div class="line">    pool.close()</div><div class="line">    pool.join()</div><div class="line">    <span class="comment">#print(res.get())</span></div></pre></td></tr></table></figure></p>
<p> 比如这个，如果我不get() 一下 apply_async后的返回的话，看不到任何报错信息，解决办法就是用get()后才<br> 能得知报错的信息。</p>
<h4 id="加装饰器后报错"><a href="#加装饰器后报错" class="headerlink" title="加装饰器后报错"></a>加装饰器后报错</h4><p> 比如我们在<code>run_in_process</code>方法上了个装饰器，然后就报错了。<br> <figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeit</span><span class="params">(method)</span>:</span></div><div class="line">   <span class="function"><span class="keyword">def</span> <span class="title">timed</span><span class="params">(*args, **kw)</span>:</span></div><div class="line">       ts = time.time()</div><div class="line">       result = method(*args, **kw)</div><div class="line">       te = time.time()</div><div class="line">       <span class="keyword">if</span> <span class="string">'log_time'</span> <span class="keyword">in</span> kw: </div><div class="line">           name = kw.get(<span class="string">'log_name'</span>, method.__name__.upper())</div><div class="line">           kw[<span class="string">'log_time'</span>][name] = int((te - ts) * <span class="number">1000</span>)</div><div class="line">       <span class="keyword">else</span>:</div><div class="line">           print(<span class="string">'%r 执行时长  %2.2f s'</span> % (method.__name__, (te - ts) ))</div><div class="line">       <span class="keyword">return</span> result</div><div class="line">   <span class="keyword">return</span> timed</div><div class="line">   </div><div class="line"><span class="meta">   @timeit</span></div><div class="line">   <span class="function"><span class="keyword">def</span> <span class="title">run_in_process</span><span class="params">(self, x, y=<span class="string">'hehe'</span>)</span>:</span></div><div class="line">       <span class="comment"># 这里我们把engine改为了方法的内部变量，而不是实例变量。</span></div><div class="line">       print(x)</div><div class="line">       print(y)</div><div class="line">       </div><div class="line">   <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></div><div class="line">       x = <span class="string">'x'</span></div><div class="line">       res = self.pool.apply_async(self.run_in_process, args=(x,), kwds=&#123;<span class="string">'y'</span>:<span class="string">'shit'</span>&#125;)</div></pre></td></tr></table></figure></p>
<p>报错说:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Traceback (most recent call last):</div><div class="line">  File &quot;/data/software/miniconda3/lib/python3.7/multiprocessing/process.py&quot;, line 297, in _bootstrap</div><div class="line">    self.run()</div><div class="line">  File &quot;/data/software/miniconda3/lib/python3.7/multiprocessing/process.py&quot;, line 99, in run</div><div class="line">    self._target(*self._args, **self._kwargs)</div><div class="line">  File &quot;/data/software/miniconda3/lib/python3.7/multiprocessing/pool.py&quot;, line 110, in worker</div><div class="line">    task = get()</div><div class="line">  File &quot;/data/software/miniconda3/lib/python3.7/multiprocessing/queues.py&quot;, line 354, in get</div><div class="line">    return _ForkingPickler.loads(res)</div><div class="line">AttributeError: &apos;Client&apos; object has no attribute &apos;timed&apos;</div></pre></td></tr></table></figure></p>
<p>解决办法：<br>比较麻烦，不能用@了。可以以这种办法代替装饰器：<br><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">decorate_func</span><span class="params">(f)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_decorate_func</span><span class="params">(*args, **kwargs)</span>:</span></div><div class="line">        <span class="keyword">print</span> <span class="string">"I'm decorating"</span></div><div class="line">        <span class="keyword">return</span> f(*args, **kwargs)</div><div class="line">    <span class="keyword">return</span> _decorate_func</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">actual_func</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> x ** <span class="number">2</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">wrapped_func</span><span class="params">(*args, **kwargs)</span>:</span></div><div class="line">    <span class="keyword">return</span> decorate_func(actual_func)(*args, **kwargs)</div><div class="line"></div><div class="line">my_swimming_pool = Pool()</div><div class="line">result = my_swimming_pool.apply_async(wrapped_func,(<span class="number">2</span>,))</div><div class="line"><span class="keyword">print</span> result.get()</div></pre></td></tr></table></figure></p>
<h4 id="关于处理ctrl-c-的报错"><a href="#关于处理ctrl-c-的报错" class="headerlink" title="关于处理ctrl-c 的报错:"></a>关于处理ctrl-c 的报错:</h4><p>有时候我们用multiprocessing处理一些任务，当我们想终止任务时候，用Ctrl+C 然后会看到一堆的报错，有时候还得连续按很多CTRL+C完全终止掉。<br>下面是最佳解决方案：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">解决办法是 首先防止子进程接收KeyboardInterrupt，然后完全交给父进程catch interrupt然后清洗进程池。通过这种方法可以</div><div class="line">避免在子进程写处理异常逻辑，并且防止了由idle workers 生成的无止尽的Error。</div></pre></td></tr></table></figure></p>
<p>解决方案代码:<br><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">import</span> multiprocessing</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> signal</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_worker</span><span class="params">()</span>:</span></div><div class="line">    signal.signal(signal.SIGINT, signal.SIG_IGN)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_worker</span><span class="params">()</span>:</span></div><div class="line">    time.sleep(<span class="number">15</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"Initializng 5 workers"</span></div><div class="line">    pool = multiprocessing.Pool(<span class="number">5</span>, init_worker)</div><div class="line"></div><div class="line">    <span class="keyword">print</span> <span class="string">"Starting 3 jobs of 15 seconds each"</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">        pool.apply_async(run_worker)</div><div class="line"></div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        print(<span class="string">"Waiting 10 seconds"</span>)</div><div class="line">        time.sleep(<span class="number">10</span>)</div><div class="line"></div><div class="line">    <span class="keyword">except</span> KeyboardInterrupt:</div><div class="line">        print(<span class="string">"Caught KeyboardInterrupt, terminating workers"</span>)</div><div class="line">        pool.terminate()</div><div class="line">        pool.join()</div><div class="line"></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">print</span> <span class="string">"Quitting normally"</span></div><div class="line">        pool.close()</div><div class="line">        pool.join()</div></pre></td></tr></table></figure></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://docs.python.org/3/library/pickle.html" target="_blank" rel="external">https://docs.python.org/3/library/pickle.html</a><br><a href="https://stackoverflow.com/questions/25382455/python-notimplementederror-pool-objects-cannot-be-passed-between-processes/25385582#25385582" target="_blank" rel="external">https://stackoverflow.com/questions/25382455/python-notimplementederror-pool-objects-cannot-be-passed-between-processes/25385582#25385582</a></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive 使用orc进行事务操作(update)</title>
    <url>/2019/hive_orc.html</url>
    <content><![CDATA[<h2 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h2><p>需求方需要用Hive来进行一些update操作。以往一般用Parquet这种格式作为Hive的存储格式，查文档得知Parquet不支持<br>update，orc格式可以支持update。</p>
<h2 id="开始试验"><a href="#开始试验" class="headerlink" title="开始试验"></a>开始试验</h2><h3 id="创建测试数据"><a href="#创建测试数据" class="headerlink" title="创建测试数据"></a>创建测试数据</h3><p>首先我们在Hive上简单地创建一个表作为测试：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">create table test_format (</div><div class="line">    car_name string,</div><div class="line">    series string,</div><div class="line">    e_series string,</div><div class="line">    model string,</div><div class="line">    variant string</div><div class="line">) clustered by (car_name) into 5 buckets stored as orc TBLPROPERTIES(&apos;transactional&apos;=&apos;true&apos;);</div><div class="line"></div><div class="line">insert into test_format values</div><div class="line">(&apos;2017款宝马3系320i M运动型&apos;,&apos;3&apos;,&apos;F30&apos;,&apos;320i&apos;,&apos;320i M Sport Package&apos;),</div><div class="line">(&apos;2018款宝马3系320i M运动套装&apos;,&apos;3&apos;,&apos;F30&apos;,&apos;320i&apos;,&apos;320i M Sport Package&apos;),</div><div class="line">(&apos;2018款宝马3系320i M运动曜夜版&apos;,&apos;3&apos;,&apos;F30&apos;,&apos;320i&apos;,&apos;320i M Sport Dark Edition&apos;),</div><div class="line">(&apos;2019款宝马3系320i M 运动套装&apos;,&apos;3&apos;,&apos;F30&apos;,&apos;320i&apos;,&apos;320i M Sport Package&apos;);</div></pre></td></tr></table></figure></p>
<p>刚开始会有报错：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">This command is not allowed on an ACID table auto_projects.test_format with a non-ACID transaction manager. Failed command: insert into test_format value</div></pre></td></tr></table></figure></p>
<p>这是因为有些配置文件需要修改才能支持transaction操作。</p>
<p>在hive-site.xml里面添加如下配置：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.optimize.sort.dynamic.partition&lt;/name&gt;</div><div class="line">    &lt;value&gt;false&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.support.concurrency&lt;/name&gt;</div><div class="line">    &lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.enforce.bucketing&lt;/name&gt;</div><div class="line">    &lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.exec.dynamic.partition.mode&lt;/name&gt;</div><div class="line">    &lt;value&gt;nonstrict&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.txn.manager&lt;/name&gt;</div><div class="line">    &lt;value&gt;org.apache.hadoop.hive.ql.lockmgr.DbTxnManager&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.compactor.initiator.on&lt;/name&gt;</div><div class="line">    &lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.compactor.worker.threads&lt;/name&gt;</div><div class="line">    &lt;value&gt;1&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.in.test&lt;/name&gt;</div><div class="line">    &lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure></p>
<p>Cloudera Manager的话则可以在WebUI上完成。</p>
<ol>
<li>在WebUI上先点击Hive集群。</li>
<li>点击配置，然后找到一个叫hive-site.xml 的 HiveServer2 高级配置代码段（安全阀）的tag中</li>
<li>将上述配置内容复制在文本框内。</li>
<li>重启集群</li>
</ol>
<h3 id="测试更新数据"><a href="#测试更新数据" class="headerlink" title="测试更新数据"></a>测试更新数据</h3><p>测试更新和删除部分数据<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">update test_format set model=&apos;test&apos; where series=&apos;3&apos;;</div><div class="line"></div><div class="line">delete  from test_format where model=&apos;test&apos;;</div></pre></td></tr></table></figure></p>
<p>这里需要注意的一个地方就是 要更新的字段不能是设置为bucket的那个字段，不然会报错：</p>
<p>成功执行！</p>
<h3 id="踩的一些坑"><a href="#踩的一些坑" class="headerlink" title="踩的一些坑"></a>踩的一些坑</h3><p>因为我是用的单节点部署的CDH集群。刚开始，在Hive上执行select count(*) ; 和 insert操作时候，没有任何报错，<br>但会一直卡在那里。后来查看到cloudera community说单节点要改个mapred-site.xml的一个参数：<br>mapreduce.framework.name</p>
<p>默认是用的yarn，单机的话要改为local，然后重启集群insert 和 select count(*) 就不会卡住了,<br>CDH也是在WebUI上的Yarn集群上的配置上修改。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.cnblogs.com/qifengle-2446/p/6424620.html" target="_blank" rel="external">https://www.cnblogs.com/qifengle-2446/p/6424620.html</a><br><a href="https://community.cloudera.com/t5/Batch-SQL-Apache-Hive/Hive-server-query-hanging-when-issue-ing-select-count-on-CLI/m-p/67119#M2662" target="_blank" rel="external">https://community.cloudera.com/t5/Batch-SQL-Apache-Hive/Hive-server-query-hanging-when-issue-ing-select-count-on-CLI/m-p/67119#M2662</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>经典的大数据面试题</title>
    <url>/2019/typical_bigdata_interview_question.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近面试大数据工程师岗位，同一个问题被连续问了两次。题目大概是这样的：<br>如果你有一台机器，内存是有限的，要你统计一个很大的日志文件里的数据，比如统计UV top-N；<br>另外一个公司是这么问：<br>如果你有一台机器，内存有限，要你对某个指标做一个全局排序。比如对单词频次进行全局排序。</p>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>第一种问题是比较简单的。这种问题一般都问在某个条件下如何在大数据中求出top-N。这种问题的解决方案如下步骤：</p>
<ol>
<li>分而治之/hash映射。</li>
<li>hash统计。</li>
<li>堆/快速/归并排序；</li>
</ol>
<p>首先，为什么药hash映射呢？<br>因为，我们的目的要将一个大文件切割成N个小文件，去进行分而治之。而hash 加 取模 这种方式可以帮我们把数据<br>均匀随机地分布在N个地方。更重要的是，hash可以让我们把相同的东西放在同一个地方！</p>
<p>其实这种做法在有些地方出现过，比如Hive的bucket实现原理就是这个中道理。用hash加取模的方式保证同样的值被<br>分配到同一个地方。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://blog.csdn.net/v_july_v/article/details/7382693" target="_blank" rel="external">https://blog.csdn.net/v_july_v/article/details/7382693</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive bucket和partition的区别</title>
    <url>/2019/hive_bucket_partition.html</url>
    <content><![CDATA[<h2 id="Hive-partition和bucket的区别"><a href="#Hive-partition和bucket的区别" class="headerlink" title="Hive partition和bucket的区别"></a>Hive partition和bucket的区别</h2><ul>
<li>翻译文</li>
</ul>
<p>为了更好地阐述partition和bucket的区别，我们先看看数据是怎么保存在Hive上面的。比如，你有一个表：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">CREATE TABLE mytable ( </div><div class="line">         name string,</div><div class="line">         city string,</div><div class="line">         employee_id int ) </div><div class="line">PARTITIONED BY (year STRING, month STRING, day STRING) </div><div class="line">CLUSTERED BY (employee_id) INTO 256 BUCKETS</div></pre></td></tr></table></figure></p>
<p>然后，Hive会将数据保存为如下的层级：<br>/user/hive/warehouse/mytable/y=2015/m=12/d=02<br>所以，用partition的时候必须小心，因为如果你用employee_id来partition的话，如果有上百万个employee，那么你会看到有上百万个目录被创建在你的系统上。<br>“cardinality” 这个术语被用来表示不同字段值的数量。比如，你有country这个字段，世界上会有300个国家，所以这里的”cardinality”是300这样。<br>对于像’timestamp_ms’的字段，它的’cardinality’会有几十亿。<br>总的来说，当我们用partition的时候，不要用在cardinality很高的字段上。因为它会导致生成太多的目录。</p>
<p>说说bucket了，在指定了bucket数后，会使得文件的数量固定。Hive会做的是计算字段的hash值，然后分发一个记录给那个bucket。<br>但是比如说你用总共256个bucket在一个较低的cardinality的字段上会发生什么呢？（比如，美国的州，只有50个）<br>只有50个bucket有数据，其它206个bucket是没有数据的。</p>
<p>有人已经提到partition可以极大地减少查询数据的量。<br>那在我这个例子中，如果你想在某个日期上进一步查询，对 yearn/month/day的partition会大大地减少IO。<br>我觉得有人已经提到bucket可以加速和其它恰好在同一个bucket的表的join操作。那么在我的案例中，如果你正在通过employee_id来join两个表，<br>Hive能够在每个每个bucket地内部进行join（如果它们已经通过employee_id排序好的话，效果会更好！）</p>
<p>总结，<br>bucket在字段有很高的cardinality，和在数据在不同bucket中均匀地分布的时候，会表现出优越性。<br>partition在cardinality partition的字段不是很多的时候，会表现出优越性。</p>
<p>另外，你可以按顺序同时partiton多个字段，比如（yearn/month/day），但bucket只能取一个字段。</p>
<ul>
<li>原文<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">There are a few details missing from the previous explanations. To better understand how partitioning and bucketing works,</div><div class="line"> you should look at how data is stored in hive. Let&apos;s say you have a table</div><div class="line"></div><div class="line">CREATE TABLE mytable ( </div><div class="line">         name string,</div><div class="line">         city string,</div><div class="line">         employee_id int ) </div><div class="line">PARTITIONED BY (year STRING, month STRING, day STRING) </div><div class="line">CLUSTERED BY (employee_id) INTO 256 BUCKETS</div><div class="line">then hive will store data in a directory hierarchy like</div><div class="line"></div><div class="line">/user/hive/warehouse/mytable/y=2015/m=12/d=02</div><div class="line">So, you have to be careful when partitioning, because if you for instance partition by employee_id and you have millions of employees, </div><div class="line">you&apos;ll end up having millions of directories in your file system. The term &apos;cardinality&apos; refers to the number of possible value a field can have. </div><div class="line">For instance, if you have a &apos;country&apos; field, the countries in the world are about 300, so cardinality would be ~300. </div><div class="line">For a field like &apos;timestamp_ms&apos;, which changes every millisecond, cardinality can be billions. </div><div class="line">In general, when choosing a field for partitioning, it should not have a high cardinality, because you&apos;ll end up with way too many directories in your file system.</div><div class="line"></div><div class="line">Clustering aka bucketing on the other hand, will result with a fixed number of files, since you do specify the number of buckets. </div><div class="line">What hive will do is to take the field, calculate a hash and assign a record to that bucket. </div><div class="line">But what happens if you use let&apos;s say 256 buckets and the field you&apos;re bucketing on has a low cardinality (for instance, it&apos;s a US state, so can be only 50 different values) ? </div><div class="line">You&apos;ll have 50 buckets with data, </div><div class="line">and 206 buckets with no data.</div><div class="line"></div><div class="line">Someone already mentioned how partitions can dramatically cut the amount of data you&apos;re querying. </div><div class="line">So in my example table, if you want to query only from a certain date forward, the partitioning by year/month/day is going to dramatically cut the amount of IO. </div><div class="line">I think that somebody also mentioned how bucketing can speed up joins with other tables that have exactly the same bucketing, </div><div class="line">so in my example, if you&apos;re joining two tables on the same employee_id, </div><div class="line">hive can do the join bucket by bucket (even better if they&apos;re already sorted by employee_id since it&apos;s going to mergesort parts that are already sorted, </div><div class="line">which works in linear time aka O(n) ).</div><div class="line"></div><div class="line">So, bucketing works well when the field has high cardinality and data is evenly distributed among buckets. </div><div class="line">Partitioning works best when the cardinality of the partitioning field is not too high.</div><div class="line"></div><div class="line">Also, you can partition on multiple fields, with an order (year/month/day is a good example), while you can bucket on only one field.</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://stackoverflow.com/questions/19128940/what-is-the-difference-between-partitioning-and-bucketing-a-table-in-hive" target="_blank" rel="external">https://stackoverflow.com/questions/19128940/what-is-the-difference-between-partitioning-and-bucketing-a-table-in-hive</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark什么时候用 persist</title>
    <url>/2019/spark_when_to_cache.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在用Spark做一些数据统计，有个任务要跑几个小时，所以需要优化一下。首先想到的是用 persist或者cache(persist的其中一种方式)</p>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="场景一"><a href="#场景一" class="headerlink" title="场景一"></a>场景一</h3><h4 id="首先看看在Stackoverflow的一个回答"><a href="#首先看看在Stackoverflow的一个回答" class="headerlink" title="首先看看在Stackoverflow的一个回答"></a>首先看看在Stackoverflow的一个回答</h4><p>Spark很多惰性算子，它并不会立即执行，persist就是惰性的。只有当被action trigger的时候，叫 lineage的RDD 链才会被执行。<br>如果是线性的lineage的话，persist是没用的。但如果RDD的lineage被分流出去的话，那么persist就有用了。</p>
<p>举个例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># 比如这种情况，有个两个action算子count；</div><div class="line">val positiveWordsCount = wordsRDD.filter(word =&gt; isPositive(word)).count()</div><div class="line">val negativeWordsCount = wordsRDD.filter(word =&gt; isNegative(word)).count()</div><div class="line"></div><div class="line"></div><div class="line"># 可以在公用的RDD上加个cache，让这个flatMap只计算一次</div><div class="line">val textFile = sc.textFile(&quot;/user/emp.txt&quot;)</div><div class="line">val wordsRDD = textFile.flatMap(line =&gt; line.split(&quot;\\W&quot;))</div><div class="line">wordsRDD.cache()</div><div class="line">val positiveWordsCount = wordsRDD.filter(word =&gt; isPositive(word)).count()</div><div class="line">val negativeWordsCount = wordsRDD.filter(word =&gt; isNegative(word)).count()</div></pre></td></tr></table></figure>
<p>另外一个案例：<br><figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_url</span><span class="params">(value)</span>:</span></div><div class="line">    url = <span class="string">'http://domain.com/'</span> + str(value)</div><div class="line">    <span class="keyword">if</span> value == <span class="string">'1134021255504498689'</span>:</div><div class="line">        print(value)  <span class="comment"># 这里print出来可以作为一个标记，知道 这个udf执行了几次</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> url </div><div class="line">    </div><div class="line">url_udf = udf(make_url, StringType())</div><div class="line">df = spark.read.csv(<span class="string">'file:///Users/xiaofeilong/Documents/twitter_tweet_keyword.csv'</span>)</div><div class="line"></div><div class="line">df = df.withColumn(<span class="string">'url'</span>, url_udf(<span class="string">'_c0'</span>))</div><div class="line"><span class="comment"># df.cache() </span></div><div class="line">df1 = df.filter(df._c1.isin([<span class="string">'petcare'</span>]))</div><div class="line">df2 = df.filter(df._c1.isin([<span class="string">'hound'</span>]))</div><div class="line"></div><div class="line">df_union = df1.union(df2)</div><div class="line">df_union = df_union.orderBy(<span class="string">'url'</span>)  <span class="comment"># 这里加order by url的原因是，要让它执行url_udf。不然如果这个字段没用上的话，Spark并不会执行url_udf</span></div><div class="line">print(df_union.count())</div></pre></td></tr></table></figure></p>
<p>上述例子中，如果不加cache的话。log中可以看到打印了两次 1134021255504498689。而且会看到有两条这样的信息：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">INFO FileScanRDD: Reading File path: file:///Users/xiaofeilong/Documents/twitter_tweet_keyword.csv, range: 0-2315578, partition values: [empty row]</div><div class="line">.</div><div class="line">.</div><div class="line">.</div><div class="line">FileScanRDD: Reading File path: file:///Users/xiaofeilong/Documents/twitter_tweet_keyword.csv, range: 0-2315578, partition values: [empty row]</div></pre></td></tr></table></figure></p>
<h4 id="测试结论！"><a href="#测试结论！" class="headerlink" title="测试结论！"></a>测试结论！</h4><ul>
<li><p>说明csv文件被load了两次！！！</p>
</li>
<li><p>如果用了cache的话，只会出现一次！！！</p>
</li>
</ul>
<h3 id="场景二"><a href="#场景二" class="headerlink" title="场景二"></a>场景二</h3><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://stackoverflow.com/questions/28981359/why-do-we-need-to-call-cache-or-persist-on-a-rdd" target="_blank" rel="external">https://stackoverflow.com/questions/28981359/why-do-we-need-to-call-cache-or-persist-on-a-rdd</a><br><a href="https://blog.csdn.net/ainidong2005/article/details/53152605" target="_blank" rel="external">https://blog.csdn.net/ainidong2005/article/details/53152605</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Git删除误add的文件</title>
    <url>/2019/git_filter_branch.html</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h4 id="很多时候，我们经常会不小心把一个不应该add进来的文件-add到-Repo里了。比如："><a href="#很多时候，我们经常会不小心把一个不应该add进来的文件-add到-Repo里了。比如：" class="headerlink" title="很多时候，我们经常会不小心把一个不应该add进来的文件 add到 Repo里了。比如："></a>很多时候，我们经常会不小心把一个不应该add进来的文件 add到 Repo里了。比如：</h4><p>password.txt，<em>.log  </em>.mp4 等等。</p>
<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><h4 id="这种情况下，我们可以选择用-git-filter-branch-来将历史的所有commit都重新过滤一下。"><a href="#这种情况下，我们可以选择用-git-filter-branch-来将历史的所有commit都重新过滤一下。" class="headerlink" title="这种情况下，我们可以选择用 git filter-branch 来将历史的所有commit都重新过滤一下。"></a>这种情况下，我们可以选择用 git filter-branch 来将历史的所有commit都重新过滤一下。</h4><h4 id="查看Git的文档得知，filter-branch是一个核弹级操作"><a href="#查看Git的文档得知，filter-branch是一个核弹级操作" class="headerlink" title="查看Git的文档得知，filter-branch是一个核弹级操作:"></a>查看Git的文档得知，filter-branch是一个核弹级操作:</h4><blockquote>
<p>如果你想用脚本的方式修改大量的提交，还有一个重写历史的选项可以用——例如，全局性地修改电子邮件地址或者将一个文件从所有提交中删除。<br>这个命令是filter-branch，这个会大面积地修改你的历史，所以你很有可能不该去用它，除非你的项目尚未公开，没有其他人在你准备修改的提交的基础上工作。<br>尽管如此，这个可以非常有用。你会学习一些常见用法，借此对它的能力有所认识。<br><br>从所有提交中删除一个文件<br>这个经常发生。有些人不经思考使用git add .，意外地提交了一个巨大的二进制文件，你想将它从所有地方删除。<br>也许你不小心提交了一个包含密码的文件，而你想让你的项目开源。filter-branch大概会是你用来清理整个历史的工具。<br>要从整个历史中删除一个名叫password.txt的文件，你可以在filter-branch上使用–tree-filter选项</p>
</blockquote>
<h4 id="Example：把历史中不小心添加进来的所有mp4文件清除。"><a href="#Example：把历史中不小心添加进来的所有mp4文件清除。" class="headerlink" title="Example：把历史中不小心添加进来的所有mp4文件清除。"></a>Example：把历史中不小心添加进来的所有mp4文件清除。</h4><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"></div><div class="line">删除所有后缀为mp4 的历史提交文件</div><div class="line"></div><div class="line">git filter-branch --index-filter &apos;git rm -r --cached --ignore-unmatch *.mp4&apos; --prune-empty -f</div><div class="line">git for-each-ref --format=&apos;delete %(refname)&apos; refs/original | git update-ref --stdin </div><div class="line">git reflog expire --expire=now --all &amp;&amp; git gc --aggressive --prune=now </div><div class="line"></div><div class="line">如果要push到remote Repo的话，需要加-f 强推</div></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Celery让某个task一个一个地执行</title>
    <url>/2019/celery_lock_task.html</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>最近有个需求是这样子的，某个task要求一定要一个一个地执行，不能并发执行。<br>比较简单的办法是直接将 celery worker启动为 一个进程: “-c 1”。 但是，这种方法会导致其它的task也只能单进程了。</p>
<p>后来通过Google，查找了很多例子，最普遍的一个做法是参考官方文档的做法， 代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">from</span> celery <span class="keyword">import</span> task</div><div class="line"><span class="keyword">from</span> celery.five <span class="keyword">import</span> monotonic</div><div class="line"><span class="keyword">from</span> celery.utils.log <span class="keyword">import</span> get_task_logger</div><div class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> contextmanager</div><div class="line"><span class="keyword">from</span> django.core.cache <span class="keyword">import</span> cache</div><div class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</div><div class="line"><span class="keyword">from</span> djangofeeds.models <span class="keyword">import</span> Feed</div><div class="line"></div><div class="line">logger = get_task_logger(__name__)</div><div class="line"></div><div class="line">LOCK_EXPIRE = <span class="number">60</span> * <span class="number">10</span>  <span class="comment"># Lock expires in 10 minutes</span></div><div class="line"></div><div class="line"><span class="meta">@contextmanager</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">memcache_lock</span><span class="params">(lock_id, oid)</span>:</span></div><div class="line">    timeout_at = monotonic() + LOCK_EXPIRE - <span class="number">3</span></div><div class="line">    <span class="comment"># cache.add fails if the key already exists</span></div><div class="line">    status = cache.add(lock_id, oid, LOCK_EXPIRE)  </div><div class="line">    <span class="comment"># 如果存在lock_id的话会返回False，不存在的话会返回True。这个也可以换成用Redis实现，比如用 setnx</span></div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        <span class="keyword">yield</span> status</div><div class="line">    <span class="keyword">finally</span>:</div><div class="line">        <span class="comment"># memcache delete is very slow, but we have to use it to take</span></div><div class="line">        <span class="comment"># advantage of using add() for atomic locking</span></div><div class="line">        <span class="keyword">if</span> monotonic() &lt; timeout_at <span class="keyword">and</span> status:</div><div class="line">            <span class="comment"># don't release the lock if we exceeded the timeout</span></div><div class="line">            <span class="comment"># to lessen the chance of releasing an expired lock</span></div><div class="line">            <span class="comment"># owned by someone else</span></div><div class="line">            <span class="comment"># also don't release the lock if we didn't acquire it</span></div><div class="line">            cache.delete(lock_id)</div><div class="line"></div><div class="line"><span class="meta">@task(bind=True)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">import_feed</span><span class="params">(self, feed_url)</span>:</span></div><div class="line">    <span class="comment"># The cache key consists of the task name and the MD5 digest</span></div><div class="line">    <span class="comment"># of the feed URL.</span></div><div class="line">    feed_url_hexdigest = md5(feed_url).hexdigest()</div><div class="line">    lock_id = <span class="string">'&#123;0&#125;-lock-&#123;1&#125;'</span>.format(self.name, feed_url_hexdigest)</div><div class="line">    logger.debug(<span class="string">'Importing feed: %s'</span>, feed_url)</div><div class="line">    <span class="keyword">with</span> memcache_lock(lock_id, self.app.oid) <span class="keyword">as</span> acquired:</div><div class="line">        <span class="keyword">if</span> acquired:</div><div class="line">            <span class="keyword">return</span> Feed.objects.import_feed(feed_url).url</div><div class="line">    logger.debug(</div><div class="line">        <span class="string">'Feed %s is already being imported by another worker'</span>, feed_url)</div></pre></td></tr></table></figure>
<p>但是，上面的逻辑只是在有task正执行的时候忽略了新增task。比如说有个import_feed task 正在运行，还没有运行完，<br>再调用apply_async的时候就会不做任何操作。</p>
<p>所以得在上面代码的基础上改一改。</p>
<h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><p>这里我用了一个上厕所的例子。假设有一个公共厕所。如果有人在用着这个厕所的时候其他人就不能使用了，得在旁边排队等候。<br>一次只能进去一个人。废话少说直接上代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"></div><div class="line"><span class="meta">@app.task(bind=True, base=ShitTask, max_retries=10)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">shit_task</span><span class="params">(self, toilet_id)</span>:</span></div><div class="line">    lock_id = <span class="string">'&#123;0&#125;-lock'</span>.format(toilet_id)</div><div class="line">    shit_queue = <span class="string">'&#123;0&#125;-queue'</span>.format(self.name)</div><div class="line">    <span class="comment"># 这里我选用了Redis来处理队列</span></div><div class="line">    <span class="keyword">with</span> memcache_lock(lock_id, self.app.oid) <span class="keyword">as</span> acquired:</div><div class="line">        <span class="keyword">if</span> acquired:</div><div class="line">            <span class="comment"># 当进入到厕所的时候，先把门锁上（其他人就进不来了，然后再拉</span></div><div class="line">            print(<span class="string">'Oh yes, Lock the door and it is my time to shit. '</span>)</div><div class="line">            time.sleep(<span class="number">5</span>)</div><div class="line">            <span class="keyword">return</span> <span class="string">'I finished shit'</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># 有人在用厕所，得在外面等着 </span></div><div class="line">            print(<span class="string">'Oops, somebody engaged the toilet, I have to queue up'</span>)</div><div class="line">            rdb.lpush(shit_queue, json.dumps(list(self.request.args)))</div><div class="line">            <span class="comment"># 将其他人放到Redis里排队等候</span></div><div class="line">            <span class="keyword">raise</span> Ignore()</div></pre></td></tr></table></figure>
<p>上面代码是主要的task处理所及。另外，要先重写一下Task类的 after_return 方法，使得当没能执行的task（在门口排队的人）<br>在正在执行task（正在用厕所的人）成功执行完后，接着执行下一个task（下个人接着用厕所）。</p>
<p>完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment"># coding=u8</span></div><div class="line"><span class="keyword">from</span> celery <span class="keyword">import</span> Celery, Task</div><div class="line"><span class="keyword">from</span> celery.exceptions <span class="keyword">import</span> Ignore</div><div class="line"><span class="keyword">from</span> celery <span class="keyword">import</span> task</div><div class="line"><span class="keyword">import</span> redis</div><div class="line"><span class="keyword">import</span> pickle</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">from</span> celery.five <span class="keyword">import</span> monotonic</div><div class="line"><span class="keyword">from</span> celery.utils.log <span class="keyword">import</span> get_task_logger</div><div class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> contextmanager</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line">app = Celery(<span class="string">'tasks'</span>, broker=<span class="string">'redis://localhost:6379/10'</span>)</div><div class="line"></div><div class="line">logger = get_task_logger(__name__)</div><div class="line"></div><div class="line">LOCK_EXPIRE = <span class="number">60</span> * <span class="number">10</span>  <span class="comment"># Lock expires in 10 minutes</span></div><div class="line"></div><div class="line">rdb = redis.Redis(db=<span class="number">11</span>)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShitTask</span><span class="params">(Task)</span>:</span></div><div class="line">    abstract = <span class="keyword">True</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">after_return</span><span class="params">(self, status, retval, task_id, args, kwargs, einfo)</span>:</span></div><div class="line">        print(status)</div><div class="line">        <span class="keyword">if</span> retval:</div><div class="line">            <span class="comment"># 这个retval的内容就是task return过来的内容</span></div><div class="line">            print(<span class="string">'somebody finished shit, calling the next one to shit'</span>)</div><div class="line">            shit_queue = <span class="string">'&#123;0&#125;-queue'</span>.format(self.name)</div><div class="line">            task_args = rdb.rpop(shit_queue)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> task_args:</div><div class="line">                task_args = json.loads(task_args)</div><div class="line">                self.delay(*task_args)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="meta">@contextmanager</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">memcache_lock</span><span class="params">(lock_id, oid)</span>:</span></div><div class="line">    timeout_at = monotonic() + LOCK_EXPIRE - <span class="number">3</span></div><div class="line">    <span class="comment"># status = cache.add(lock_id, oid, LOCK_EXPIRE)  # 如果lock_id 存在则返回False，如果不存在则返回True</span></div><div class="line">    status = rdb.setnx(lock_id, oid)</div><div class="line">    rdb.expire(lock_id, LOCK_EXPIRE)</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        <span class="keyword">yield</span> status</div><div class="line">    <span class="keyword">finally</span>:</div><div class="line">        <span class="keyword">if</span> monotonic() &lt; timeout_at <span class="keyword">and</span> status:</div><div class="line">            <span class="comment"># 设置一个时间限制，一个人不能占用厕所太久，而且只有占用厕所的那人才能开锁把厕所门打开</span></div><div class="line">            print(<span class="string">'release the lock and open the door of the toilet %s'</span> % lock_id)</div><div class="line">            rdb.delete(lock_id)</div><div class="line"></div><div class="line"></div><div class="line"><span class="meta">@app.task(bind=True, base=ShitTask, max_retries=10)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">shit_task</span><span class="params">(self)</span>:</span></div><div class="line">    print(<span class="string">'task name %s'</span> % self.name)</div><div class="line">    </div><div class="line">    lock_id = <span class="string">'&#123;0&#125;-lock'</span>.format(self.name)</div><div class="line">    shit_queue = <span class="string">'&#123;0&#125;-queue'</span>.format(self.name)</div><div class="line">    print(lock_id)</div><div class="line">    <span class="keyword">with</span> memcache_lock(lock_id, self.app.oid) <span class="keyword">as</span> acquired:</div><div class="line">        print(<span class="string">'acquired'</span>, acquired)</div><div class="line">        <span class="keyword">if</span> acquired:</div><div class="line">            print(<span class="string">'Oh yes, Lock the door and it is my time to shit. '</span>)</div><div class="line">            time.sleep(<span class="number">5</span>)</div><div class="line">            <span class="keyword">return</span> <span class="string">'I finished shit'</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            print(<span class="string">'Oops, somebody engaged the toilet, I have to queue up'</span>)</div><div class="line">            <span class="comment">#pending_task = pickle.dumps(self)</span></div><div class="line">            <span class="comment">#rdb.lpush(shit_queue, pending_task)</span></div><div class="line">            <span class="comment"># 不能用pickle 的去序列化task。在after_return load的时候会出现很诡异的现象。load出的task是第一个acquired的task</span></div><div class="line">            <span class="comment"># 改为用json来做序列化</span></div><div class="line">            rdb.lpush(shit_queue, json.dumps(list(self.request.args)))</div><div class="line">            <span class="keyword">raise</span> Ignore()</div></pre></td></tr></table></figure>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>可以新建一个test_celery项目来检验一下。新建一个目录名叫 test_celery<br>然后新建一个tasks.py文件，内容就是上面代码。<br>用下面命令来启动Celery worker，这里用了8个进程来处理。设置多点可以增加task的并行执行任务数。<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">celery -A tasks worker -l info -c 8</div></pre></td></tr></table></figure></p>
<p>然后，可以启动ipython 进行调用task。<br><img src="/images/2019/celery_lock_task/1d60a617.png" alt=""><br>快速地敲几个task.delay</p>
<p>在celery日志中我们可以看到两个shit_tasks 是一个接一个来运行的。而不是并行执行。<br><img src="/images/2019/celery_lock_task/50ef5fbc.png" alt=""></p>
<p>代码放在了：<br><a href="https://github.com/mikolaje/celery_toys/tree/master/test_lock" target="_blank" rel="external">https://github.com/mikolaje/celery_toys/tree/master/test_lock</a></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="http://docs.celeryproject.org/en/latest/tutorials/task-cookbook.html#ensuring-a-task-is-only-executed-one-at-a-time" target="_blank" rel="external">http://docs.celeryproject.org/en/latest/tutorials/task-cookbook.html#ensuring-a-task-is-only-executed-one-at-a-time</a></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>celery</tag>
      </tags>
  </entry>
  <entry>
    <title>新西兰十二日自由行攻略</title>
    <url>/2019/nz_travel.html</url>
    <content><![CDATA[<p>新西兰:flag-nz:十二日自由行攻略<br>更多详情可上新西兰旅游局官方网站查询<a href="https://www.newzealand.com/cn/" target="_blank" rel="external">https://www.newzealand.com/cn/</a></p>
<p>:memo:签证：提前三个月办理即可，一般都能拿到有效期五年的签证。现在新西兰是电子签证，无需贴签，本人是找淘宝旅行社办理，如果个人不嫌麻烦完全可以自己登陆相关网站申请签证，蜂窝网有很全面的介绍，我是懒得自己弄。新西兰签证比欧美签证简单，银行卡半年流水只需三万，无需按指纹面试等，两周内可以出签。签证出来后留电子版或者打印出来，到新西兰过关时直接刷护照即可过关，都没人检查电子签，因为系统已有纪录。</p>
<p>另外如果是要转机的话，需要提前确认是否需要过境签。比如，在澳大利亚转机就要申请过境签。澳大利亚的过境签申请挺简单的，在官网注册个账号然后填个表就好了，免费的。</p>
<p><a href="https://immi.homeaffairs.gov.au/visas/getting-a-visa/visa-listing/transit-771" target="_blank" rel="external">https://immi.homeaffairs.gov.au/visas/getting-a-visa/visa-listing/transit-771</a></p>
<p>:airplane:飞机：建议提前买，签证没出来之前也可以买了（因为签证很容易过），越早买越便宜！直飞大概要11.5个小时，飞奥克兰的班次多且便宜。</p>
<p>我们来回买的都是在澳大利亚悉尼转机的。去的是维珍航空，回的是澳大利亚航空。维珍的机票比较便宜，便宜的缺点就是他们的refreshment好难吃，有一餐我根本就没吃饱，另外要了一小碗泡面。相比之下，澳大利亚航空的伙食就好太多了，吃得都好。</p>
<p>:dollar:现金：提前去银行换好新西兰币，汇率大概是1:4.6。准备好visa或者master卡，新西兰旅游景点很多可以刷微信、支付宝、银联。</p>
<p>:phone:电话卡：建议提前在淘宝买电话卡，比较便宜。朋友建议买Vodafone的卡，价格比较高，也可以买2degree的卡，华为背景价格便宜。我买了Vodafone和2degree的卡，感觉没啥区别，到了偏远地区都是没有信号。如果要自驾游的朋友最好提早在谷歌下载离线地图，免得走错路线。</p>
<p>:moneybag:保险：建议出发前买个基础保险，新西兰看病非常贵，发生拉肚子阑尾炎等情况必须看病，买了保险可以减轻负担。我们买的是安联保险，性价比很高。</p>
<p>:house:住宿：为了深入了解当地文化以及省钱，我们全程在Airbnb上订民宿，大家一定要订超赞房东，看住客的评论，以免入坑。</p>
<p>:bus:交通：新西兰交通费很贵，出租车尤其贵，Uber稍微比的士便宜点。公交车也比国内贵很多，如果是在皇后镇没办卡的情况下，每个人是五刀。办卡的话大概是2刀一次这样。自驾行虽然方便但是难度大，新西兰驾驶方向和国内相反，靠左行驶，山路多，当地人开车速度快。如果对国外驾驶规则不是很熟的话，不建议自驾。</p>
<p>具体行程:car:<br>前四天：奥克兰:boat:<br>奥克兰是新西兰最大的城市，一半是海水，一半是都市，是著名的风帆之都。这是我们在新西兰最喜欢的城市，大部分华人都住在奥克兰。我们参观了</p>
<p>【奥克兰战争纪念博物馆】</p>
<p><img src="/images/2019/nz_travel/DSC00223.JPG" alt="Sample Image Added via Markdown"></p>
<p>【伊甸山】</p>
<p>【天空塔】</p>
<p><img src="/images/2019/nz_travel/DSC00091.JPG" alt="Sample Image Added via Markdown"></p>
<p>【皇后大街】【海边码头】<br>强烈推荐新西兰唯一官方旅行社isite，在天空塔楼下就有一家，我们报了三次一日游，大巴包接送。最后一天，我们报了一日游前往【玛塔玛塔霍比特村】<br><img src="/images/2019/nz_travel/DSC00356.JPG" alt="Sample Image Added via Markdown"></p>
<p>，中土世界的童话小镇，满眼都是绿油油的草地。在奥克兰的交通方式就是uber和走路。<br>:fork_and_knife:推荐餐厅：Depot（天空塔下）、Giapo冰淇淋</p>
<p>中间四天：皇后镇:crown:<br>皇后镇是非常出名的旅游城市啦，超级多游客，中国游客尤其多。皇后镇衣食住行都很贵。皇后镇风光旖旎，生态环境也很好，新西兰的自来水可以直接喝。我们参观了</p>
<p>【格林诺奇】</p>
<p>【箭镇】</p>
<p>【瓦卡蒂普湖】</p>
<p><img src="/images/2019/nz_travel/IMG_3222.JPG" alt="Sample Image Added via Markdown"></p>
<p>【坐蒸汽船看牧场喂动物】</p>
<p><img src="/images/2019/nz_travel/IMG_3200.JPG" alt="Sample Image Added via Markdown"></p>
<p>皇后镇可以买优惠公交卡，10纽币可以坐5次，非常划算！最后一天，我们报了I-site的一日游，从皇后镇坐大巴到基督城，沿途经过库克山/蒂卡波湖/好牧羊人教堂。<br>:fork_and_knife:推荐餐厅：Fergburger汉堡</p>
<p><img src="/images/2019/nz_travel/IMG_3081.JPG" alt="Sample Image Added via Markdown"></p>
<p>最后两天：基督城:latin_cross:<br>基督城于2011年发生大地震，现在城市还没恢复过来，比较萧条，没什么人气。可以购买有轨复古电车:railway_car:一日游，随上随下，</p>
<p><img src="/images/2019/nz_travel/DSC00670.JPG" alt="Sample Image Added via Markdown"></p>
<p>在站点下车可参观【基督城博物馆】</p>
<p>【超级漂亮的海格利公园】</p>
<p><img src="/images/2019/nz_travel/DSC00650.JPG" alt="Sample Image Added via Markdown"></p>
<p>【基督城大教堂】</p>
<p><img src="/images/2019/nz_travel/IMG_3253.jpg" alt="Sample Image Added via Markdown"></p>
<p>【图书馆】</p>
<p>【纸板大教堂】</p>
<p>【皇后广场】基</p>
<p>督城内很多涂鸦，比较文艺。</p>
<p><img src="/images/2019/nz_travel/IMG_3266.JPG" alt="Sample Image Added via Markdown"></p>
<p>留一两天就够了。<br>newzealand.com<br>新西兰旅游局欢迎您 | 最全面最官方的新西兰旅游攻略<br>欢迎您来新西兰旅游。敬请浏览新西兰旅游局官方网站，了解新西兰旅游攻略、经典观光线路推荐、特色景点介绍、当地美酒佳肴、酒店住宿推荐、航班与机场信息等内容。</p>
]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
        <tag>travel</tag>
      </tags>
  </entry>
  <entry>
    <title>Ngrok服务器部署</title>
    <url>/2019/ngrok_deployment.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>自己使用Google Cloud，阿里云，AWS有好几年了，在云服务器上花了不少钱，有段时间做爬虫，需要较大的磁盘空间（几百G），每个月大概要5，600元。<br>一年下来就6，7钱了。想想，还是直接买一台主机或服务器在家里放着好了，也就几千块钱。于是我买了台主机在家里放着，配置远远比原来ECS的配置要高，<br>而且还更便宜。把数据全部迁移到了自己的主机，但很多时候自己都不在家，不可能把笨重的主机带出去外面。于是得想办法在其它地方通过网络连接到主机。<br>刚开选择用Teamviewer，可以远程连接到主机的桌面，我的主机的操作系统是Ubuntu的，感觉还行。但是，慢慢感觉效率不是很高，因为是用桌面远程，</p>
<p>所以网速要求，还是有点高，更大的问题是Teamviewer是收费的，而且价格不菲，这样成本算下来一年下来也要花费挺多钱。<br>于是，我想到了以前的花生壳 内网穿透映射，但那个也是收费的。终于找到了一个令我满意的解决方案—-Ngrok！</p>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>Ngrok到底好在哪里呢？ 首先，也是最重要的是它的成本底。<br>主机买服务的话也就10多块钱一个月，网上一搜就很多Ngrok的提供商；自己搭建的话，也就买一台虚拟机的价格，我买的是某云的ECS。大概也就几十来块钱。</p>
<p>其次，Ngrok可以把你家里的电脑当成服务器来使用，远程登陆SSH；或者作为数据库，暴露对应的端口提供服务；或者直接作为Web服务，直接提供http访问。<br>这使得不用买昂贵的云服务，省了不少钱。不够配置的话可以自己买内存，SSD增加机器配置。</p>
<h3 id="准备工作："><a href="#准备工作：" class="headerlink" title="准备工作："></a>准备工作：</h3><h4 id="需要购买的服务"><a href="#需要购买的服务" class="headerlink" title="需要购买的服务"></a>需要购买的服务</h4><ol>
<li>有一个外网的虚拟机，我用的是国内的阿里云的，操作系统是Ubuntu。</li>
<li>有个自己注册的域名。<br>需要添加2条域名解析：<br><img src="/images/2019/ngrok_deployment/647f55bb.png" alt=""></li>
</ol>
<h4 id="部署工作："><a href="#部署工作：" class="headerlink" title="部署工作："></a>部署工作：</h4><ol>
<li><p>apt-get install golang<br>因为ngrok是用Go 来写的，所以要安装一下Go环境</p>
</li>
<li><p>git clone <a href="https://github.com/inconshreveable/ngrok.git" target="_blank" rel="external">https://github.com/inconshreveable/ngrok.git</a><br>然后把ngrok的源码clone下来。</p>
</li>
</ol>
<h4 id="证书安装："><a href="#证书安装：" class="headerlink" title="证书安装："></a>证书安装：</h4><blockquote>
<p>使用ngrok.com官方服务时，我们使用的是官方的SSL证书。自己建立ngrok服务，需要我们生成自己的证书，并提供携带该证书的ngrok客户端。首先指定域名：</p>
</blockquote>
<p>进入ngrok目录后，运行下面的指令<br><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="built_in">export</span> NGROK_DOMAIN=<span class="string">"ngrok.xfl.host"</span>  </div><div class="line">//这里换成你的自己注册的域名，比如你注册的域名为abc.com。这里可以填 ngrok.abc.com</div><div class="line"></div><div class="line">openssl genrsa -out rootCA.key 2048</div><div class="line">openssl req -x509 -new -nodes -key rootCA.key -subj <span class="string">"/CN=<span class="variable">$NGROK_DOMAIN</span>"</span> -days 5000 -out rootCA.pem</div><div class="line">openssl genrsa -out device.key 2048</div><div class="line">openssl req -new -key device.key -subj <span class="string">"/CN=<span class="variable">$NGROK_DOMAIN</span>"</span> -out device.csr</div><div class="line">openssl x509 -req -in device.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out device.crt -days 5000</div></pre></td></tr></table></figure></p>
<p>我们在编译可执行文件之前，需要把生成的证书分别替换到 assets/client/tls和assets/server/tls中，这两个目录分别存放着ngrok和ngrokd的默认证书。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">cp rootCA.pem assets/client/tls/ngrokroot.crt</div><div class="line">cp device.crt assets/server/tls/snakeoil.crt</div><div class="line">cp device.key assets/server/tls/snakeoil.key</div></pre></td></tr></table></figure>
<h3 id="ngrok部署安装"><a href="#ngrok部署安装" class="headerlink" title="ngrok部署安装"></a>ngrok部署安装</h3><h4 id="编译ngrokd-和-ngrok"><a href="#编译ngrokd-和-ngrok" class="headerlink" title="编译ngrokd 和 ngrok"></a>编译ngrokd 和 ngrok</h4><p>首先需要知道，ngrokd 为服务端的执行文件，ngrok为客户端的执行文件。</p>
<p>有没有release的区别是，包含release的编译结果会把assets目录下的内容包括进去，从而可以独立执行。<br>如果你今后还要更换证书，建议编译不包含release的版本。。首先编译ngrok服务端（ngrokd），默认为Linux版本：<br><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">make clean</div><div class="line">make release-server</div></pre></td></tr></table></figure></p>
<p>编译ngrokd后，我们来编译ngrok(客户端)<br>在编译客户端的时候需要指明对应的操作系统和构架：</p>
<p>Linux 平台 32 位系统：GOOS=linux GOARCH=386<br>Linux 平台 64 位系统：GOOS=linux GOARCH=amd64<br>Windows 平台 32 位系统：GOOS=windows GOARCH=386<br>Windows 平台 64 位系统：GOOS=windows GOARCH=amd64<br>MAC 平台 32 位系统：GOOS=darwin GOARCH=386<br>MAC 平台 64 位系统：GOOS=darwin GOARCH=amd64<br>ARM 平台：GOOS=linux GOARCH=arm</p>
<p>我的Ubuntu属于linux 64，所以我执行如下指令：<br><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">GOOS=linux GOARCH=amd64 make release-client</div></pre></td></tr></table></figure></p>
<h4 id="启动ngrokd"><a href="#启动ngrokd" class="headerlink" title="启动ngrokd"></a>启动ngrokd</h4><p>编译后生成两个文件分别为服务端（ngrokd）和客户端(ngrok)。切换到对应的文件夹，运行服务端：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">./ngrokd -domain=<span class="string">"<span class="variable">$NGROK_DOMAIN</span>"</span> -httpAddr=<span class="string">":801"</span> -httpsAddr=<span class="string">":802"</span></div><div class="line">//801 是访问的http端口，比如，在本例子中，访问http://ngrok.xfl.host:801 就可以看到映射出的网页</div></pre></td></tr></table></figure>
<p>参数-domain表示服务器域名，请改成你自己的域名；-httpAddr表示默认监听的HTTP端口，-httpsAddr表示默认监听的HTTPS端口，<br>因为我用不到所以都设置成空字符串”“来关闭监听，如果需要打开的话记得格式是:12345（冒号+端口号）这样的；-tunnelAddr表示服务器监听客户端连接的隧道端口号，格式和前面一样；<br>-log表示日志文件位置；还有个-log-level用来控制日志记录的事件级别，选项有DEBUG、INFO、WARNING、ERROR。</p>
<p>如果编译的是不带release的版本，还可以通过-tlsCrt和-tlsKey选项来指定证书文件的位置。</p>
<p>出现类似以下内容，则说明我们的服务器端ngrokd正常运行了:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[16:41:56 CST 2017/04/20] [INFO] (ngrok/log.(*PrefixLogger).Info:83) [registry] [tun] No affinity cache specified</div><div class="line">[16:41:56 CST 2017/04/20] [INFO] (ngrok/log.(*PrefixLogger).Info:83) [metrics] Reporting every 30 seconds</div><div class="line">[16:41:57 CST 2017/04/20] [INFO] (ngrok/log.Info:112) Listening for public http connections on [::]:80</div><div class="line">[16:41:57 CST 2017/04/20] [INFO] (ngrok/log.Info:112) Listening for public https connections on [::]:443</div><div class="line">[16:41:57 CST 2017/04/20] [INFO] (ngrok/log.Info:112) Listening for control and proxy connections on [::]:4443</div><div class="line">[16:41:57 CST 2017/04/20] [INFO] (ngrok/log.(*PrefixLogger).Info:83) [tun:627acc92] New connection from 42.53.196.242:9386</div><div class="line">[16:41:57 CST 2017/04/20] [DEBG] (ngrok/log.(*PrefixLogger).Debug:79) [tun:627acc92] Waiting to read message</div><div class="line">[16:41:57 CST 2017/04/20] [DEBG] (ngrok/log.(*PrefixLogger).Debug:79) [tun:627acc92] Reading message with length: 159</div></pre></td></tr></table></figure>
<h4 id="配置ngrok客户端"><a href="#配置ngrok客户端" class="headerlink" title="配置ngrok客户端"></a>配置ngrok客户端</h4><p>将之前编译好的客户端文件(ngrok 文件)拷贝到需要使用服务的设备（自己买的那台笨重的主机）上。</p>
<h4 id="启动ngrok客户端"><a href="#启动ngrok客户端" class="headerlink" title="启动ngrok客户端"></a>启动ngrok客户端</h4><p>在ngrok同路径下建立配置文件ngrok.yml<br><figure class="highlight yaml"><table><tr><td class="code"><pre><div class="line"><span class="attr">server_addr:</span> <span class="string">"ngrok.xfl.host:4443"</span></div><div class="line"><span class="attr">trust_host_root_certs:</span> <span class="literal">false</span></div><div class="line"><span class="attr">tunnels:</span></div><div class="line"><span class="attr">  ssh:</span></div><div class="line"><span class="attr">    remote_port:</span> <span class="number">6666</span></div><div class="line"><span class="attr">    proto:</span></div><div class="line"><span class="attr">      tcp:</span> <span class="number">22</span></div></pre></td></tr></table></figure></p>
<p>server_addr端口默认4443，可通过ngrokd服务端启动修改端口。在tunnels里配置隧道信息，<br>注意http和https隧道可设置subdomain和auth，而tcp里只能设置remote_port。</p>
<p>使用如下命令启动ngrok客户端：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ngrok -log=stdout -config=./ngrok.yml start ssh</div></pre></td></tr></table></figure></p>
<p>正常启动，你将会看到如下日志：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ngrok                                                                                                                                                                                     (Ctrl+C to quit)</div><div class="line">                                                                                                                                                                                                          </div><div class="line">Tunnel Status                 online                                                                                                                                                                      </div><div class="line">Version                       1.7/1.7                                                                                                                                                                     </div><div class="line">Forwarding                    http://demo.ngrok.xfl.host -&gt; 127.0.0.1:19999                                                                                                                               </div><div class="line">Forwarding                    https://demo.ngrok.xfl.host -&gt; 127.0.0.1:19999                                                                                                                              </div><div class="line">Web Interface                 127.0.0.1:4040                                                                                                                                                              </div><div class="line"># Conn                        0                                                                                                                                                                           </div><div class="line">Avg Conn Time                 0.00ms</div></pre></td></tr></table></figure></p>
<p>Notice:如果显示reconnecting说明连接有错，在运行时加入-log=stdout来进行debug。</p>
<ol>
<li>有可能是因为你的Ngrok服务器没有开 4443端口</li>
<li>有可能是域名没有解析成功</li>
</ol>
<h4 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h4><p>ngrok 的配置文件是完全可选的非常简单 YAML 格式文件，他可以允许你使用 ngrok 一些更高级的功能，例如：</p>
<p>同时运行多个隧道</p>
<ul>
<li>连接到自定义的 ngrok 服务器</li>
<li>调整 ngrok 一些很神秘的功能</li>
<li><p>ngrok 的配置文件默认从 ~/.ngrok 加载。你可以通过 -config 参数重写配置文件的地址</p>
</li>
<li><p>同时运行多个隧道<br>为了运行多个隧道，你需要在配置文件当中使用 tunnels 参数配置每个隧道。隧道的参数以字典的形式配置在配置文件当中。<br>举个例子，让我们来定义三个不同的隧道。第一个隧道是一个有认证的只转发 https 的隧道。第二个隧道转发我们自己机器的 22 端口以便让我可以通过隧道连接到自己的电脑。<br>最后，我们使用自己的域名创造了一个隧道，我们将要在黑客马拉松中展示这个。</p>
</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><div class="line"><span class="attr">tunnels:</span></div><div class="line"><span class="attr">  client:</span></div><div class="line"><span class="attr">    auth:</span> <span class="string">"user:password"</span></div><div class="line"><span class="attr">    proto:</span></div><div class="line"><span class="attr">      https:</span> <span class="number">8080</span></div><div class="line"><span class="attr">  ssh:</span></div><div class="line"><span class="attr">    proto:</span></div><div class="line"><span class="attr">      tcp:</span> <span class="number">22</span></div><div class="line">  hacks.inconshreveable.com:</div><div class="line"><span class="attr">    proto:</span></div><div class="line"><span class="attr">      http:</span> <span class="number">9090</span></div></pre></td></tr></table></figure>
<p>通过 ngrok start 命令，我们可以同时运行三个隧道，后面要接上我们要启动的隧道名。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">ngrok start client ssh hacks.inconshreveable.com</div></pre></td></tr></table></figure>
<ul>
<li>隧道设置<br>每一个隧道都可以设置以下五个参数：proto，subdomain，auth，hostname 以及 remote_port。<br>每一个隧道都必须定义 proto ，因为这定义了协议的类型以及转发的目标。当你在运行 http/https 隧道时， auth 参数是可选的，<br>同样， remote_port 也是可选的，他声明了某个端口将要作为远程服务器转发的端口，请注意这只适用于 TCP 隧道。<br>ngrok 使用每个隧道的名字做到子域名或者域名，但你可以重写他：</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><div class="line"><span class="attr">tunnels:</span></div><div class="line"><span class="attr">  client:</span></div><div class="line"><span class="attr">    subdomain:</span> <span class="string">"example"</span></div><div class="line"><span class="attr">    auth:</span> <span class="string">"user:password"</span></div><div class="line"><span class="attr">    proto:</span></div><div class="line"><span class="attr">      https:</span> <span class="number">8080</span></div></pre></td></tr></table></figure>
<p>对于 TCP 隧道，你可以会通过 remote_port 参数来指定一个远程服务器的端口作为映射。如果没有声明，服务器将会给你随机分配一个端口。</p>
<h3 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h3><p>如果使用国内IP或者国内的域名的话，用http的话貌似因为要备案，所以访问不了。前两天我是可以正常访问的，但是过了两天后就突然访问不了了。<br>可以改为用TCP的方式比如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">netdata:</div><div class="line">  remote_port: 801 </div><div class="line">  proto:</div><div class="line">    tcp: 19999</div></pre></td></tr></table></figure></p>
<p>访问 ngnrok.xfl.host:801 即可。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://morongs.github.io/2016/12/28/dajian-ngrok/" target="_blank" rel="external">https://morongs.github.io/2016/12/28/dajian-ngrok/</a><br><a href="https://luozm.github.io/ngrok" target="_blank" rel="external">https://luozm.github.io/ngrok</a><br><a href="https://imlonghao.com/28.html" target="_blank" rel="external">https://imlonghao.com/28.html</a></p>
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>ngrok</tag>
      </tags>
  </entry>
  <entry>
    <title>CDH 在ubuntu上的部署和安装，以及一些坑</title>
    <url>/2019/cdh_install.html</url>
    <content><![CDATA[<p>最近两天在自己电脑上搭建一个Cloudera Manager来玩玩。本来以为挺简单的，只是在Web UI上无脑下一步就好了，<br>但其实还是遇到挺多问题的。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="在服务器上的操作"><a href="#在服务器上的操作" class="headerlink" title="在服务器上的操作"></a>在服务器上的操作</h3><p>刚开始基本上，就是按照官网的步骤来走，首先做一些前置工作:</p>
<ol>
<li>配置下apt</li>
<li>安装JDK.</li>
<li>安装下NTP时间同步的程序；</li>
<li>安装好Mysql，MariaDB，Posgres。<br>其中的一个数据库，刚开始以为都要安装。。。然后又把MariaDB这些一个个卸载了；</li>
<li>在Mysql中创建一些CM所需的数据库和表。如下所是。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">CREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON scm.* TO &apos;scm&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON rman.* TO &apos;rman&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON hue.* TO &apos;hue&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON metastore.* TO &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON sentry.* TO &apos;sentry&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE nav DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON nav.* TO &apos;nav&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE navms DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON navms.* TO &apos;navms&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON oozie.* TO &apos;oozie&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div></pre></td></tr></table></figure>
<ol>
<li>通过scm_prepare_database.sh 脚本来进一步设置Manager Database<br>sudo /opt/cloudera/cm/schema/scm_prepare_database.sh [options] <databasetype> <databasename> <databaseuser> <password><br>只需要执行一次<code>sudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm mypassword</code>就好了。</password></databaseuser></databasename></databasetype></li>
</ol>
<h3 id="在Web-UI上的安装"><a href="#在Web-UI上的安装" class="headerlink" title="在Web UI上的安装"></a>在Web UI上的安装</h3><ol>
<li>首先记得在每台机器上配置好/etc/hosts</li>
<li>在Web上的安装基本上就是点继续。有些地方要注意。<br>这一步会等待比较长的时间，会下载安装一些parcels。</li>
</ol>
<p><img src="/images/2019/cdh_install/323205c4.png" alt="Sample Image Added via Markdown"></p>
<ol>
<li>然后，基本上就是下一步了。到这一步，我是创建了一个叫cloudera的用户，要给与它sudo以及password-exempt<br><img src="/images/2019/cdh_install/cloudera_install.jpg" alt="Sample Image Added via Markdown"></li>
</ol>
<p>对了，因为我的是单机版的，所以HDFS那边会报错一个叫：副本不足的块 存在隐患。<br>这是因为只有一个节点，Block块无法，分配到其它的节点作为备份。默认是有2个备份Block分发到其它节点。</p>
<h3 id="启动CDH"><a href="#启动CDH" class="headerlink" title="启动CDH"></a>启动CDH</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">sudo systemctl start cloudera-scm-server</div><div class="line"></div><div class="line"># 查看scm server日志，scm的全称是：The Service and Configuration Manager </div><div class="line">sudo tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log</div></pre></td></tr></table></figure>
<h3 id="停止CDH"><a href="#停止CDH" class="headerlink" title="停止CDH"></a>停止CDH</h3><p>有些时候我们要停机检修一下电脑，所以要停止Cluster。很简单，首先进入到CDH的Web UI的Cluster主界面, 左上角有个Action<br>，点一下弹出下拉条，然后选停止。等几分钟后集群的所有组件就会停止了。</p>
<p>然后进入到主节点的终端，输入<code>sudo systemctl stop cloudera-scm-server</code>，就全部停止了。</p>
<h2 id="CDH的一些配置"><a href="#CDH的一些配置" class="headerlink" title="CDH的一些配置"></a>CDH的一些配置</h2><h3 id="Yarn-RM-NM共用一个host"><a href="#Yarn-RM-NM共用一个host" class="headerlink" title="Yarn: RM,NM共用一个host"></a>Yarn: RM,NM共用一个host</h3><p>默认情况下Resource Manager会单独用一个节点。但是我的RM host内存和CPU都有剩余，跑app的时候把资源压在<br>device2上有点浪费了，我利用起device1的资源来。<br>首先进入到Yarn的版块，Action下拉框，点击Add Role Instance<br><img src="/images/2019/cdh_install/yarn_rm_nm1.png" alt="Sample Image Added via Markdown"><br><img src="/images/2019/cdh_install/yarn_rm_nm1.png" alt="Sample Image Added via Markdown"></p>
<h3 id="增加服务"><a href="#增加服务" class="headerlink" title="增加服务"></a>增加服务</h3><p>如果我们想新增加一些组件，比如Kafka或Spark，然后我们可以点击Cluster版块的Action下拉框，选中第一个 Add Service<br>进入新增Service的页面。</p>
<h2 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h2><h3 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h3><p>我增加一个节点的时候遇到如下报错<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Host with invalid Cloudera Manager GUID is detected</div><div class="line">...</div><div class="line">Error, CM server guid updated, expected c3b5fe15-5f29-434b-ae0a-4750b56c72ab, received dc1d28d4-4c78-4a07-919b-a9eaf7190d41</div></pre></td></tr></table></figure></p>
<p>解决方法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">验证如下配置文件，确定hostname是否正确</div><div class="line">$ nano /etc/cloudera-scm-agent/config.ini</div><div class="line">so that the hostname where the same as the command $ hostname returned.</div><div class="line">Then rm /var/lib/cloudera-scm-agent/cm_guid</div><div class="line">然后删除每个节点的cm_guid</div><div class="line">then I restarted the agent and the server of cloudera:</div><div class="line">然后重启</div><div class="line">$ service cloudera-scm-agent restart</div><div class="line">$ service cloudera-scm-server restart</div></pre></td></tr></table></figure></p>
<h3 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h3><p>在NameNode Format的时候遇到如下报错<br>Running in non-interactive mode, and data appears to exist in Storage Directory /dfs/nn. Not formatting.</p>
<p>解决方案：<br>删除/dfs/nn 以及 /dfs/dn里面的所有数据<br>因为之前我安装了一个单机集群，HDFS里面放了一些数据</p>
<h3 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h3><p>Cloudera 在Validate Hive Metastore schema的时候出现如下错误，发现metastore里面没有VERSION table<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Fri Jul 19 14:06:33 CST 2019 WARN: Establishing SSL connection without server&apos;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&apos;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &apos;false&apos;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</div><div class="line">org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version, Cause:Table &apos;metastore.VERSION&apos; doesn&apos;t exist</div><div class="line">org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version, Cause:Table &apos;metastore.VERSION&apos; doesn&apos;t exist</div><div class="line">	at org.apache.hadoop.hive.metastore.CDHMetaStoreSchemaInfo.getMetaStoreSchemaVersion(CDHMetaStoreSchemaInfo.java:342)</div><div class="line">	at org.apache.hive.beeline.HiveSchemaTool.validateSchemaVersions(HiveSchemaTool.java:685)</div><div class="line">	at org.apache.hive.beeline.HiveSchemaTool.doValidate(HiveSchemaTool.java:578)</div><div class="line">	at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1142)</div><div class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</div><div class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</div><div class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class="line">	at java.lang.reflect.Method.invoke(Method.java:498)</div><div class="line">	at org.apache.hadoop.util.RunJar.run(RunJar.java:313)</div><div class="line">	at org.apache.hadoop.util.RunJar.main(RunJar.java:227)</div><div class="line">*** schemaTool failed ***</div></pre></td></tr></table></figure></p>
<p>解决方案：<br><code>dennis@device1:/opt/cloudera/parcels/CDH/lib/hive/bin$ schematool -dbType mysql -initSchema -passWord password -userName hive</code></p>
<h3 id="问题4"><a href="#问题4" class="headerlink" title="问题4"></a>问题4</h3><p>在Hue上的hive上运行一些 insert 和count(*) 操作时候会一直卡住（stuck, hang），没有任何反应，也没报错。<br>看日志是说MR 还没有启动。在Cloudera的community上查到 要mapred-site.xml的参数 mapreduce.framework.name 设置为 local</p>
<p>于是我在CDH中的Yarn集群下修改了mapreduce.framework.name 为 local，然后重启集群后就成功了。 select count(*) 和 insert就不会卡住了。</p>
<h3 id="问题5"><a href="#问题5" class="headerlink" title="问题5"></a>问题5</h3><p>在hue上面可以正常地使用Hive。在device2下用hive cli没有问题。但在device1 下的bash执行hive command，里面输入show databases后报错：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</div></pre></td></tr></table></figure></p>
<p> 解决方案：<br> 先用 hive -hiveconf hive.root.logger=DEBUG,console<br> 在调试，查看到更多有价值的报错信息。<br> 果然，查到如下信息：<br> <figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"> Caused by: java.io.IOException: Keystore was tampered with, or password was incorrect</div><div class="line">	at com.sun.crypto.provider.JceKeyStore.engineLoad(JceKeyStore.java:865) ~[sunjce_provider.jar:1.8.0_112]</div><div class="line">	at java.security.KeyStore.load(KeyStore.java:1445) ~[?:1.8.0_121]</div><div class="line">	at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.locateKeystore(AbstractJavaKeyStoreProvider.java:322) ~[hadoop-common-3.0.0-cdh6.2.0.jar:?]</div><div class="line">	at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.&lt;init&gt;(AbstractJavaKeyStoreProvider.java:86) ~[hadoop-common-3.0.0-cdh6.2.0.jar:?]</div><div class="line">	at org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider.&lt;init&gt;(LocalJavaKeyStoreProvider.java:58) ~[hadoop-common-3.0.0-cdh6.2.0.jar:?]</div><div class="line"></div><div class="line">	at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:237) ~[hive-exec-2.1.1-cdh6.2.0.jar:2.1.1-cdh6.2.0]</div><div class="line">	... 23 more</div><div class="line">Caused by: java.security.UnrecoverableKeyException: Password verification failed</div></pre></td></tr></table></figure></p>
<p>按日志的报错信息来说是我的源数据库密码不对，于是我查看hive-site.xml配置文件，发现<br>/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/hive/conf/hive-site.xml<br>也就是/etc/hive/conf/hive-site.xml(我猜CDH会把上面目录的所有配置文件复制一遍到 /etc/hive/conf/下)<br>我之前把它改了，所以那配置有问题，我把它改为默认的配置重启后就恢复正常了！</p>
<h3 id="问题6"><a href="#问题6" class="headerlink" title="问题6"></a>问题6</h3><p>启动 cloudera-scm-server 时候报错如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table &apos;metastore.CM_VERSION&apos; doesn&apos;t exist</div></pre></td></tr></table></figure></p>
<p> 解决方案：<br><code>sudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm mypassword</code><br>重新执行下这条命令。之前我把所有的服务都执行了一遍（amon, rman, … metastore, … etc)，是我误解了scm_prepare_database.sh的作用。<br>按官网所说的：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Cloudera Manager Server includes a script that can create and configure a database for itself. The script can:</div><div class="line">Create the Cloudera Manager Server database configuration file.</div><div class="line">(MariaDB, MySQL, and PostgreSQL) Create and configure a database for Cloudera Manager Server to use.</div><div class="line">(MariaDB, MySQL, and PostgreSQL) Create and configure a user account for Cloudera Manager Server.</div></pre></td></tr></table></figure></p>
<p>这个脚本只需要执行一次就好了，就是<code>sudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm mypassword</code><br>然后重启cloudera-scm-server解决问题。此外，通过<code>/etc/cloudera-scm-server/db.properties</code> 也可以确定目前scm用的是哪个数据库。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.cloudera.com/documentation/enterprise/6/6.2/topics/introduction.html" target="_blank" rel="external">https://www.cloudera.com/documentation/enterprise/6/6.2/topics/introduction.html</a><br><a href="https://blog.csdn.net/qq_24409555/article/details/76139886" target="_blank" rel="external">https://blog.csdn.net/qq_24409555/article/details/76139886</a><br><a href="https://community.cloudera.com/t5/Batch-SQL-Apache-Hive/Hive-Errors-happend-when-execute-hive-service-metastore/m-p/93050#M3282" target="_blank" rel="external">https://community.cloudera.com/t5/Batch-SQL-Apache-Hive/Hive-Errors-happend-when-execute-hive-service-metastore/m-p/93050#M3282</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>cloudera</tag>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title>启德教育被坑记</title>
    <url>/2018/study_abroad_agency.html</url>
    <content><![CDATA[<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>留学一直都是我的梦想，但因为种种原因毕业后没能实现，工作了几年后，还是放不下，于是打算去香港留学，因为离家里比较近，年纪大了，不方便出走太远。<br>但由于平时要上班工作，没有时间，也不想花太多时间在申请上，于是就打算找中介。在国内，留学中介中知名度<br>最高的是启德吧。我看他们家的广告随处可见，之前也些朋友找的启德做中介的。于是我就选了启德。</p>
<p>当时中介告诉我，已经是年底了，申请的时间比较晚，要赶紧行动。于是中介耐心地给我介绍了我这条件能去的学校和专业，就赶紧交钱申请。<br>当时交了1.5万（还不包括申请费，申请费还得要近3000RMB），6各专业好像。他们是按专业数量收费的。加专业还要加钱。</p>
<p>当时我完全不知道他们是怎么操作的，对留学申请我也完全没有经验和概念。我只是在很多年前考过一次雅思，6分。就这样，我被中介牵着鼻子走了。<br>中介让我干嘛我就干嘛。</p>
<p>中介说让我先准备考雅思。然后在用中文写下各专业的文书，再递给中介让她们翻译。</p>
<p>经过一两个月的努力，我把雅思考到6.5(低分6)了。于是就把中文的文书交给中介，结果他们等了大半个月才翻译出来。翻译出来的质量当时我根本没有看。<br>于是他们就帮我投递了。最先投的是浸会大学，很快就有回复了，于是安排我面试。面试时候，面试官只是简单跟我聊聊，为什么选择这个专业呀，对选择这个program有什么挑战，<br>等等。很快，一个礼拜后就给我发offer了。</p>
<p>收到offer，意味着不能退款了。1.5万就没了。后来等了很久也没等到，其它专业的offer，城大，理工的都没有任何回复，连面试通知都没。很失落，最后也没有接收浸会的offer，<br>没有交留位费。就放弃了当年的入学机会。</p>
<p>时隔半年后，我再次打算申请。我又交了1.2万给启德，当时也是怪自己没有花精力去了解留学申请，太过于依赖中介了。并且，自己的留学欲望又很强。这就导致我很轻易地把这钱交给中介了。<br>我都没怎么犹豫，也不觉得一两万前很多，很果断地打钱过去了。</p>
<p>去年的入学失败，使得我今年必须得抓住机会，不然，很有可能以后都没有机会出去了。于是，我今年的申请盯得特别紧。我第一次认真看启德写的PS文书，对比地看了下985学生申请的文书，<br>那差距啊。。。启德写的文书这是什么玩意啊，简直就是笑话。写的基本上通篇都是废话。开头就是说自己的人生格言，然后是出生家庭什么的。说了一大堆跟自己的能力展现没任何关系的<br>废话内容。于是，我基本上把所有专业的PS文书都重新写了遍。然后是进行艰苦的修改阶段， 改了又改。自己还另外掏了千百块块找了其他人帮忙修改查看。花了好多心血才把终稿定下来了。</p>
<p>TM的我这一切努力都和中介没毛关系啊，我出这么钱，中介没帮我啥事情啊。从备战雅思，去学校申请资料，修改文书PS，都是我自己的努力和艰辛。中介做的就是帮你递交申请资料而已，<br>PS这些虽然他们写了，但质量太差，基本上用不了，还是要你自己重新再写一遍。这些都算了，最让我气愤的是：交完钱后，中介就对你不上心了。比如微信回复不及时回，而且回复的态度<br>完全和你交钱之前的不一样。带电话咨询她，她还说，明天有问题问助理，我明天休息不方便接听电话。这让人听起来感觉是非常不热情，不友好的。</p>
]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>daily</tag>
      </tags>
  </entry>
  <entry>
    <title>华强北被坑记</title>
    <url>/2018/huaqiangbei_black_market.html</url>
    <content><![CDATA[<h2 id="好奇心"><a href="#好奇心" class="headerlink" title="好奇心"></a>好奇心</h2><p>之前有段时间一直为了好奇，想拍摄一些比较隐蔽的探访视频然后放YouTube上赚点广告费。想了很久，终于跑到忍不住<br>跑到华强北去寻找买一个眼镜摄像仪。就是带着摄像头在眼镜上的偷拍神器。去到华强北摄像头市场里，问了好多商铺，<br>他们都不卖，当然是私底下地问，因为这东西肯定不是不能明着卖的。后来还是找了个路边站街拉客的大妈，一问就有了。<br>她让我跟着她走，走到一个稍微人少点地方，她打电话给卖这东西的贩子，然后就在那等那贩子过来。</p>
<p>10分钟后，一个矮矮的中年男子（四五十岁），拿着个黑色大袋子走过来，说带我去一个更少人的角落验货。终于看到了<br>传说中的眼镜拍摄神器。这玩意超出我的预期，并没有我想象中的那么好，因为它一看就有些不太对头，不像一个正常的眼镜，<br>左右两根支架特别粗，因为里面有电路板，还要放置MINI-SD卡。它的摄像头在眼镜的最中间位置，也就是连接两个镜片<br>的地方，不仔细看的话，不会发现，不过稍微仔细点看还是能看出来它是有个摄像头在上面的。</p>
<p>接着，那个贩子拍摄了一段视频，我感觉视频的质量，分别率不是很好，大概就是几百块钱手机的拍摄效果吧。虽然不太满意，<br>但感觉跟那贩子看了有点久，有点骑虎难下，最后还是买下来了，花了750，他开价是900块。终于止住我那段时间以来的好奇心和欲望。</p>
<p>这东西还不带SD卡，于是我还花了几十块钱买了个SD卡。那天总共花了850左右吧，现在想想感觉还是挺不值的，因为拿回来后<br>也就拍了一下，就再也没用过了，感觉就是一个玩具而已，没有什么实际的用途。</p>
<p><img src="/images/2018/huaqiangbei_black_market/36750b47.png" alt=""></p>
<p><img src="/images/2018/huaqiangbei_black_market/8984d475.png" alt=""></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>就当花钱买个教训吧，我记得，我不是我第一次在华强北被坑，以前还很想买一种录音手表，后来以色列人送了我一个，大概是一百多块钱吧，用了几次就扔了。<br>以后还是要注意下自己的消费欲望。买之前再三思考，这东西值不值。不过很多东西其实还是买来后才知道它值不值。</p>
]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
  <entry>
    <title>Git stash的用处</title>
    <url>/2018/git_stash.html</url>
    <content><![CDATA[<h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>1 当正在dev分支上开发某个项目，这时项目中出现一个bug，需要紧急修复，<br>但是正在开发的内容只是完成一半，还不想提交，这时可以用git stash命令将修改的内容保存至堆栈区，<br>然后顺利切换到hotfix分支进行bug修复，修复完成后，再次切回到dev分支，从堆栈中恢复刚刚保存的内容。</p>
<p>2 由于疏忽，本应该在dev分支开发的内容，却在master上进行了开发，需要重新切回到dev分支上进行开发，<br>可以用git stash将内容保存至堆栈中，切回到dev分支后，再次恢复内容即可。</p>
<p>总的来说，git stash命令的作用就是将目前还不想提交的但是已经修改的内容进行保存至堆栈中，后续可以在某个分支上恢复出堆栈中的内容。<br>这也就是说，stash中的内容不仅仅可以恢复到原先开发的分支，也可以恢复到其他任意指定的分支上。<br>git stash作用的范围包括工作区和暂存区中的内容，也就是说没有提交的内容都会保存至堆栈中。</p>
<h2 id="命令详解"><a href="#命令详解" class="headerlink" title="命令详解"></a>命令详解</h2><h3 id="1-git-stash"><a href="#1-git-stash" class="headerlink" title="1 git stash"></a>1 git stash</h3><p>能够将所有未提交的修改（工作区和暂存区）保存至堆栈中，用于后续恢复当前工作目录。<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$ git status</div><div class="line">On branch master</div><div class="line">Changes not staged for commit:</div><div class="line">  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</div><div class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</div><div class="line"></div><div class="line">        modified:   src/main/java/com/wy/CacheTest.java</div><div class="line">        modified:   src/main/java/com/wy/StringTest.java</div><div class="line"></div><div class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</div><div class="line"></div><div class="line">$ git stash</div><div class="line">Saved working directory and index state WIP on master: b2f489c second</div><div class="line"></div><div class="line">$ git status</div><div class="line">On branch master</div><div class="line">nothing to commit, working tree clean</div></pre></td></tr></table></figure></p>
<h3 id="2-git-stash-save"><a href="#2-git-stash-save" class="headerlink" title="2 git stash save"></a>2 git stash save</h3><p>作用等同于git stash，区别是可以加一些注释，如下：<br>git stash的效果：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">stash@&#123;0&#125;: WIP on master: b2f489c second</div></pre></td></tr></table></figure></p>
<p>git stash save “test1”的效果：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">stash@&#123;0&#125;: On master: test1</div></pre></td></tr></table></figure></p>
<h3 id="3-git-stash-list"><a href="#3-git-stash-list" class="headerlink" title="3. git stash list"></a>3. git stash list</h3><p>查看当前stash中的内容</p>
<h3 id="4-git-stash-pop"><a href="#4-git-stash-pop" class="headerlink" title="4 git stash pop"></a>4 git stash pop</h3><p>将当前stash中的内容弹出，并应用到当前分支对应的工作目录上。<br>注：该命令将堆栈中最近保存的内容删除（栈是先进后出）<br>顺序执行git stash save “test1”和git stash save “test2”命令，效果如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$ git stash list</div><div class="line">stash@&#123;0&#125;: On master: test2</div><div class="line">stash@&#123;1&#125;: On master: test1</div><div class="line"></div><div class="line"></div><div class="line">$ git stash pop</div><div class="line">On branch master</div><div class="line">Changes not staged for commit:</div><div class="line">  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</div><div class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</div><div class="line"></div><div class="line">        modified:   src/main/java/com/wy/StringTest.java</div><div class="line"></div><div class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</div><div class="line">Dropped refs/stash@&#123;0&#125; (afc530377eacd4e80552d7ab1dad7234edf0145d)</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$ git stash list</div><div class="line">stash@&#123;0&#125;: On master: test1</div></pre></td></tr></table></figure>
<p>可见，test2的stash是首先pop出来的。<br>如果从stash中恢复的内容和当前目录中的内容发生了冲突，也就是说，恢复的内容和当前目录修改了同一行的数据，那么会提示报错，需要解决冲突，可以通过创建新的分支来解决冲突。</p>
<h3 id="5-git-stash-apply"><a href="#5-git-stash-apply" class="headerlink" title="5 git stash apply"></a>5 git stash apply</h3><p>将堆栈中的内容应用到当前目录，不同于git stash pop，该命令不会将内容从堆栈中删除，也就说该命令能够将堆栈的内容多次应用到工作目录中，适应于多个分支的情况。<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$ git stash apply</div><div class="line">On branch master</div><div class="line">Changes not staged for commit:</div><div class="line">  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</div><div class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</div><div class="line"></div><div class="line">        modified:   src/main/java/com/wy/StringTest.java</div><div class="line"></div><div class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$ git stash list</div><div class="line">stash@&#123;0&#125;: On master: test2</div><div class="line">stash@&#123;1&#125;: On master: test1</div><div class="line">堆栈中的内容并没有删除。</div></pre></td></tr></table></figure>
<p>可以使用git stash apply + stash名字（如stash@{1}）指定恢复哪个stash到当前的工作目录。</p>
<h3 id="6-git-stash-drop-名称"><a href="#6-git-stash-drop-名称" class="headerlink" title="6 git stash drop + 名称"></a>6 git stash drop + 名称</h3><p>从堆栈中移除某个指定的stash</p>
<h3 id="7-git-stash-clear"><a href="#7-git-stash-clear" class="headerlink" title="7 git stash clear"></a>7 git stash clear</h3><p>清除堆栈中的所有 内容</p>
<h3 id="8-git-stash-show"><a href="#8-git-stash-show" class="headerlink" title="8 git stash show"></a>8 git stash show</h3><p>查看堆栈中最新保存的stash和当前目录的差异。<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$ git stash show</div><div class="line"> src/main/java/com/wy/StringTest.java | 2 +-</div><div class="line">  file changed, 1 insertion(+), 1 deletion(-)</div></pre></td></tr></table></figure></p>
<p>git stash show stash@{1}查看指定的stash和当前目录差异。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://blog.csdn.net/stone_yw/article/details/80795669" target="_blank" rel="external">https://blog.csdn.net/stone_yw/article/details/80795669</a></p>
]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu的.so文件问题</title>
    <url>/2018/ubuntu_shared_object.html</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>昨天在anaconda的虚拟环境py3 下装uwsgi</p>
<p>但执行uwsgi命令的时候报错提示：<br>uwsgi: error while loading shared libraries: libpcre.so.1: cannot open…</p>
<p>找不到libpcre.so.1 这个动态链接库。</p>
<p>这个东西叫做 动态链接库。</p>
<ul>
<li>An .so file is a compiled library file. It stands for “Shared Object” and is analogous to a Windows DLL</li>
</ul>
<h3 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h3><p>首先我学习了几个linux命令：</p>
<ol>
<li>ldd命令用于打印程序或者库文件所依赖的共享库列表。<br>2.ldconfig命令的用途主要是在默认搜寻目录/lib和/usr/lib以及动态库配置文件/etc/ld.so.conf内所列的目录下，搜索出可共享的动态链接库（格式如lib<em>.so</em>）,进而创建出动态装入程序(ld.so)所需的连接和缓存文件。<br>3.locate 让使用者可以很快速的搜寻档案系统内是否有指定的档案。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ldconfig /opt/anaconda3/lib/</div><div class="line">ldd /data/software/anaconda2/bin/uwsgi</div><div class="line">locate libcrypto.so.1.0.0</div><div class="line"></div><div class="line">libcrypto.so.1.0.0 =&gt; /data/software/anaconda2/lib/libcrypto.so.1.0.0的  (0x00007f90b5695000)</div><div class="line">表示libcrypto.so.1.0.0 用的是 /data/software/anaconda2/lib/libcrypto.so.1.0.0的 动态 链接库</div><div class="line">用locate libcrypto.so.1.0.0 查看系统一共有几个 libcrypto.so.1.0.0 动态链接库</div></pre></td></tr></table></figure>
<p>ldconfig的几点注意事项：</p>
<ol>
<li>往/lib和/usr/lib里面加东西，是不用修改/etc/ld.so.conf的，但是完了之后要调一下ldconfig，不然这个library会找不到。</li>
<li>想往上面两个目录以外加东西的时候，一定要修改/etc/ld.so.conf，然后再调用ldconfig，不然也会找不到。</li>
<li>比如安装了一个mysql到/usr/local/mysql，mysql有一大堆library在/usr/local/mysql/lib下面，这时就需要在/etc/ld.so.conf下面加一行/usr/local/mysql/lib，保存过后ldconfig一下，新的library才能在程序运行时被找到。</li>
<li>如果想在这两个目录以外放lib，但是又不想在/etc/ld.so.conf中加东西（或者是没有权限加东西）。那也可以，就是export一个全局变量LD_LIBRARY_PATH，然后运行程序的时候就会去这个目录中找library。一般来讲这只是一种临时的解决方案，在没有权限或临时需要的时候使用</li>
</ol>
<p>in my case:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">add the  line below in /etc/profile:</div><div class="line">export LD_LIBRARY_PATH=/data/software/anaconda2/lib</div><div class="line"></div><div class="line">ldconfig /lib/x86_64-linux-gnu/</div></pre></td></tr></table></figure></p>
<p>finished!</p>
<blockquote>
<p>另外 LD_LIBRARY_PATH的优先级是最高的。</p>
</blockquote>
<p>The order is documented in the manual of the dynamic linker, which is ld.so. It is:</p>
<ol>
<li>directories from LD_LIBRARY_PATH;</li>
<li>directories from /etc/ld.so.conf;</li>
<li>/lib;</li>
<li>/usr/lib.<br>(I’m simplifying a little, see the manual for the full details.)</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">The order makes sense when you consider that it&apos;s the only way to override a library in a default location with a custom library.</div><div class="line">LD_LIBRARY_PATH is a user setting, it has to come before the others. /etc/ld.so.conf is a local setting, it comes before the operating system default.</div><div class="line">So as a user, if I want to run a program with a different version of a library, I can run the program with LD_LIBRARY_PATH containing the location of that different library version.</div><div class="line">And as an administrator, I can put a different version of the library in /usr/local/lib and list /usr/local/lib in /etc/ld.so.conf.</div></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive UDF 快速教程</title>
    <url>/2018/hive_udf.html</url>
    <content><![CDATA[<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="Step1"><a href="#Step1" class="headerlink" title="Step1"></a>Step1</h3><p>首先，我们创建一个目录 udf_test/;<br>创建子目录org/dennis/udf<br>在子目录里创建一个MyUpper.java文件。里面内容为：<br><figure class="highlight java"><table><tr><td class="code"><pre><div class="line"><span class="keyword">package</span> org.dennis.udf;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">MyUpper</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(<span class="keyword">final</span> Text s)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (s == <span class="keyword">null</span>) &#123; <span class="keyword">return</span> <span class="keyword">null</span>; &#125;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Text(s.toString().toUpperCase());</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="Step2"><a href="#Step2" class="headerlink" title="Step2"></a>Step2</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">javac -cp  \</div><div class="line">/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/jars/hive-exec-2.1.1-cdh6.2.0.jar: \</div><div class="line">/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/jars/hadoop-common-3.0.0-cdh6.2.0.jar \</div><div class="line">org/dennis/udf/MyUpper.java</div></pre></td></tr></table></figure>
<p>这里我们要把依赖的jar包写进来，这里我们依赖hive-exec*.jar 以及 hadoop-common.jar</p>
<ul>
<li>javac -cp 的作用  <figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">javac -cp 指明了.java文件里import的类的位置</div><div class="line"></div><div class="line">java -cp 指明了执行这个class文件所需要的所有类的包路径-即系统类加载器的路径（涉及到类加载机制）</div><div class="line"></div><div class="line">路径在linux中用：隔开  在windows中用；隔开</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Step3"><a href="#Step3" class="headerlink" title="Step3"></a>Step3</h3><p>生成一个jar文件。<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">jar -cf myudfs.jar  -C . .</div><div class="line">会在当前目录下生成myudfs.jar 文件</div></pre></td></tr></table></figure></p>
<h3 id="Step4"><a href="#Step4" class="headerlink" title="Step4"></a>Step4</h3><h4 id="第一种方式"><a href="#第一种方式" class="headerlink" title="第一种方式"></a>第一种方式</h4><p>进入Hive命令行<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">hive&gt; add jar myudfs.jar;</div><div class="line">Added [myudfs.jar] to class path</div><div class="line">Added resources: [myudfs.jar]</div><div class="line">hive&gt; create temporary function dennisUpper as &apos;org.dennis.udf.MyUpper&apos;;</div><div class="line">OK</div><div class="line">Time taken: 0.067 seconds</div><div class="line">hive&gt;</div></pre></td></tr></table></figure></p>
<p>搞定！</p>
<h4 id="第二种方式"><a href="#第二种方式" class="headerlink" title="第二种方式"></a>第二种方式</h4><p>另外，也可以在Hue上点击 setting后，上传对应的jar文件，然后将对应的function name 和 class name也填一下就OK了。<br><img src="/images/2018/hive_udf/hue_udf_upload.jpg" alt="Sample Image Added via Markdown"></p>
<p>然后再Hue上执行<code>create temporary function dennisUpper as &#39;org.dennis.udf.MyUpper&#39;;</code>  注册下对应信息。</p>
<h3 id="Step5"><a href="#Step5" class="headerlink" title="Step5"></a>Step5</h3><p>执行SQL，检验一下:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">select dennisUpper(field) from table;</div></pre></td></tr></table></figure></p>
<h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><h3 id="找不到类"><a href="#找不到类" class="headerlink" title="找不到类"></a>找不到类</h3><p>有时候我们add jar后，create function时候会出现如下报错：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Added resources: [myudfs.jar]</div><div class="line">hive&gt; create temporary function dennisUpper as &apos;org.dennis.udf.MyUpper&apos;;</div><div class="line">FAILED: Class org.dennis.udf.MyUpper not found</div><div class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.FunctionTask</div></pre></td></tr></table></figure></p>
<p>我们可以解压一下jar包检查下:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">unzip myudf.jar -d check_jar</div><div class="line">解压到 check_jar目录</div></pre></td></tr></table></figure></p>
<p>如果有的话，check_dir下应该是有这么一个目录文件:<br><code>org/dennis/udf/MyUpper.class</code></p>
<h3 id="reload更新的jar包"><a href="#reload更新的jar包" class="headerlink" title="reload更新的jar包"></a>reload更新的jar包</h3><p>我们更新了UDF，重新打个jar包替换掉原来的。但如果要让它生效需要做一些操作：</p>
<ol>
<li><p>added a config in hive-site.xml, then restart the hive server.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.reloadable.aux.jars.path&lt;/name&gt;</div><div class="line">    &lt;value&gt;/user/hive/udf&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
</li>
<li><p>deleted the old jar file in HDFS, and upload the new jar file.</p>
</li>
<li><p>DROP TEMPORARY FUNCTION IF EXISTS isstopword;</p>
</li>
<li><p>in hive console, run <code>list jar;</code> to check the local jar files, it would print something like this:</p>
</li>
</ol>
<p><code>/tmp/83ce8586-7311-4e97-813f-f2fbcec63a55_resources/isstopwordudf.jar</code><br>then delete them in your server file system.</p>
<ol>
<li>create a temp function again.<br>create temporary function isstopword as ‘org.dennis.udf.IsStopWord’;</li>
</ol>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="http://bdlabs.edureka.co/static/help/topics/cm_mc_hive_udf.html#concept_zb2_rxr_lw_unique_1" target="_blank" rel="external">http://bdlabs.edureka.co/static/help/topics/cm_mc_hive_udf.html#concept_zb2_rxr_lw_unique_1</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>UDF</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive 优化</title>
    <url>/2018/hive_optimization.html</url>
    <content><![CDATA[<p>对于如果join中有小表的话，可以开启map</p>
<h3 id="Dynamic-Partition-Pruning-for-Hive-Map-Joins"><a href="#Dynamic-Partition-Pruning-for-Hive-Map-Joins" class="headerlink" title="Dynamic Partition Pruning for Hive Map Joins"></a>Dynamic Partition Pruning for Hive Map Joins</h3><p>You can enable dynamic partition pruning for map joins when you are running Hive on Spark (HoS), it is not available for Hive on MapReduce.<br>Dynamic partition pruning (DPP) is a database optimization that can significantly decrease the amount of data that a query scans, thereby executing your workloads faster.<br>DPP achieves this by dynamically determining and eliminating the number of partitions that a query must read from a partitioned table.</p>
<p>Map joins also optimize how Hive executes queries. They cause a small table to be scanned and loaded in memory as a hash table<br>so that a fast join can be performed entirely within a mapper without having to use another reduce step.<br>If you have queries that join small tables, map joins can make them execute much faster.<br>Map joins are enabled by default in CDH with the Enable MapJoin Optimization setting for HiveServer2 in Cloudera Manager.<br>Hive automatically uses map joins for join queries that involve a set of tables where:</p>
<ul>
<li>There is one large table and there is no limit on the size of that large table.</li>
<li>All other tables involved in the join must have an aggregate size under the value set for Hive Auto Convert Join Noconditional Size for HiveServer2, which is set to 20MB by default in Cloudera Manager.</li>
</ul>
<p>关于map-side join的配置:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">SET hive.auto.convert.join=true;</div><div class="line">SET hive.auto.convert.join.noconditionaltask.size=&lt;number_in_megabytes&gt;;</div></pre></td></tr></table></figure></p>
<h2 id="一次调优实战"><a href="#一次调优实战" class="headerlink" title="一次调优实战"></a>一次调优实战</h2><p>最近在ETL过程中发现有条SQL执行时间非常长，其实数据量很小的，但为什么这么长呢。我带着极度好奇，抱着死缠烂打的精神，怎么也要把<br>问题给解决掉。SQL是这样的:<br><figure class="highlight sql"><table><tr><td class="code"><pre><div class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">from</span> zhihu_answer </div><div class="line"><span class="keyword">where</span> ym <span class="keyword">in</span> (<span class="keyword">select</span> <span class="keyword">distinct</span>(ym) <span class="keyword">from</span> zhihu.zhihu_answer_increment);</div></pre></td></tr></table></figure></p>
<p>先说说这两个表数据量吧：<br>zhihu_answer数据量大概是一亿，zhihu_answer_increment 也就是几十万条。<br>首先，我用<code>explain extended</code>查看下执行计划:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">explain extended</div><div class="line">select count(1) from zhihu_answer </div><div class="line">where ym in (select distinct(ym) from zhihu.zhihu_answer_increment);</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">Explain	</div><div class="line">STAGE DEPENDENCIES:	</div><div class="line">  Stage-3 is a root stage	</div><div class="line">  Stage-5 depends on stages: Stage-3 , consists of Stage-6, Stage-1	</div><div class="line">  Stage-6 has a backup stage: Stage-1	</div><div class="line">  Stage-4 depends on stages: Stage-6	</div><div class="line">  Stage-2 depends on stages: Stage-1, Stage-4	</div><div class="line">  Stage-1	</div><div class="line">  Stage-0 depends on stages: Stage-2	</div><div class="line">	</div><div class="line">STAGE PLANS:	</div><div class="line">  Stage: Stage-3	</div><div class="line">    Map Reduce	</div><div class="line">      Map Operator Tree:	</div><div class="line">          TableScan	</div><div class="line">            alias: zhihu_answer_increment	</div><div class="line">            filterExpr: ym is not null (type: boolean)	</div><div class="line">            Statistics: Num rows: 347468 Data size: 73315748 Basic stats: COMPLETE Column stats: COMPLETE	</div><div class="line">            GatherStats: false	</div><div class="line">            Select Operator	</div><div class="line">              expressions: ym (type: string)	</div><div class="line">              outputColumnNames: ym	</div><div class="line">              Statistics: Num rows: 347468 Data size: 73315748 Basic stats: COMPLETE Column stats: COMPLETE	</div><div class="line">              Group By Operator	</div><div class="line">                keys: ym (type: string)	</div><div class="line">                mode: hash	</div><div class="line">                outputColumnNames: _col0	</div><div class="line">                Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE	</div><div class="line">                Reduce Output Operator	</div><div class="line">                  key expressions: _col0 (type: string)	</div><div class="line">                  null sort order: a	</div><div class="line">                  sort order: +	</div><div class="line">                  Map-reduce partition columns: _col0 (type: string)	</div><div class="line">                  Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE	</div><div class="line">                  tag: -1	</div><div class="line">                  auto parallelism: false	</div><div class="line">      Execution mode: vectorized	</div><div class="line">      Path -&gt; Alias:	</div><div class="line">        nullscan://null/zhihu.zhihu_answer_increment/part_ym=201902_ [sq_1:zhihu_answer_increment]	</div><div class="line">      Path -&gt; Partition:	</div><div class="line">        nullscan://null/zhihu.zhihu_answer_increment/part_ym=201902_ 	</div><div class="line">          Partition	</div><div class="line">            input format: org.apache.hadoop.hive.ql.io.OneNullRowInputFormat	</div><div class="line">            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	</div><div class="line">            partition values:	</div><div class="line">              ym 201902	</div><div class="line">            properties:	</div><div class="line">              COLUMN_STATS_ACCURATE &#123;&quot;BASIC_STATS&quot;:&quot;true&quot;&#125;	</div><div class="line">              bucket_count 256	</div><div class="line">              bucket_field_name answer_id	</div><div class="line">              columns admin_closed_comment,answer_content,answer_created,answer_id,answer_updated,author_headline,author_id,author_name,author_type,author_url_token,avatar_url,badge_num,can_comment,comment_count,gender,insert_time,is_advertiser,is_collapsed,is_copyable,is_org,question_created,question_id,question_title,question_type,reward_member_count,reward_total_money,voteup_count	</div><div class="line">              columns.comments 	</div><div class="line">              columns.types boolean:string:string:string:string:string:string:string:string:string:string:smallint:boolean:int:string:string:boolean:boolean:boolean:boolean:string:string:string:string:int:int:int	</div><div class="line">              file.inputformat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat	</div><div class="line">              file.outputformat org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat	</div><div class="line">              location hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer_increment/ym=201902	</div><div class="line">              name zhihu.zhihu_answer_increment	</div><div class="line">              numFiles 256	</div><div class="line">              numRows 347468	</div><div class="line">              partition_columns ym	</div><div class="line">              partition_columns.types string	</div><div class="line">              rawDataSize 9381636	</div><div class="line">              serialization.ddl struct zhihu_answer_increment &#123; bool admin_closed_comment, string answer_content, string answer_created, string answer_id, string answer_updated, string author_headline, string author_id, string author_name, string author_type, string author_url_token, string avatar_url, i16 badge_num, bool can_comment, i32 comment_count, string gender, string insert_time, bool is_advertiser, bool is_collapsed, bool is_copyable, bool is_org, string question_created, string question_id, string question_title, string question_type, i32 reward_member_count, i32 reward_total_money, i32 voteup_count&#125;	</div><div class="line">              serialization.format 1	</div><div class="line">              serialization.lib org.apache.hadoop.hive.serde2.NullStructSerDe	</div><div class="line">              totalSize 433473813	</div><div class="line">              transient_lastDdlTime 1571983508	</div><div class="line">            serde: org.apache.hadoop.hive.serde2.NullStructSerDe	</div><div class="line">          	</div><div class="line">              input format: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat	</div><div class="line">              output format: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat	</div><div class="line">              properties:	</div><div class="line">                bucket_count 256	</div><div class="line">                bucket_field_name answer_id	</div><div class="line">                columns admin_closed_comment,answer_content,answer_created,answer_id,answer_updated,author_headline,author_id,author_name,author_type,author_url_token,avatar_url,badge_num,can_comment,comment_count,gender,insert_time,is_advertiser,is_collapsed,is_copyable,is_org,question_created,question_id,question_title,question_type,reward_member_count,reward_total_money,voteup_count	</div><div class="line">                columns.comments 	</div><div class="line">                columns.types boolean:string:string:string:string:string:string:string:string:string:string:smallint:boolean:int:string:string:boolean:boolean:boolean:boolean:string:string:string:string:int:int:int	</div><div class="line">                file.inputformat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat	</div><div class="line">                file.outputformat org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat	</div><div class="line">                location hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer_increment	</div><div class="line">                name zhihu.zhihu_answer_increment	</div><div class="line">                partition_columns ym	</div><div class="line">                partition_columns.types string	</div><div class="line">                serialization.ddl struct zhihu_answer_increment &#123; bool admin_closed_comment, string answer_content, string answer_created, string answer_id, string answer_updated, string author_headline, string author_id, string author_name, string author_type, string author_url_token, string avatar_url, i16 badge_num, bool can_comment, i32 comment_count, string gender, string insert_time, bool is_advertiser, bool is_collapsed, bool is_copyable, bool is_org, string question_created, string question_id, string question_title, string question_type, i32 reward_member_count, i32 reward_total_money, i32 voteup_count&#125;	</div><div class="line">                serialization.format 1	</div><div class="line">                serialization.lib org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe	</div><div class="line">                transient_lastDdlTime 1571983018	</div><div class="line">              serde: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe	</div><div class="line">              name: zhihu.zhihu_answer_increment	</div><div class="line">            name: zhihu.zhihu_answer_increment	</div><div class="line">      Truncated Path -&gt; Alias:	</div><div class="line">        nullscan://null/zhihu.zhihu_answer_increment/part_ym=201902_ [sq_1:zhihu_answer_increment]	</div><div class="line">      Needs Tagging: false	</div><div class="line">      Reduce Operator Tree:	</div><div class="line">        Group By Operator	</div><div class="line">          keys: KEY._col0 (type: string)	</div><div class="line">          mode: mergepartial	</div><div class="line">          outputColumnNames: _col0	</div><div class="line">          Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE	</div><div class="line">          Group By Operator	</div><div class="line">            keys: _col0 (type: string)	</div><div class="line">            mode: hash	</div><div class="line">            outputColumnNames: _col0	</div><div class="line">            Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE	</div><div class="line">            File Output Operator	</div><div class="line">              compressed: false	</div><div class="line">              GlobalTableId: 0	</div><div class="line">              directory: hdfs://device1:8020/tmp/hive/hive/7f0887a3-8c5a-44b6-b5ef-f0c7530a6b15/hive_2019-10-25_14-46-02_198_8962679143430564511-1/-mr-10004	</div><div class="line">              NumFilesPerFileSink: 1	</div><div class="line">              table:	</div><div class="line">                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat	</div><div class="line">                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat	</div><div class="line">                  properties:	</div><div class="line">                    columns _col0	</div><div class="line">                    columns.types string	</div><div class="line">                    escape.delim \	</div><div class="line">                    serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe	</div><div class="line">                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe	</div><div class="line">              TotalFiles: 1	</div><div class="line">              GatherStats: false	</div><div class="line">              MultiFileSpray: false	</div><div class="line">	</div><div class="line">  Stage: Stage-5	</div><div class="line">    Conditional Operator	</div><div class="line">	</div><div class="line">  Stage: Stage-6	</div><div class="line">    Map Reduce Local Work	</div><div class="line">      Alias -&gt; Map Local Tables:	</div><div class="line">        $INTNAME 	</div><div class="line">          Fetch Operator	</div><div class="line">            limit: -1	</div><div class="line">      Alias -&gt; Map Local Operator Tree:	</div><div class="line">        $INTNAME 	</div><div class="line">          TableScan	</div><div class="line">            GatherStats: false	</div><div class="line">            HashTable Sink Operator	</div><div class="line">              keys:	</div><div class="line">                0 ym (type: string)	</div><div class="line">                1 _col0 (type: string)	</div><div class="line">              Position of Big Table: 0	</div><div class="line">	</div><div class="line">  Stage: Stage-4	</div><div class="line">    Map Reduce	</div><div class="line">      Map Operator Tree:	</div><div class="line">          TableScan	</div><div class="line">            alias: zhihu_answer	</div><div class="line">            filterExpr: ym is not null (type: boolean)	</div><div class="line">            Statistics: Num rows: 102075765 Data size: 21537986328 Basic stats: COMPLETE Column stats: COMPLETE	</div><div class="line">            GatherStats: false	</div><div class="line">            Map Join Operator	</div><div class="line">              condition map:	</div><div class="line">                   Left Semi Join 0 to 1	</div><div class="line">              keys:	</div><div class="line">                0 ym (type: string)	</div><div class="line">                1 _col0 (type: string)	</div><div class="line">              Position of Big Table: 0	</div><div class="line">              Statistics: Num rows: 4253156 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE	</div><div class="line">              Group By Operator	</div><div class="line">                aggregations: count(1)	</div><div class="line">                mode: hash	</div><div class="line">                outputColumnNames: _col0	</div><div class="line">                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE	</div><div class="line">                File Output Operator	</div><div class="line">                  compressed: false	</div><div class="line">                  GlobalTableId: 0	</div><div class="line">                  directory: hdfs://device1:8020/tmp/hive/hive/7f0887a3-8c5a-44b6-b5ef-f0c7530a6b15/hive_2019-10-25_14-46-02_198_8962679143430564511-1/-mr-10003	</div><div class="line">                  NumFilesPerFileSink: 1	</div><div class="line">                  table:	</div><div class="line">                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat	</div><div class="line">                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat	</div><div class="line">                      properties:	</div><div class="line">                        columns _col0	</div><div class="line">                        columns.types bigint	</div><div class="line">                        escape.delim \	</div><div class="line">                        serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe	</div><div class="line">                      serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe	</div><div class="line">                  TotalFiles: 1	</div><div class="line">                  GatherStats: false	</div><div class="line">                  MultiFileSpray: false	</div><div class="line">      Local Work:	</div><div class="line">        Map Reduce Local Work	</div><div class="line">      Path -&gt; Alias:	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201705 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201706 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201707 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201708 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201709 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201710 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201711 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201712 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201801 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201802 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201803 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201804 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201805 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201806 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201807 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201808 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201809 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201810 [zhihu_answer]	</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201811 [zhihu_answer]</div></pre></td></tr></table></figure></p>
<p>一脸懵逼，不会看呀。。<br>然后我测试了下单独执行：<code>select distinct(ym) from zhihu_answer_increment;</code>，也就不到2分钟就出结果了。为什么组合在一起就要这么长时间呢？？<br>这条SQL的执行结果就是<code>&quot;201902&quot;</code>。我把这个结果复制进去执行：<br><figure class="highlight sql"><table><tr><td class="code"><pre><div class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">from</span> zhihu_answer <span class="keyword">where</span> ym <span class="keyword">in</span> (<span class="number">201902</span>);</div></pre></td></tr></table></figure></p>
<p>可能是因为我之前执行过的原因，这条语句的执行时间基本上是秒出呀。几秒内就出结果了。</p>
<p>我再一次执行了那句执行时间很长的SQL，看它的执行时候的log，我发现慢原因是在Stage-4 ！！！回到上面那个explain的信息，我发现Hive在做全表扫描呀！Why?<br>为什么要做全表扫描呢？ 因为Hive还是要join的 in （select ** ） 这种子查询中用的是semi join，所以要进行join，它就会进行全表扫描。我的解释不是很详细，<br>但隐隐约约我能理解为什么Hive在这要做全表扫描了，其实如果写死的话，比如where ym in （201902）它就不会做join，也就不用全表扫描了。所以解决方案还是要能<br>拿到 <code>201902</code>这个变量，这个value，再拼接到Hive SQL中。我查了下，Hive貌似目前还不支持以SQL查询结果作为新的SQL变量。所以，暂时还是以这种办法解决吧。</p>
<p>让我无比开心的是，改进后，SQL执行快了N倍，因为避免了全表扫描。从原来2个小时的执行，变为了几分钟！</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过Explain打印看看执行计划有哪些；<br>通过执行的log看看到底是哪个Stage耗时比较长；</p>
<h1 id="Referrence"><a href="#Referrence" class="headerlink" title="Referrence"></a>Referrence</h1><p><a href="https://docs.cloudera.com/documentation/enterprise/latest/topics/admin_hos_oview.html#concept_i22_l1h_1v" target="_blank" rel="external">https://docs.cloudera.com/documentation/enterprise/latest/topics/admin_hos_oview.html#concept_i22_l1h_1v</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Git rebase的一些运用场景</title>
    <url>/2018/git_rebase.html</url>
    <content><![CDATA[<h2 id="Git-usage"><a href="#Git-usage" class="headerlink" title="Git usage"></a>Git usage</h2><p>最近在为github开源项目修修bug。提了几个Pull Request。又巩固了下git的知识，熟练了git的使用。</p>
<p>我在提交PR后，发现有个地方改错了。那怎么改呢，其实很简单，只需要再次push 到自己fork的repo就可以了。</p>
<p>PR的branch是和自己fork的repo的branch联动的。</p>
<p>但是，这又有个问题，我再次commit后，就会有两个commit。这两个commit其实解决的都是同一个问题。<br>在正式的repo中尽量要精简commit，同一个问题的多个commit需要squash为一个commit。</p>
<p>于是，我就得用git rebase 来帮我完成了。</p>
<p>通过这个命令，可以重新修改最近的两个commit<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">git rebase -i HEAD~2</div></pre></td></tr></table></figure></p>
<p>执行命令后会弹出一个vim编辑:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">pick 01d1124 Message....</div><div class="line">pick 6340aaa Message....</div></pre></td></tr></table></figure></p>
<p>然后，将要squash的commit的那行最前面的<code>pick</code> 改为 <code>squash</code><br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">p 01d1124 Message....</div><div class="line">s 6340aaa Message....</div></pre></td></tr></table></figure></p>
<h3 id="改commit名字"><a href="#改commit名字" class="headerlink" title="改commit名字"></a>改commit名字</h3><p>大项目的commit的名字都有比较严格的要求，有些时候，我们要改下自己不规范的命名的话。<br>可以也可以用 git rebase.</p>
<p>很简单，同样先用git rebase -i HEAD~n<br>在需要改的commit的最前面把 <code>pick</code> 改为<code>reword</code><br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">pick 01d1124 Message....</div><div class="line">reword 6340aaa Message....</div></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka Introduction</title>
    <url>/2018/kafka_intro.html</url>
    <content><![CDATA[<ul>
<li>最近面试经常会被问到Kafka的问题。看了很多Kafka的介绍，发现还是美团技术团队总结的最清楚易懂。下面转了美团的关于Kafka的一篇文章</li>
</ul>
<h2 id="Kafka是什么"><a href="#Kafka是什么" class="headerlink" title="Kafka是什么"></a>Kafka是什么</h2><p>Kafka是最初由Linkedin公司开发，是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统(也可以当做MQ系统)，常见可以用于web/nginx日志、访问日志，消息服务等等，Linkedin于2010年贡献给了Apache基金会并成为顶级开源项目。</p>
<p>一个商业化消息队列的性能好坏，其文件存储机制设计是衡量一个消息队列服务技术水平和最关键指标之一。 下面将从Kafka文件存储机制和物理结构角度，分析Kafka是如何实现高效文件存储，及实际应用效果。</p>
<p>Kafka部分名词解释如下：</p>
<ul>
<li>Broker：消息中间件处理结点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群。</li>
<li>Topic：一类消息，例如page view日志、click日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发。</li>
<li>Partition：topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。</li>
<li>Segment：partition物理上由多个segment组成，下面2.2和2.3有详细说明。</li>
<li>offset：每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中。partition中的每个消息都有一个连续的序列号叫做offset,用于partition唯一标识一条消息.</li>
</ul>
<p>分析过程分为以下4个步骤：</p>
<ul>
<li>topic中partition存储分布</li>
<li>partiton中文件存储方式</li>
<li>partiton中segment文件存储结构</li>
<li>在partition中如何通过offset查找message</li>
</ul>
<p>通过上述4过程详细分析，我们就可以清楚认识到kafka文件存储机制的奥秘。</p>
<h2 id="2-1-topic中partition存储分布"><a href="#2-1-topic中partition存储分布" class="headerlink" title="2.1 topic中partition存储分布"></a>2.1 topic中partition存储分布</h2><p>假设实验环境中Kafka集群只有一个broker，xxx/message-folder为数据文件存储根目录，在Kafka broker中server.properties文件配置(参数log.dirs=xxx/message-folder)，例如创建2个topic名称分别为report_push、launch_info, partitions数量都为partitions=4 存储路径和目录规则为： xxx/message-folder<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">|--report_push-0</div><div class="line">|--report_push-1</div><div class="line">|--report_push-2</div><div class="line">|--report_push-3</div><div class="line">|--launch_info-0</div><div class="line">|--launch_info-1</div><div class="line">|--launch_info-2</div><div class="line">|--launch_info-3</div></pre></td></tr></table></figure></p>
<p>在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。 如果是多broker分布情况，请参考kafka集群partition分布原理分析</p>
<h2 id="2-2-partiton中文件存储方式"><a href="#2-2-partiton中文件存储方式" class="headerlink" title="2.2 partiton中文件存储方式"></a>2.2 partiton中文件存储方式</h2><p>下面示意图形象说明了partition中文件存储方式:</p>
<p><img src="/images/2018/kafka_intro/kafka_partition.png" alt="Sample Image Added via Markdown"></p>
<ul>
<li>每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。但每个段segment file消息数量不一定相等，这种特性方便old segment file快速被删除。</li>
<li>每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。</li>
</ul>
<p>这样做的好处就是能快速删除无用文件，有效提高磁盘利用率。</p>
<h2 id="2-3-partiton中segment文件存储结构"><a href="#2-3-partiton中segment文件存储结构" class="headerlink" title="2.3 partiton中segment文件存储结构"></a>2.3 partiton中segment文件存储结构</h2><p>读者从2.2节了解到Kafka文件系统partition存储方式，本节深入分析partion中segment file组成和物理结构。</p>
<ul>
<li>segment file组成：由2大部分组成，分别为index file和data file，此2个文件一一对应，成对出现，后缀”.index”和“.log”分别表示为segment索引文件、数据文件.</li>
<li>segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值。数值最大为64位long大小，19位数字字符长度，没有数字用0填充。</li>
</ul>
<p>下面文件列表是笔者在Kafka broker上做的一个实验，创建一个topicXXX包含1 partition，设置每个segment大小为500MB,并启动producer向Kafka broker写入大量数据,如下图2所示segment文件列表形象说明了上述2个规则：</p>
<p><img src="/images/2018/kafka_intro/kafka_index.png" alt="Sample Image Added via Markdown"></p>
<p>以上述图2中一对segment file文件为例，说明segment中index&lt;—-&gt;data file对应关系物理结构如下：</p>
<p>上述图3中索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。 其中以索引文件中元数据3,497为例，依次在数据文件中表示第3个message(在全局partiton表示第368772个message)、以及该消息的物理偏移地址为497。</p>
<p>从上述图3了解到segment data file由许多message组成，下面详细说明message物理结构如下：<br><img src="/images/2018/kafka_intro/kafka_message.png" alt="Sample Image Added via Markdown"></p>
<p>参数说明：<br>关键字    解释说明<br>8 byte offset    在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message<br>4 byte message size    message大小<br>4 byte CRC32    用crc32校验message<br>1 byte “magic”    表示本次发布Kafka服务程序协议版本号<br>1 byte “attributes”    表示为独立版本、或标识压缩类型、或编码类型。<br>4 byte key length    表示key的长度,当key为-1时，K byte key字段不填<br>K byte key    可选<br>value bytes payload    表示实际消息数据。</p>
<h2 id="2-4-在partition中如何通过offset查找message"><a href="#2-4-在partition中如何通过offset查找message" class="headerlink" title="2.4 在partition中如何通过offset查找message"></a>2.4 在partition中如何通过offset查找message</h2><p>例如读取offset=368776的message，需要通过下面2个步骤查找。</p>
<p>第一步查找segment file 上述图2为例，其中00000000000000000000.index表示最开始的文件，起始偏移量(offset)为0.第二个文件00000000000000368769.index的消息量起始偏移量为368770 = 368769 + 1.同样，第三个文件00000000000000737337.index的起始偏移量为737338=737337 + 1，其他后续文件依次类推，以起始偏移量命名并排序这些文件，只要根据offset <strong>二分查找</strong>文件列表，就可以快速定位到具体文件。 当offset=368776时定位到00000000000000368769.index|log</p>
<p>第二步通过segment file查找message 通过第一步定位到segment file，当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和00000000000000368769.log的物理偏移地址，然后再通过00000000000000368769.log顺序查找直到offset=368776为止。</p>
<p>从上述图3可知这样做的优点，segment index file采取稀疏索引存储方式，它减少索引文件大小，通过mmap可以直接内存操作，稀疏索引为数据文件的每个对应message设置一个元数据指针,它比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间。</p>
<p>3 Kafka文件存储机制–实际运行效果<br>实验环境：</p>
<p>Kafka集群：由2台虚拟机组成<br>cpu：4核<br>物理内存：8GB<br>网卡：千兆网卡<br>jvm heap: 4GB<br>详细Kafka服务端配置及其优化请参考：kafka server.properties配置详解<br><img src="/images/2018/kafka_intro/kafka_read_disk.png" alt="Sample Image Added via Markdown"></p>
<p>从上述图5可以看出，Kafka运行时很少有大量读磁盘的操作，主要是定期批量写磁盘操作，因此操作磁盘很高效。这跟Kafka文件存储中读写message的设计是息息相关的。Kafka中读写message有如下特点:</p>
<h3 id="写message"><a href="#写message" class="headerlink" title="写message"></a>写message</h3><ul>
<li>消息从java堆转入page cache(即物理内存)。</li>
<li>由异步线程刷盘,消息从page cache刷入磁盘。</li>
</ul>
<h3 id="读message"><a href="#读message" class="headerlink" title="读message"></a>读message</h3><ul>
<li>消息直接从page cache转入socket发送出去。</li>
<li>当从page cache没有找到相应数据时，此时会产生磁盘IO,从磁 盘Load消息到page cache,然后直接从socket发出去</li>
</ul>
<h3 id="Kafka高效文件存储设计特点"><a href="#Kafka高效文件存储设计特点" class="headerlink" title="Kafka高效文件存储设计特点"></a>Kafka高效文件存储设计特点</h3><ul>
<li><p>Kafka把topic中一个partition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。</p>
</li>
<li><p>通过索引信息可以快速定位message和确定response的最大大小。</p>
</li>
<li>通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作。</li>
<li>通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。</li>
</ul>
<p>1.Linux Page Cache机制 </p>
<p>2.Kafka官方文档</p>
<h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>面试的时候经常问到Kafka怎么防止重复消费</p>
<ul>
<li><p>比如，你拿到这个消息做数据库的insert操作，那就容易了，给这个消息做一个唯一的主键，那么就算出现重复消费的情况，就会导致主键冲突，避免数据库出现脏数据。</p>
</li>
<li><p>再比如，你拿到这个消息做Redis的set的操作，那就容易了，不用解决，因为你无论set几次结果都是一样的，set操作本来就算幂等操作。</p>
</li>
<li><p>如果上面两种情况还不行，上大招。准备一个第三方介质，来做消费记录。以Redis为例，给消息分配一个全局id，只要消费过该消息，将<id,message>以K-V形式写入Redis. 那消费者开始消费前，先去Redis中查询有没有消费记录即可。</id,message></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Python Dict Internal</title>
    <url>/2018/python_dict.html</url>
    <content><![CDATA[<p>前言：最近在知乎上看到很多Python职位的面试都会问Python的数据结构。很多大公司，比如知乎，在面试python的工程师的时候都会很注重基础；<br>比如会问：python内置的list dict set等使用和原理。collections 模块里的deque，Counter，OrderDict等。 常用的排序算法和时间复杂度。</p>
<p>今天我就简单说说我学到的关于Python Dict 的见闻。</p>
<p>于是，首先我查看到《流畅的python》一书的第3章。</p>
<ol>
<li>首先我们看看Python是如何用散列表来实现dict类型。</li>
</ol>
<p>什么是散列表：<br>散列表其实是一个稀疏数组（总是有空白元素的数组称为稀疏数组）。<br>在一般的数据结构教材中，散列表里的单元通常叫作表元（bucket）。在 dict 的散列表当中，每个键值对都占用一个表元，每个表元都有两<br>个部分，一个是对键的引用，另一个是对值的引用。因为所有表元的大小一致，所以可以通过偏移量来读取某个表元。</p>
<p>散列算法：<br>1.为了获取 my_dict[search_key] 背后的值，Python 首先会调用hash(search_key) 来计算 search_key 的散列值，把这个值最低的几位数字当作偏移量(貌似是hash(key) 和 数组的长度-1 作 与运算，hash(key)&amp; (size-1)，在散列表里查找表元（具体取几位，得看当前散列表的大小）。</p>
<p>2.若找到的表元是空的，则抛出 KeyError 异常。若不是空的，则表元里会有一对 found_key:found_value。</p>
<p>3.这时候 Python 会检验 search_key == found_key 是否为真，如果它们相等的话，就会返回 found_value。</p>
<p>4.如果 search_key 和 found_key 不匹配的话，这种情况称为散列冲突。</p>
<p>5.Python 解决散列冲突的方法用的是开放地址法。<br>如下图所示：<br><img src="/images/2018/python_dict/python_dict1.png" alt="Sample Image Added via Markdown"></p>
<h3 id="Python-dict的特性："><a href="#Python-dict的特性：" class="headerlink" title="Python dict的特性："></a>Python dict的特性：</h3><p>1.字典在内存上的开销巨大。<br>由于字典使用了散列表，而散列表又必须是稀疏的，这导致它在空间上的效率低下。<br>2.键查询很快。<br>dict 的实现是典型的空间换时间：字典类型有着巨大的内存开销，但它们提供了无视数据量大小的快速访问——只要字典能被装在内存里。<br>3.往字典里添加新键可能会改变已有键的顺序。<br>无论何时往字典里添加新的键，Python 解释器都可能做出为字典扩容的决定。扩容导致的结果就是要新建一个更大的散列表，并把字典里已有的元素添加到新表里。</p>
<p>关于Python的开放地制法解决hash冲突的实现可以参考下<br><a href="https://harveyqing.gitbooks.io/python-read-and-write/content/python_advance/python_dict_implementation.html" target="_blank" rel="external">https://harveyqing.gitbooks.io/python-read-and-write/content/python_advance/python_dict_implementation.html</a></p>
<p><img src="/images/2018/python_dict/python_dict2.png" alt="Sample Image Added via Markdown"></p>
<p><em>若要了解详细的内部机制，推荐阅读</em><br><a href="http://python.jobbole.com/85040/" target="_blank" rel="external">http://python.jobbole.com/85040/</a></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>堆排序学习总结</title>
    <url>/2017/heap_sort.html</url>
    <content><![CDATA[<p>在学习《算法》一书中了解到：<br>优先队列是一种抽象的数据类型，优先队列最重要的操作就是删除最大元素和插入元素。<br>实现优先队列可以用数组，链表，最好的方式还是用堆。</p>
<p>我们可以把任意优先队列变成一种排序方法。将所有元素插入一个查找最小化元素的优先队列，然后再重复调用删除最小元素的操作将它们按顺序删去。用无序数组实现的优先队列这么做相当于进行一次选择排序。用基于堆得优先队列这样做等同于堆排序！</p>
<p>要了解堆首先得了解一下二叉树，在计算机科学中，二叉树是每个节点最多有两个子树的树结构。通常子树被称作“左子树”（left subtree）和“右子树”（right subtree）。</p>
<p>二叉树又分为满二叉树（full binary tree） 和 完全二叉树（complete binary tree）</p>
<p>满二叉树：一棵深度为 k，且有 2k - 1 个节点称之为满二叉树</p>
<p><img src="/images/2017/heap_sort/full_binary_tree.png" alt="Sample Image Added via Markdown"></p>
<p>完全二叉树则满足序号和满二叉树的完全对应。</p>
<p><img src="/images/2017/heap_sort/complete_binary_tree.png" alt="Sample Image Added via Markdown"></p>
<p>如下图，是一个堆和数组的相互关系<br><img src="/images/2017/heap_sort/heap_sort1.png" alt="Sample Image Added via Markdown"></p>
<p>对于给定的某个结点的下标 i，可以很容易的计算出这个结点的父结点、孩子结点的下标：</p>
<ul>
<li>Parent(i) = floor(i/2)，i 的父节点下标</li>
<li>Left(i) = 2i，i 的左子节点下标</li>
<li>Right(i) = 2i + 1，i 的右子节点下标</li>
</ul>
<p>堆排序原理</p>
<p>堆排序就是把最大堆堆顶的最大数取出，将剩余的堆继续调整为最大堆，再次将堆顶的最大数取出，这个过程持续到剩余数只有一个时结束。在堆中定义以下几种操作：</p>
<p>最大堆调整（Max-Heapify）：将堆的末端子节点作调整，使得子节点永远小于父节点(左子树不一定大于右子树)<br>创建最大堆（Build-Max-Heap）：将堆所有数据重新排序，使其成为最大堆<br>堆排序（Heap-Sort）：移除位在第一个数据的根节点，并做最大堆调整的递归运算</p>
<p>继续进行下面的讨论前，需要注意的一个问题是：数组都是 Zero-Based，这就意味着我们的堆数据结构模型要发生改变</p>
<p>相应的，几个计算公式也要作出相应调整：</p>
<p>Parent(i) = floor((i-1)/2)，i 的父节点下标<br>Left(i) = 2i + 1，i 的左子节点下标<br>Right(i) = 2(i + 1)，i 的右子节点下标</p>
<p>最大堆调整（MAX‐HEAPIFY）的作用是保持最大堆的性质，是创建最大堆的核心子程序，作用过程如图所示：</p>
<p><img src="/images/2017/heap_sort/heap_sort2.png" alt="Sample Image Added via Markdown"></p>
<p>源码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">heapsort</span><span class="params">(arr)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">max_heapify</span><span class="params">(arr, index, heap_size)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        # 最大堆调整（MAX‐HEAPIFY）的作用是保持最大堆的性质。 将堆的末端子节点作调整，使得子节点永远小于父节点</div><div class="line">        # 意思就是要让index这元素去到它该去的位置</div><div class="line">        《算法》P200</div><div class="line">        由上至下的对有序化（下沉）</div><div class="line">        如果我们把堆想象成一个严密的黑色会组织，每个子节点都表示一个下属（父节点则表示它的直接上级），name这些操作就可以得到很有趣的解释。</div><div class="line">        max_hepify这个方法就相当于一个领导，如果占着老大的位置，但如果自己下属有比自己厉害的就要退位让贤。自己去到该去的层次</div><div class="line"></div><div class="line">        注意的是：这方法是有惰性的。调用一次该方法仍然不行。如果该index的左右子树都小于自己的话，它就不会递归了。</div><div class="line">        这个方法只是让index尽量下沉下去。如果是堆有序的话，那么执行该方法后index位置就是该位置最大的值</div><div class="line">        """</div><div class="line"></div><div class="line">        <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">            imax = index</div><div class="line">            ileft = <span class="number">2</span> * index + <span class="number">1</span></div><div class="line">            iright = <span class="number">2</span> * index + <span class="number">2</span></div><div class="line"></div><div class="line">            <span class="keyword">if</span> ileft &lt; heap_size <span class="keyword">and</span> arr[index] &lt; arr[ileft]:</div><div class="line">                imax = ileft</div><div class="line"></div><div class="line">            <span class="keyword">if</span> iright &lt; heap_size <span class="keyword">and</span> arr[imax] &lt; arr[iright]:</div><div class="line">                imax = iright</div><div class="line"></div><div class="line">            <span class="keyword">if</span> imax != index:</div><div class="line">                arr[imax], arr[index] = arr[index], arr[imax]</div><div class="line">                index = imax</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                <span class="keyword">break</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_maxheap</span><span class="params">(arr)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        # 将堆所有数据重新排序，使其成为最大堆</div><div class="line"></div><div class="line">        创建最大堆（Build-Max-Heap）的作用是将一个数组改造成一个最大堆，接受数组和堆大小两个参数，Build-Max-Heap 将自下而上的调用 Max-Heapify 来改造数组，建立最大堆。</div><div class="line">        因为 Max-Heapify 能够保证下标 i 的结点之后结点都满足最大堆的性质，所以自下而上的调用 Max-Heapify 能够在改造过程中保持这一性质。</div><div class="line">        如果最大堆的数量元素是 n，那么 Build-Max-Heap 从 Parent(n) 开始，往上依次调用 Max-Heapify。</div><div class="line">        """</div><div class="line"></div><div class="line">        iparent = len(arr) // <span class="number">2</span> - <span class="number">1</span>   <span class="comment"># 最后一个节点的父节点</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(iparent, <span class="number">-1</span>, <span class="number">-1</span>):</div><div class="line">            max_heapify(arr, i, len(arr))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sort</span><span class="params">(arr)</span>:</span></div><div class="line">        build_maxheap(arr)  <span class="comment"># build后左子树 还是可能大于 右子树的.所以这并不是一个有序数组</span></div><div class="line">        print(arr)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</div><div class="line">            arr[<span class="number">0</span>], arr[i] = arr[i], arr[<span class="number">0</span>]</div><div class="line"></div><div class="line">            max_heapify(arr, <span class="number">0</span>, i)  <span class="comment"># 这里这个i 相当于比当前的length - 1；第i个元素为当前最大的元素，就被固定死了。不再访问它</span></div><div class="line">        <span class="keyword">return</span> arr</div><div class="line"></div><div class="line">    <span class="keyword">return</span> sort(arr)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line"></div><div class="line">    print(heapsort([<span class="number">5</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">3</span>,<span class="number">8</span>,<span class="number">2</span>,<span class="number">11</span>,<span class="number">15</span>,<span class="number">17</span>,<span class="number">16</span>,<span class="number">13</span>,<span class="number">10</span>,<span class="number">9</span>,<span class="number">12</span>,<span class="number">1</span>]))</div></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>TCP/IP 的一些学习</title>
    <url>/2017/tcp_knowledge.html</url>
    <content><![CDATA[<p>还记得两三年前，去一个大数据公司面试。首先要笔试，题目中有道题目就是填空TCP报文的表格。<br>首先我们看看TCP的报头：</p>
<p> <img src="/images/2017/tcp_knowledge/tcp_head.jpg" alt="Sample Image Added via Markdown"></p>
<p>当时觉得这是啥玩意呀。考这东西跟大数据有毛线关系。今天，才明白，这些都是计算机的基础。基本功来的，跑不了。</p>
<p>下面我们可以通过一张图就能比较清晰地展现TCP/IP建立连接的过程。也就是我们常说的3次握手，4次挥手。</p>
<p>我们先看3次握手的过程：<br><img src="/images/2017/tcp_knowledge/tcp_connection.png" alt="Sample Image Added via Markdown"></p>
<p>1.首先Server端是处在一个Listen的状态。经常你用netstat可以看到，比如打开了一个web 服务，就卡看到80端口处在listen状态<br>2.由客户端发送一个Segment(传输层的数据包单位)到服务的，主要内容有 SYN=1，seq=x(一个随机数)。发送后，客户端状态变为SYN-SENT<br>3.服务端收到客户端发送的Segment，状态变为SYN-REVD，并发送一个SYN=1,ACK=1,seq=y,ack=x+1 的Segment返还给客户端。表示自己收到了你的消息。<br>4.客户端收到服务端返还的内容后，状态就变成了ESTABLISHED。表示成功建立了连接。并且再次发送一个ACK=1,ack=y+1,seq=x+1 的Segment。<br>5.服务端收到客户端发来的Segment后，状态变为ESTABLISHED。成功完成3次握手。</p>
<p>建立一次连接，需要3次握手，也就是说发送3个数据包即可建立连接。发送的数据包内容中我们可以发现一点规律。ack的数值内容都是对方发送给自己的seq数值 +1  。</p>
<p>接下来我们看看4次挥手的过程。<br><img src="/images/2017/tcp_knowledge/tcp_finish.png" alt="Sample Image Added via Markdown"></p>
<p>1.首先由终端的提出方(假设是客户端)发送一个FIN=1,seq=u 的Segment到服务端。发送后客户端状态变为FIN-WAIT-1<br>2.服务端收到了客户端的Segment后状态变为CLOSE-WAIT，并反回一个ACK=1,seq=v,ack=u+1 的Segment给客户端。<br>3.客户端收到了服务端返回的消息后状态变为FIN-WAIT-2,此时服务端并没有关闭，意思是告诉客户端，请您耐心等待，我看看还有什么数据没有给你的，把剩下的数据发给你先。<br>4.过了片刻，服务端发送一个FIN=1,ACK=1,seq=w,ack=u+1 的Segment给客户端。然后服务端状态变为了LAST-ACK。<br>5.客户端收到了服务端发来的消息后，状态变为了TIME-WAIT。并发送ACK=1,seq=u+1,ack=w+1的Segment。表示我知道了，我再等等你(2MSL)还有什么话跟我说的。没有的话我就关了。<br>6.服务端收到客户端发送的消息后，状态就变为CLOSED了。</p>
]]></content>
      <categories>
        <category>study</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title>第一次去加州 （下）</title>
    <url>/2017/california_trip2.html</url>
    <content><![CDATA[<h2 id="加州游记-下"><a href="#加州游记-下" class="headerlink" title="加州游记 下"></a>加州游记 下</h2><p>一路沿海的路途。本来应该比较好的方案是从旧金山开到洛杉矶，这样就可以靠海看风景了。<br><img src="/images/2017/california_trip/IMG_0737.jpg" alt=""></p>
<p>在进旧金山之前，路上车非常少<br><img src="/images/2017/california_trip/IMG_0759.jpg" alt=""></p>
<p><img src="/images/2017/california_trip/IMG_0763.jpg" alt=""></p>
<p>都说国外的月亮比较远，记得那晚正是中秋节，我们来路途的一个医院附近休息赏月<br><img src="/images/2017/california_trip/IMG_0764.jpg" alt=""></p>
<h3 id="卡梅尔小镇-Carmel-by-the-sea"><a href="#卡梅尔小镇-Carmel-by-the-sea" class="headerlink" title="卡梅尔小镇 Carmel-by-the-sea"></a>卡梅尔小镇 Carmel-by-the-sea</h3><p>这里有非常不错的海景，吸引了不少游客。本来打算在这住一晚的，然后打开AirBnb一看，根本没有空房了，不知道得提前多久预定。<br>这里的房价贵的吓人。我们当时就在那里转了一个早上就走了。<br><img src="/images/2017/california_trip/IMG_0771.jpg" alt=""></p>
<h3 id="Santa-Jose"><a href="#Santa-Jose" class="headerlink" title="Santa Jose"></a>Santa Jose</h3><p>圣克拉拉大学<br>里面的设施非常新，貌似是一个贵族学校<br><img src="/images/2017/california_trip/IMG_0793.jpg" alt=""></p>
<h3 id="山景城"><a href="#山景城" class="headerlink" title="山景城"></a>山景城</h3><p>Google， 我最想去的公司，也是很多程序员最理想的工作地方吧<br><img src="/images/2017/california_trip/IMG_0818.jpg" alt=""></p>
<h3 id="San-Francisco"><a href="#San-Francisco" class="headerlink" title="San Francisco"></a>San Francisco</h3><p>Facebook 的停车场门上堆满了汽车<br><img src="/images/2017/california_trip/IMG_0840.jpg" alt=""></p>
<p>斯坦福大学，里面的草太大，太绿了，修这些草不知要花多少钱。</p>
<p><img src="/images/2017/california_trip/IMG_0848.jpg" alt=""></p>
<p><img src="/images/2017/california_trip/IMG_0869.jpg" alt=""></p>
<p>在美国问了几次路。发现最友好的是黑人。黑人总是给人一种很热情，乐观的感觉。还记得，在斯坦福校园里迷路的时候，一直问路人。<br>最友好的是一个混血小哥，像亨利的那种肤色。在小哥，锁单车准备进图书馆的时候，我走过去问他。<br>小哥非常热情地告诉我路怎么走。真是很感激，真想当场要个联系方式，也是这么牛逼的人了，还这么热情。</p>
<p>塞车的旧金山<br><img src="/images/2017/california_trip/IMG_0887.jpg" alt=""></p>
<p>感受一下旧金山的停车费<br><img src="/images/2017/california_trip/IMG_0890.jpg" alt=""></p>
<p>旧金山的中国城<br><img src="/images/2017/california_trip/IMG_0894.jpg" alt=""></p>
<p>在旧金山开车很有难度，一些性能较差的车可能在旧金山都开不了。因为有很多那种非常陡的路，下坡要踩很重的刹车，上坡就得考研车的动力了。<br>这条路倾斜调度应该有40度吧。<br><img src="/images/2017/california_trip/IMG_0901.jpg" alt=""></p>
<h3 id="在加州开车的感觉"><a href="#在加州开车的感觉" class="headerlink" title="在加州开车的感觉"></a>在加州开车的感觉</h3><p>加州人开车非常生猛。起步通常都是地板油起步。有一次我要掉头没走到最左侧的道路。在等着红灯。绿灯亮了时候我打灯后向左边偏转，后面突然冲出一辆车，差点撞到我。我吓尿了，半天没缓过神来。<br>在旧金山时候，很多斜坡上等红灯时候，他们起步都会让车轮发出吱吱的尖叫声，因为油门踩得太猛了。</p>
<p>最后一晚住在一个中国房东家里，非常不错的感觉，挺整齐温馨的。<br><img src="/images/2017/california_trip/IMG_0909.jpg" alt=""></p>
<h2 id="再见了加州"><a href="#再见了加州" class="headerlink" title="再见了加州"></a>再见了加州</h2><p>乘坐国泰航空的波音-777回来<br><img src="/images/2017/california_trip/IMG_0924.jpg" alt=""></p>
]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
        <tag>travel</tag>
      </tags>
  </entry>
  <entry>
    <title>第一次去加州 （上）</title>
    <url>/2017/california_trip.html</url>
    <content><![CDATA[<h2 id="加州游记-上"><a href="#加州游记-上" class="headerlink" title="加州游记 上"></a>加州游记 上</h2><h3 id="A380体验"><a href="#A380体验" class="headerlink" title="A380体验"></a>A380体验</h3><p>9点20分起飞。我们大概五点多一点到达了白云机场。国际航班一般都要提前3个小时到达机场</p>
<p>到达白云机场，在登机之前。完全不知道要有EVUS才能登机。还好有补救措施。可以到咨询台那里办理，大概每人300块钱吧。当时很担心会因为这个耽误了登机。<br>幸好，那工作人员姐姐，还是很快就办理完成了，大概只花了15分钟两个人。</p>
<p>在候机大厅里那里吃了面条，大概是70块钱这样吧。还不算很贵的。然后我们就坐在那儿等待起飞。<br>我们透过玻璃拍了一些A380的照片。那飞机特别大，有四个螺旋桨。</p>
<p>八点半多久开始登记了。因为这飞机人多，所以登机的时间也需要提前比较长，所有人登机完都要花费一个多小时。我们的位置是在第一层最靠后的中间位置，我旁边就是上二层的楼梯口。<br><img src="/images/2017/california_trip/IMG_0524.jpg" alt=""></p>
<p>晚上十一点的时候飞机的乘务人员就给我们准备了晚餐，确切说是夜宵吧。还不错，吃完就睡觉了。飞机的空调有点大，找空姐要了两张被子。<br>在飞机上睡觉还是比较不舒服的。因为座椅并不能打很低。<br><img src="/images/2017/california_trip/IMG_0535.jpg" alt=""><br>勉强能睡着，在飞机上我大概睡了5-6个小时吧，就醒来了，打开窗看看风景吧，只看到底下是一片太平洋。</p>
<h3 id="刚到美国"><a href="#刚到美国" class="headerlink" title="刚到美国"></a>刚到美国</h3><p>飞机抵达洛杉矶，很兴奋地去过关，我还对着海关人员说how are you。。边检冷冷地问我来美国干嘛的？我说旅游的。<br>自驾从1号公路到旧金山。我说一号公路，那海关人员说什么！？ 我再解释了一遍，我说租了一辆车走沿海的一号公路去旧金山。<br>– 噢！知道了，你早说嘛，走吧。<br>– Thank you sir.<br>就这么通过了。</p>
<p>进入到机场里面，机场比我想象中落后好多呀。人不多，而且感觉大门一点都不气派，像是国内一个二线城市机场的水平。<br><img src="/images/2017/california_trip/IMG_0552.jpg" alt=""><br>走出门口，手机连不上网，我不知道怎么打车。我就问问路边开车拉客（貌似是开黑车）的老黑，问他到那个地点要多少钱，他说要30美金，<br>我要这么多啊，那老黑很自信地说道：不信你问别的司机。我就去问不远处的的士司机，司机是一个亚洲面孔，顿时让我放心很多，<br>他看了看地图，查了下，说大概20美金。于是我们就打了这辆的士到达我在Airbnb提前预定的旅店。聊天中知道司机是个韩国人。</p>
<p>大概坐车20分钟，到了住所。见到了房东的老婆，一个菲律宾人，她非常友好热情地招呼了我们。在美国，<br>遇到亚洲面孔感觉都会比较亲切，比较放松，没那么紧张。</p>
<p>这是房东Matt家的厅<br><img src="/images/2017/california_trip/IMG_0572.jpg" alt=""><br>厨房<br><img src="/images/2017/california_trip/IMG_0574.jpg" alt=""></p>
<h3 id="提车"><a href="#提车" class="headerlink" title="提车"></a>提车</h3><p>第二天早上，起来就看到房东了，非常高大的白人，比起他老婆内敛很多。他也在机场上班就顺路开车载我们去机场拿车。<br>这就是我这7天的座驾了<br><img src="/images/2017/california_trip/IMG_0583.jpg" alt=""></p>
<p>动力还挺强劲的。</p>
<h3 id="Staples-Center"><a href="#Staples-Center" class="headerlink" title="Staples Center"></a>Staples Center</h3><p>拿到车，我们就在洛杉矶四处乱逛了。真的是乱逛，一点计划都没有的。<br>洛杉矶某个角落<br><img src="/images/2017/california_trip/IMG_0593.jpg" alt=""><br>下午2点回到旅店休息，本来打算睡个午觉的，没想时差还没有倒过来，睡了一大觉到晚上九点多了。就开车出去逛了。</p>
<p>湖人主场<br><img src="/images/2017/california_trip/IMG_0626.jpg" alt=""><br>湖人主场附近<br><img src="/images/2017/california_trip/IMG_0633.jpg" alt=""></p>
<p>湖人主场附近豪车多<br><img src="/images/2017/california_trip/IMG_0631.jpg" alt=""></p>
<h3 id="Beverly-Hill"><a href="#Beverly-Hill" class="headerlink" title="Beverly Hill"></a>Beverly Hill</h3><p>在美国走斑马线过马路。刚开始不知道白色的灯表示通过。有一次在比佛利山庄附近过马路时候，忘了是什么颜色的灯，本来是应该车通过的，结果我跑过马路，那些车都准备启动，我就这么跑过去了。<br>我不太确定是否可以过，然后就急急忙忙跑过去了。好尴尬。结果那些车还是让我了。</p>
<p>来富人区这看看<br><img src="/images/2017/california_trip/IMG_0643.jpg" alt=""></p>
<h3 id="Holly-Wood"><a href="#Holly-Wood" class="headerlink" title="Holly Wood"></a>Holly Wood</h3><p>走路来星光大道<br><img src="/images/2017/california_trip/IMG_0678.jpg" alt=""></p>
<h3 id="UCLA"><a href="#UCLA" class="headerlink" title="UCLA"></a>UCLA</h3><p>晚上又是倒时差，出来逛逛UCLA。UCLA挺豪华的感觉，设施都很新。<br>很大的停车场<br><img src="/images/2017/california_trip/IMG_0689.jpg" alt=""></p>
<p>教学楼<br><img src="/images/2017/california_trip/IMG_0694.jpg" alt=""></p>
<h3 id="圣莫妮卡"><a href="#圣莫妮卡" class="headerlink" title="圣莫妮卡"></a>圣莫妮卡</h3><p>还记得那天晚上接近凌晨2点钟的洛杉矶，我没有订到酒店，于是漫无目的地在洛杉矶街头上开着车，由于GTA5真人版。我一直往圣莫妮卡的方向前行。<br>走到快到海边附近，我尿急想附近找个地方解决，于是看到不远的地方有个麦当劳，于是我把车开到里，发现凌晨两点钟居然还有这么多人，大部分都是黑人，那个地方。<br>我走进麦当劳餐厅里面，我发现身边的都是黑人，白人都几乎没有，更没有黄种人了。我走进去他们都看着我，我很担心他们因为种族不同欺负我，我头不敢乱晃紧紧盯着前方，进去厕所后把尿撒完后赶紧就跑路了。<br>那里的人貌似都是homeless的，看起来都很穷。这么晚还不回家，应该就是无家可归的人吧。</p>
<p>仿佛来到了现场版的侠盗车手<br><img src="/images/2017/california_trip/IMG_0705.jpg" alt=""></p>
<p><img src="/images/2017/california_trip/IMG_0733.jpg" alt=""><br>圣莫妮卡的海边<br><img src="/images/2017/california_trip/IMG_0736.jpg" alt=""></p>
<p>洛杉矶之旅就告一段落了，接下来是一段沿海一号公路长途自驾游。 </p>
]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
        <tag>travel</tag>
      </tags>
  </entry>
  <entry>
    <title>怎么成为一个Apache Contributor贡献者？</title>
    <url>/2017/how_tobe_apache_contributor.html</url>
    <content><![CDATA[<h1 id="通往Apache-Contributor之路"><a href="#通往Apache-Contributor之路" class="headerlink" title="通往Apache Contributor之路"></a>通往Apache Contributor之路</h1><h2 id="根据个人贡献获得价值（Government-By-Merit）"><a href="#根据个人贡献获得价值（Government-By-Merit）" class="headerlink" title="根据个人贡献获得价值（Government By Merit）"></a>根据个人贡献获得价值（Government By Merit）</h2><p>回忆我刚参与ServiceComb项目，面对上万行的存量代码，总觉得无从下手，甚至认为开源社区高手如云，如果没有深厚且对口的技术功底，还是不要来掺和了。</p>
<p>在这个困难而关键的时候，社区导师给了我明确的指导——不要怕，从小事做起，不要“善小而不为”。于是我静下心来，在Jira上寻找最简单的任务，主动请缨的第一个任务是支持配置兼容，具体需求是cse.xxx配置项和servicecomb.xxx配置项要具备等同效果，经过一番努力，成功Merge PR ；之后我又接下另一个简单任务，增加一个Annotation用于支持Json String作为请求参数……</p>
<p>Apache Way非常看重个人贡献，没有贡献，一切无从谈起，与开源软件同行，不仅看你获得了多少，更要坚持长期贡献，这是它与商业软件最大的不同。贡献并不区分大小，无论是增加重大特性，还是小小的改进、Bug Fix和修订文档错误，这些同样是项目茁壮成长的关键。</p>
<p>实际上大多数开源爱好者都是从修订文档错误开始的，例如改正错别字、格式不正确以及订正描述等等，我对ServiceComb的理解也绝大多数来自这方面的工作；这样不但能够在阅读文档的过程中更快的了解技术细节，也比较容易入手做出贡献。</p>
<p>总之坚持下来，个人积累的贡献慢慢变厚，获得Apache的认可自然水到渠成。</p>
<h2 id="社区驱动（Community）"><a href="#社区驱动（Community）" class="headerlink" title="社区驱动（Community）"></a>社区驱动（Community）</h2><p>参与社区是技术成长最快的方式之一，Follow Apache社区的方式有订阅邮件列表和加入Gitter聊天室；从看大家讨论（讨论邮件一般会使用[Discussion]开头），到回答大家的问题（回复邮件和发送Gitter聊天），相信用不了多久你就能收获颇丰，并冒出自己的想法，迈出第一步提交第一个PR也就不难了。</p>
<p>ServiceComb作为一个微服务一站式解决方案，融合侵入式、非侵入式场景并支持多语言，解放开发者，帮助用户和开发者将企业应用轻松微服务化和上云；大家在这里讨论的话题、发起的投票、以及提交的代码，无不与微服务密切相关。在这个社区中我不但学习到了知识，完成了开源（也包含微服务）小白的蜕变，还进一步接受了开源的洗礼——前辈指导我进步，我将所学传递给新人；这个过程中我结识了很多新朋友，大家互通有无，不但极大的开阔了视野，也提高了自己的社交能力。</p>
<p>Apache开发者来自全球，社区大多都是用英文来交流。通过阅读英文资料，使用英文在Gitter上与开发人员直接交流，通过英文邮件来探讨问题，在不知不觉中自己的英文水平也大大提高了。</p>
<h2 id="协作开发（Collaborative-Development）"><a href="#协作开发（Collaborative-Development）" class="headerlink" title="协作开发（Collaborative Development）"></a>协作开发（Collaborative Development）</h2><p>这也是我极力推荐参与开源社区开发的重要原因之一，当你提交PR后，无论代码有多么烂，你总能收获宝贵的Comments，不花钱获得编程大师的指点——教你怎么写出优秀的代码，这是多么合算的买卖！</p>
<p>我在参与ServiceComb社区前，只知道Java基本语法，IDE不熟练（之前一直是用VS写C#，不使用Eclipse和IntelliJ IDEA），不会Git，不懂Maven，还能有更糟糕的起点吗：）</p>
<p>不用担心，社区会指导你。我前文提到的第一个简单的任务，花费了将近一周时间，被打回来了四五次后才被Merge；一个PR收获60+ Comments也是家常便饭。这个过程中我的Java水平成长得飞快，不久后就能独立承担新特性的设计和开发——Metrics，当然，完成这个新特性的过程中Committer和其他开发者给予了很多支持，所以，请大胆的提交你的第一个PR，成为一名Contributor吧。</p>
<h2 id="民主，开放和透明（Consensus-Open-and-Transparency）"><a href="#民主，开放和透明（Consensus-Open-and-Transparency）" class="headerlink" title="民主，开放和透明（Consensus, Open and Transparency）"></a>民主，开放和透明（Consensus, Open and Transparency）</h2><p>在Apache社区里投票至关重要，你可以感受到你的建议将被充分重视，我很喜欢这种参与感；大家的讨论和建议也完全公开透明，并且能够长时间通过邮件列表查询到，沟通效率非常高。所以大家多多参与，一定能收获惊喜，从万能的社区里寻找自己想要的答案，请记住，当你对某个问题产生困惑，即使是强大的StackOverflow也不会有原作者的答复更为准确。</p>
<p>写在最后，我想说从一名开源小白成长为Apache Committer并不是无比艰难又遥不可及的事情，只需要日积月累的在社区中投入一点业余时间，就能梦想成真。当然最好找到自己感兴趣的社区，这会让这段旅途更加愉快，也能交到更多志同道合的的朋友。</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://servicecomb.apache.org/cn/docs/how-to-grow-up-to-be-an-apache-committer/" target="_blank" rel="external">https://servicecomb.apache.org/cn/docs/how-to-grow-up-to-be-an-apache-committer/</a></p>
]]></content>
      <categories>
        <category>github</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title>每周的总结与记录</title>
    <url>/2017/dennis_weekly.html</url>
    <content><![CDATA[<h3 id="20190924"><a href="#20190924" class="headerlink" title="20190924"></a>20190924</h3><p>早上喝了利动乳果糖口服溶液然后吃早餐，到公司九点多感觉肚子疼。于是拉肚子了。中午说拉肚子了就喝粥吧，就去红荔村吃皮蛋瘦肉粥，结果吃完后回来肚子不停地咕咕叫，叫的特别厉害。而且肚子很涨，越来越胀，然后开始涨疼了。我一直想忍过去，但一直都还会涨疼，还喝了矿泉水，冷的，更疼。下午三点多4楼看医生，说我不应该吃皮蛋瘦肉粥，他说腹泻后不能吃蛋白质类的食物，牛奶都不要喝。只能喝白粥。然后开了左氟沙星和奥美拉唑。他妈的，我以为肚子胀可以吃皮蛋瘦肉粥这些，因为这也算粥，但不知道是不能吃的。</p>
<p>晚上十点终于把皮蛋瘦肉粥拉出来了，感觉轻松一点。以后再也不好喝皮蛋瘦肉粥了。</p>
<h3 id="20191010"><a href="#20191010" class="headerlink" title="20191010"></a>20191010</h3><p>今天为了解决保存task result的task name花了很多时间看celery的源代码，也不断地尝试测试，收益很低，没有解决到问题。感觉看源码要有很强的专注力，而且要能记住很多各个不同调用的关系，不然看着就会发蒙，不知道自己要干什么。另外，Celery有些代码写着还是挺难懂的，比如说task 装饰品那部分代码。加上网络一直不稳定，尤其是连接国外的服务器。卡卡的很影响效率。然后tmux Pycharm这些工具用的不太熟练也对效率影响挺大的，要不停地去网上查文档。此外今天早上偶尔发现一个YOUTUBE的商机，偶尔看到了一个数据动态图，然后我觉得还是比较容易效仿的。最重要的是，收看率更好，而且貌似最近才兴起的，我看那些up主的历史记录都很短，最多也就半年多。</p>
<h3 id="20191013"><a href="#20191013" class="headerlink" title="20191013"></a>20191013</h3><p>今天发现爬虫的airflow调度部署很不顺利，比我想象中遇到很多麻烦。主要是执行命令时候环境变量没有起作用。Python用的是默认的解析器。反正陆陆续续弄了两三天还没弄好。<br>今天还有一件事情挺让我生气的。下午电信宽带突然就抽风了，停网了一个小时，迫于没完成Airflow事情的压力，现在又上不了网了，感觉今天什么事情都没干成。不过后来，让我有些感觉柳暗花明又一村。那电信师傅告诉我挺多的，首先他让我不要用那个房间的路由器了，因为功率低，而且有线网络达不到两百兆。然后，他让我直接用路由器来拨号，厅里那个光猫就充当中继器的作用。这样不不会把房间的路由器作为一个二级路由。有钱的话可以在淘宝买个有光纤接口的路由器。</p>
<h3 id="20191018"><a href="#20191018" class="headerlink" title="20191018"></a>20191018</h3><p>最近3天主要都是在搞Hive优化的工作，真是太难了，没有什么实质性进展，时间花费性价比很低。用了很多办法都没有实际性加快HQL运行的速度。主要是花了很多时间在Google搜索，然后各种查阅文档，然后尝试。包括在SO上发帖提问。<br>今天跟一面前同事霞琳吃饭饭聊天。收获还是挺多的，她说她已经收到了腾讯的Offer准备跳槽到。她是通过内推进去的。之前她也投过很多简历，很多也是石沉大海，也被不少公司面试后拒绝。有时候找工作真是要看运气。她和我不同的地方就是，她4月份就离开了一面，然后到9月份才开始工作。现在又拿到了腾讯的正式offer，真是苦尽甘来呀。不同的地方就是她大概经历了半年没有工作没有收入的压力，能够沉住气，这确实需要一定的勇气。而我找了一个月就来入职了。这点我还是非常佩服她的。<br>她说我可以参加一些分享会，认识一些行业内的人物。比如说她之前参加了TiDB的分享会，可以现场结识一些大牛。总结下来，就是找工作要运气，当然自身能力是前提。</p>
<h3 id="20191021"><a href="#20191021" class="headerlink" title="20191021"></a>20191021</h3><p>到现在这个数据量，接近亿级的数量，明显感觉到服务器很卡，运行时间单位都是上小时的。不知道从哪里入手优化任务。即使是用spark on yarn，依然很慢。看个主机Htop的资源利用率并不高，50%都没有用到。不知道集群内部在搞什么，完全是个黑箱操作，我只能默默地等待着。哎，优化之路很艰难。最根本的原因我觉得还是我的硬件资源不够。有钱的话还是再买一台服务器好点，或者先加一16G内存也好。</p>
]]></content>
      <categories>
        <category>daily</category>
      </categories>
      <tags>
        <tag>daily</tag>
      </tags>
  </entry>
  <entry>
    <title>怎么成为Apache Spark贡献者</title>
    <url>/2017/tobe_spark_contributor.html</url>
    <content><![CDATA[<p>You should start with:</p>
<p>Read API Docs: Spark 2.4.3 Scala Doc<br>Join the mailing list: Community | Apache Spark<br>Now you should take a look at Jira (list of known bugs / issues): - ASF JIRA</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>从网上找到自己的答案</title>
    <url>/2017/find_your_answer.html</url>
    <content><![CDATA[<h1 id="Use-the-Right-Search-Engine"><a href="#Use-the-Right-Search-Engine" class="headerlink" title="Use the Right Search Engine"></a>Use the Right Search Engine</h1><p>成为一个程序员也有一段时间了。回过头来想想看以前做的事情，只想说：非常low！</p>
<p>我记得东哥那时候推荐我用bing或者翻墙上谷歌。我当时可是一脸懵逼，感觉用习惯了，也觉得没什么不好的地方。</p>
<p>不说其他的，就说程序这块吧。当你运行代码碰到一个报错信息时候，直接用google搜的话很快就能找到解决方案。比如Python的话google返回的很有可能是stackoverflow这些链接。linux的是askubuntu的这些链接。</p>
<p>而百度返回的很有可能是Oschina或者Csdn等等。然而很多好的解答都是在stackoverflow那些网站。</p>
<h1 id="English-is-Important"><a href="#English-is-Important" class="headerlink" title="English is Important"></a>English is Important</h1><p>有些比较少罕见的问题，国内搜索引擎压根就找不到，国内压根就没有这方面答案。你必须从国外的网站获取解答。比如 Github issue 、Quora或者官方文档。</p>
<a id="more"></a>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>多用Google搜索。多到英文环境下找想要的答案。</p>
]]></content>
      <categories>
        <category>daily</category>
      </categories>
      <tags>
        <tag>daily</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark repartition和coalesce的区别</title>
    <url>/2017/spark_repartition_coalesce.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>有些时候，在很多partition的时候，我们想减少点partition的数量，不然写到HDFS上的文件数量也会很多很多。<br>我们使用reparation呢，还是coalesce。所以我们得了解这两个算子的内在区别。</p>
<h2 id="正题"><a href="#正题" class="headerlink" title="正题"></a>正题</h2><p>要知道，repartition是一个消耗比较昂贵的操作算子，Spark出了一个优化版的repartition叫做coalesce，它可以尽量避免数据迁移，<br>但是你只能减少RDD的partition.</p>
<p>举个例子，有如下数据节点分布：</p>
<p>Node 1 = 1,2,3<br>Node 2 = 4,5,6<br>Node 3 = 7,8,9<br>Node 4 = 10,11,12</p>
<p>用coalesce，将partition减少到2个：</p>
<p>Node 1 = 1,2,3 + (10,11,12)<br>Node 3 = 7,8,9 + (4,5,6)</p>
<p>注意，Node1 和 Node3 不需要移动原始的数据</p>
<p>The repartition algorithm does a full shuffle and creates new partitions with data that’s distributed evenly.<br>Let’s create a DataFrame with the numbers from 1 to 12.</p>
<p>repartition 算法会做一个full shuffle然后均匀分布地创建新的partition。我们创建一个1-12数字的DataFrame测试一下。<br>刚开始数据是这样分布的：</p>
<p>Partition 00000: 1, 2, 3<br>Partition 00001: 4, 5, 6<br>Partition 00002: 7, 8, 9<br>Partition 00003: 10, 11, 12</p>
<p>我们做一个full shuffle，将其repartition为2个。</p>
<p>这是在我机器上数据分布的情况：<br>Partition A: 1, 3, 4, 6, 7, 9, 10, 12<br>Partition B: 2, 5, 8, 11</p>
<p>The repartition method makes new partitions and evenly distributes the data in the new partitions (the data distribution is more even for larger data sets).<br>repartition方法让新的partition均匀地分布了数据（数据量大的情况下其实会更均匀）</p>
<h3 id="coalesce-和-repartition-的区别"><a href="#coalesce-和-repartition-的区别" class="headerlink" title="coalesce 和 repartition 的区别"></a>coalesce 和 repartition 的区别</h3><p>coalesce用已有的partition去尽量减少数据shuffle。<br>repartition创建新的partition并且使用 full shuffle。<br>coalesce会使得每个partition不同数量的数据分布（有些时候各个partition会有不同的size）<br>然而，repartition使得每个partition的数据大小都粗略地相等。</p>
<h3 id="coalesce-会比-repartition-快速吗？"><a href="#coalesce-会比-repartition-快速吗？" class="headerlink" title="coalesce 会比 repartition 快速吗？"></a>coalesce 会比 repartition 快速吗？</h3><p>coalesce可能会比repartition更快，但是，在partition大小不相等的时候， 总体上会比repartition慢一些。<br>通常，在过滤掉大数据集后，你需要用repartition一下。<br>我发现repartition总体上会快一些，因为Spark一般都是用相同大小的partition。</p>
<h2 id="Referrence"><a href="#Referrence" class="headerlink" title="Referrence"></a>Referrence</h2><p><a href="https://stackoverflow.com/questions/31610971/spark-repartition-vs-coalesce" target="_blank" rel="external">https://stackoverflow.com/questions/31610971/spark-repartition-vs-coalesce</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 的宽依赖和窄依赖</title>
    <url>/2016/spark_dependency.html</url>
    <content><![CDATA[<h1 id="说说Spark-的宽依赖和窄依赖"><a href="#说说Spark-的宽依赖和窄依赖" class="headerlink" title="说说Spark 的宽依赖和窄依赖"></a>说说Spark 的宽依赖和窄依赖</h1><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>　　众所周知，在Spark中可以对RDD进行多种转换，父RDD转换之后会得到子RDD，于是便可以说子RDD的诞生需要依赖父RDD。在Spark中一共有两种依赖方式，分别是宽依赖和窄依赖。</p>
<p>rdd 的 toDebugString 可以查看RDD的谱系</p>
<a id="more"></a>
<h3 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a>窄依赖</h3><p>如果父RDD的每个分区至多只对应一个子RDD的分区，则这种依赖关系称为窄依赖。</p>
<p>常见的窄依赖算子有：map、filter、union。</p>
<p>对窄依赖RDD进行Lineage恢复时，如果子RDD某个分区坏了，通过父RDD指定分区重算时，将不会重算其他正常的分区，避免了冗余计算，提高了性能。</p>
<h3 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a>宽依赖</h3><p>如果父RDD中至少有一个分区对应子RDD的多个分区(至少两个分区)，则这种依赖关系称为宽依赖。</p>
<p>常见的宽依赖算子有：groupByKey、sortByKey。</p>
<p>对宽依赖RDD进行Lineage恢复时，如果子RDD某个分区坏了，通过父RDD指定分区重算时，有可能会重算一些其他正常的分区，会有冗余计算，性能开销也会比窄依赖大很多。</p>
<p>宽依赖通常是Spark拆分Stage的边界，在同一个Stage内均为窄依赖。</p>
<p><img src="/images/2016/spark_dependency/spark-dependency.jpg" alt="Sample Image Added via Markdown"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>我们可以不用去背宽窄依赖的概念和对应关系之类的。我们应该知道为什么要区分宽窄依赖，它的目的是什么。其实目的就是提醒我们尽量地使用窄依赖，因为这样会减少风险，<br>当子RDD的某个partition损坏的时候是否要重新计算很多父RDD的多个partition呢。这个才是宽窄依赖的重点。<br>另一个划分看宽窄依赖的目的是：划分stage。至于划分stage的目的，就是要等待其它节点的所有partition全部计算完成才能作下一个阶段的计算。</p>
<p>在网上查了很多资料貌似有很多不同的定义。在看了很多解释后了解到，依赖的问题在论文里和在Spark的实现中的定义是不一样的。</p>
<ul>
<li><p>在论文中，是叫narrow dependency和wide dependency。<br>如果父RDD的每个partition只被子RDD的一个partition所依赖，就叫Narrow dependency，否则叫Wide dependency。</p>
</li>
<li><p>在Spark中，是叫narrow dependency（也叫完全依赖）和shuffle dependency（也叫部分依赖）。<br>如果父RDD的每个partition（并且是partition里的部分数据）都被子RDD的每个partition所依赖才叫ShuffleDependency，其余情况都是NarrowDependency。</p>
</li>
</ul>
<h3 id="SparkInternals-dependency图解"><a href="#SparkInternals-dependency图解" class="headerlink" title="SparkInternals dependency图解"></a>SparkInternals dependency图解</h3><p><img src="/images/2016/spark_dependency/spark_internals_dependency.png" alt="Sample Image Added via Markdown"></p>
<p>前三个是完全依赖，RDD x 中的 partition 与 parent RDD 中的 partition/partitions 完全相关。最后一个是部分依赖，RDD x 中的 partition 只与 parent RDD 中的 partition 一部分数据相关，另一部分数据与 RDD x 中的其他 partition 相关。</p>
<p>在 Spark 中，完全依赖被称为 NarrowDependency，部分依赖被称为 ShuffleDependency。其实 ShuffleDependency 跟 MapReduce 中 shuffle 的数据依赖相同（mapper 将其 output 进行 partition，然后每个 reducer 会将所有 mapper 输出中属于自己的 partition 通过 HTTP fetch 得到）。</p>
<ul>
<li>第一种 1:1 的情况被称为 OneToOneDependency。</li>
<li>第二种 N:1 的情况被称为 N:1 NarrowDependency。</li>
<li>第三种 N:N 的情况被称为 N:N NarrowDependency。不属于前两种情况的完全依赖都属于这个类别。</li>
<li>第四种被称为 ShuffleDependency。</li>
</ul>
<p>N:N NarrowDependency 的几个很经典的情况是：coalesce 和 cartesian<br><img src="/images/2016/spark_dependency/coalesce.png" alt="Sample Image Added via Markdown"></p>
<p><img src="/images/2016/spark_dependency/cartesian.png" alt="Sample Image Added via Markdown"></p>
<p>以上两种情况它们仍然可以在一个stage里的pipeline以一个task计算。不用等所有的task计算完再计算。因为它不是shuffle操作。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://github.com/JerryLead/SparkInternals/blob/master/markdown/2-JobLogicalPlan.md" target="_blank" rel="external">https://github.com/JerryLead/SparkInternals/blob/master/markdown/2-JobLogicalPlan.md</a><br><a href="https://www.zhihu.com/question/37137360/answer/715150822" target="_blank" rel="external">https://www.zhihu.com/question/37137360/answer/715150822</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 数据倾斜调优</title>
    <url>/2016/spark_bias.html</url>
    <content><![CDATA[<h1 id="关于Spark-的数据倾斜的问题"><a href="#关于Spark-的数据倾斜的问题" class="headerlink" title="关于Spark 的数据倾斜的问题"></a>关于Spark 的数据倾斜的问题</h1><h3 id="数据倾斜的现象"><a href="#数据倾斜的现象" class="headerlink" title="数据倾斜的现象"></a>数据倾斜的现象</h3><p>　　1、绝大多数task执行得都非常快，但个别task执行极慢。比如，总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时。这种情况很常见。</p>
<p>　　2、原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。</p>
<h3 id="数据倾斜的原理"><a href="#数据倾斜的原理" class="headerlink" title="数据倾斜的原理"></a>数据倾斜的原理</h3><p>　　数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。</p>
<p>　　因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。</p>
<a id="more"></a>
<p>　　下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而整个stage的运行速度也由运行最慢的那个task所决定。</p>
<p><img src="/images/2016/spark_bias/spark-bias1.jpg" alt="Sample Image Added via Markdown"></p>
<h3 id="如何定位导致数据倾斜的代码"><a href="#如何定位导致数据倾斜的代码" class="headerlink" title="如何定位导致数据倾斜的代码"></a>如何定位导致数据倾斜的代码</h3><p>　　数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。</p>
<ul>
<li>某个task执行特别慢的情况</li>
</ul>
<p>　　首先要看的，就是数据倾斜发生在第几个stage中。</p>
<p>　　如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以在Spark Web UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。</p>
<p>　　比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。</p>
<p><img src="/images/2016/spark_bias/spark-bias2.jpg" alt="Sample Image Added via Markdown"></p>
<p>　　知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。</p>
<p>　　这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。</p>
<p>　　1、stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。</p>
<p>　　2、stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。</p>
<!-- scala -->
<pre><code>val conf = new SparkConf()
val sc = new SparkContext(conf)

val lines = sc.textFile(&quot;hdfs://...&quot;)
val words = lines.flatMap(_.split(&quot; &quot;))
val pairs = words.map((_, 1))
val wordCounts = pairs.reduceByKey(_ + _)

wordCounts.collect().foreach(println(_))
</code></pre><p>　　通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由reduceByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。</p>
<ul>
<li>某个task莫名其妙内存溢出的情况</li>
</ul>
<p>　　这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。</p>
<p>　　但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。</p>
<h3 id="查看导致数据倾斜的key的数据分布情况"><a href="#查看导致数据倾斜的key的数据分布情况" class="headerlink" title="查看导致数据倾斜的key的数据分布情况"></a>查看导致数据倾斜的key的数据分布情况</h3><p>　　知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。</p>
<p>　　此时根据你执行操作的情况不同，可以有很多种查看key分布的方式：</p>
<p>　　1、如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。</p>
<p>　　2、如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。</p>
<p>　　举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。</p>
<!-- scala -->
<pre><code>val sampledPairs = pairs.sample(false, 0.1)
val sampledWordCounts = sampledPairs.countByKey()
sampledWordCounts.foreach(println(_))
</code></pre><h3 id="数据倾斜的解决方案"><a href="#数据倾斜的解决方案" class="headerlink" title="数据倾斜的解决方案"></a>数据倾斜的解决方案</h3><ul>
<li>解决方案一：使用Hive ETL预处理数据</li>
</ul>
<p>方案适用场景：<br>导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。</p>
<p>方案实现思路：<br>此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。</p>
<p>方案实现原理：<br>这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。</p>
<p>方案优点：<br>实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p>
<p>方案缺点：<br>治标不治本，Hive ETL中还是会发生数据倾斜。</p>
<p>方案实践经验：<br>在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p>
<p>项目实践经验：<br>在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。</p>
<ul>
<li>解决方案二：过滤少数导致倾斜的key</li>
</ul>
<p>方案适用场景：<br>如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p>
<p>方案实现思路：<br>如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。</p>
<p>方案实现原理：<br>将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。</p>
<p>方案优点：<br>实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p>
<p>方案缺点：<br>适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p>
<p>方案实践经验：<br>在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p>
<ul>
<li>解决方案三：提高shuffle操作的并行度</li>
</ul>
<p>方案适用场景：<br>如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。</p>
<p>方案实现思路：<br>在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p>
<p>方案实现原理：<br>增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。</p>
<p>方案优点：<br>实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p>
<p>方案缺点：<br>只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p>
<p>方案实践经验：<br>该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p>
<p><img src="/images/2016/spark_bias/spark-bias3.jpg" alt="Sample Image Added via Markdown"></p>
<ul>
<li>解决方案四：两阶段聚合（局部聚合+全局聚合）</li>
</ul>
<p>方案适用场景：<br>对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。</p>
<p>方案实现思路：<br>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</p>
<p>方案实现原理：<br>将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p>
<p>方案优点：<br>对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p>
<p>方案缺点：<br>仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</p>
<p><img src="/images/2016/spark_bias/spark-bias4.jpg" alt="Sample Image Added via Markdown"></p>
<!-- scala -->
<pre><code>// 第一步，给RDD中的每个key都打上一个随机前缀。
JavaPairRDD&lt;String, Long&gt; randomPrefixRdd = rdd.mapToPair(
        new PairFunction&lt;Tuple2&lt;Long,Long&gt;, String, Long&gt;() {
            private static final long serialVersionUID = 1L;
            @Override
            public Tuple2&lt;String, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple)
                    throws Exception {
                Random random = new Random();
                int prefix = random.nextInt(10);
                return new Tuple2&lt;String, Long&gt;(prefix + &quot;_&quot; + tuple._1, tuple._2);
            }
        });

// 第二步，对打上随机前缀的key进行局部聚合。
JavaPairRDD&lt;String, Long&gt; localAggrRdd = randomPrefixRdd.reduceByKey(
        new Function2&lt;Long, Long, Long&gt;() {
            private static final long serialVersionUID = 1L;
            @Override
            public Long call(Long v1, Long v2) throws Exception {
                return v1 + v2;
            }
        });

    // 第三步，去除RDD中每个key的随机前缀。
    JavaPairRDD&lt;Long, Long&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair(
            new PairFunction&lt;Tuple2&lt;String,Long&gt;, Long, Long&gt;() {
                private static final long serialVersionUID = 1L;
                @Override
                public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;String, Long&gt; tuple)
                        throws Exception {
                    long originalKey = Long.valueOf(tuple._1.split(&quot;_&quot;)[1]);
                    return new Tuple2&lt;Long, Long&gt;(originalKey, tuple._2);
                }
            });

    // 第四步，对去除了随机前缀的RDD进行全局聚合。
    JavaPairRDD&lt;Long, Long&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey(
            new Function2&lt;Long, Long, Long&gt;() {
                private static final long serialVersionUID = 1L;
                @Override
                public Long call(Long v1, Long v2) throws Exception {
                    return v1 + v2;
                }
            });
</code></pre><ul>
<li>解决方案五：将reduce join转为map join</li>
</ul>
<p>方案适用场景：<br>在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p>
<p>方案实现思路：<br>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p>
<p>方案实现原理：<br>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。</p>
<p>方案优点：<br>对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p>
<p>方案缺点：<br>适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p>
<p><img src="/images/2016/spark_bias/spark-bias5.jpg" alt="Sample Image Added via Markdown"></p>
<!-- scala -->
<pre><code>// 首先将数据量比较小的RDD的数据，collect到Driver中来。
List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect()
// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。
// 可以尽可能节省内存空间，并且减少网络传输性能开销。
final Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data);

// 对另外一个RDD执行map类操作，而不再是join类操作。
JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd = rdd2.mapToPair(
        new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() {
            private static final long serialVersionUID = 1L;
            @Override
            public Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, String&gt; tuple)
                    throws Exception {
                // 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。
                List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value();
                // 可以将rdd1的数据转换为一个Map，便于后面进行join操作。
                Map&lt;Long, Row&gt; rdd1DataMap = new HashMap&lt;Long, Row&gt;();
                for(Tuple2&lt;Long, Row&gt; data : rdd1Data) {
                    rdd1DataMap.put(data._1, data._2);
                }
                // 获取当前RDD数据的key以及value。
                String key = tuple._1;
                String value = tuple._2;
                // 从rdd1数据Map中，根据key获取到可以join到的数据。
                Row rdd1Value = rdd1DataMap.get(key);
                return new Tuple2&lt;String, String&gt;(key, new Tuple2&lt;String, Row&gt;(value, rdd1Value));
            }
        });

// 这里得提示一下。
// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。
// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。
// rdd2中每条数据都可能会返回多条join后的数据。
</code></pre><ul>
<li>解决方案六：采样倾斜key并分拆join操作</li>
</ul>
<p>方案适用场景：<br>两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。</p>
<p>方案实现思路：<br>　　1、对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。</p>
<p>　　2、然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。</p>
<p>　　3、接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。</p>
<p>　　4、再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。　　</p>
<p>　　5、而另外两个普通的RDD就照常join即可。</p>
<p>　　6、最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。</p>
<p>方案实现原理：<br>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。</p>
<p>方案优点：<br>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。</p>
<p>方案缺点：<br>如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。</p>
<p><img src="/images/2016/spark_bias/spark-bias6.jpg" alt="Sample Image Added via Markdown"></p>
<!-- scala -->
<pre><code>// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。
JavaPairRDD&lt;String, Row&gt; expandedRDD = rdd1.flatMapToPair(
        new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() {
            private static final long serialVersionUID = 1L;
            @Override
            public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, Row&gt; tuple)
                    throws Exception {
                List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;();
                for(int i = 0; i &lt; 100; i++) {
                    list.add(new Tuple2&lt;String, Row&gt;(0 + &quot;_&quot; + tuple._1, tuple._2));
                }
                return list;
            }
        });

// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。
JavaPairRDD&lt;String, String&gt; mappedRDD = rdd2.mapToPair(
        new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() {
            private static final long serialVersionUID = 1L;
            @Override
            public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple)
                    throws Exception {
                Random random = new Random();
                int prefix = random.nextInt(100);
                return new Tuple2&lt;String, String&gt;(prefix + &quot;_&quot; + tuple._1, tuple._2);
            }
        });

// 将两个处理后的RDD进行join即可。
JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD);
</code></pre>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>浅谈SPARK 的 广播变量</title>
    <url>/2016/spark_broadcast.html</url>
    <content><![CDATA[<h1 id="简单谈谈SPARK-广播变量的用法"><a href="#简单谈谈SPARK-广播变量的用法" class="headerlink" title="简单谈谈SPARK 广播变量的用法"></a>简单谈谈SPARK 广播变量的用法</h1><h3 id="首先我们看看官网的文档对Broadcast的描述："><a href="#首先我们看看官网的文档对Broadcast的描述：" class="headerlink" title="首先我们看看官网的文档对Broadcast的描述："></a>首先我们看看官网的文档对Broadcast的描述：</h3><ul>
<li><p>Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks.</p>
</li>
<li><p>意思就是说广播变量让你可以在每个节点机器上缓存一个只读的变量，而减少了在每个任务中复制一份的繁琐。</p>
</li>
<li>Spark还尝试使用高效地广播算法来分发变量，进而减少通信的开销。</li>
<li>Spark的动作通过一系列的步骤执行，这些步骤由分布式的洗牌操作分开。Spark自动地广播每个步骤每个任务需要的通用数据。这些广播数据被序列化地缓存，在运行任务之前被反序列化出来。这意味着当我们需要在多个阶段的任务之间使用相同的数据，或者以反序列化形式缓存数据是十分重要的时候，显式地创建广播变量才有用。</li>
</ul>
<p>创建的方法很简单<br><a id="more"></a></p>
<!-- python -->
<pre><code>import pandas as pd
table=pd.read_sql(&quot;select goods_id,goods_model,brand_id from goods&quot;,engine)
#比如我们要先读取一个比较大的mysql表以供map、filter这些变型中查询数据。我们可以用广播变量将这个公共的变量在每个节点中都保留一份

table_broadcast=sc.broadcast(table)

#调用的时候很简单，只需要在对象.value就可以

def parse(tup):
    result=table_broadcast.value[table_broadcast.value[&apos;goods_id]==100][&apos;brand_id&apos;]
    return result

rdd.map(parse)
</code></pre><p>在创建了广播变量之后，在集群上的所有函数中应该使用它来替代使用v.这样v就不会不止一次地在节点之间传输了。另外，为了确保所有的节点获得相同的变量，对象v在被广播之后就不应该再修改。</p>
<p>如果不用broadcast的话，table会从主节点为每个任务发送一个这样的数据，就会代价很大，而且再调用table的时候，还需要向每个节点再发送一遍。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>浅谈SPARK 的 combineByKey 算子</title>
    <url>/2016/spark_combinebykey.html</url>
    <content><![CDATA[<h1 id="理解comebineByKey的原理"><a href="#理解comebineByKey的原理" class="headerlink" title="理解comebineByKey的原理"></a>理解comebineByKey的原理</h1><h3 id="SPARK的combineByKey算子和aggregate类似。首先我们看看官网的文档："><a href="#SPARK的combineByKey算子和aggregate类似。首先我们看看官网的文档：" class="headerlink" title="SPARK的combineByKey算子和aggregate类似。首先我们看看官网的文档："></a>SPARK的combineByKey算子和aggregate类似。首先我们看看官网的文档：</h3><ul>
<li><p>combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<function portable_hash="" at="" 0x7ff5681b9d70="">)</function></p>
</li>
<li><p>Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a “combined type” C. Note that V and C can be different – for example, one might group an RDD of type (Int, Int) into an RDD of type (Int, List[Int]).</p>
</li>
<li><p>Users provide three functions:<br>createCombiner, which turns a V into a C (e.g., creates a one-element list)<br>mergeValue, to merge a V into a C (e.g., adds it to the end of a list)<br>mergeCombiners, to combine two C’s into a single one.</p>
</li>
</ul>
<p>都是将类型为RDD[(K,V)]的数据处理为RDD[(K,C)]。这里的V和C可以是相同类型，也可以是不同类型。</p>
<p>combineByKey函数主要接受了三个函数作为参数，分别为createCombiner、mergeValue、mergeCombiners。</p>
<p>这三个函数足以说明它究竟做了什么。理解了这三个函数，就可以很好地理解combineByKey。</p>
<a id="more"></a>
<p>1.createCombiner ,让V变为C<br>V –&gt; C<br>2.mergeValue，将V合并为C<br>C, V –&gt; C<br>3.mergeCombiners，合并所有C为一个<br>C, C –&gt; C</p>
<p>我们在命令行里输入IPYTHON=1 pyspark --master local[<em>]启动Spark 的Python shell。这里加了–master local[</em>] 是为了可以以local 模式运行，这样可以便于print 打印调试。</p>
<p>下面是一个求平均值的例子<br><!-- python --></p>
<pre><code>data = [
        (&apos;A&apos;, 2.), (&apos;A&apos;, 4.), (&apos;A&apos;, 9.), 
        (&apos;B&apos;, 10.), (&apos;B&apos;, 20.), 
        (&apos;Z&apos;, 3.), (&apos;Z&apos;, 5.), (&apos;Z&apos;, 8.), (&apos;Z&apos;, 12.) 
       ]
rdd = sc.parallelize( data )

def mergeValue(x,value):
    print &apos;what is the x&apos;,x
    return x[0]+value,x[1]+1

sumCount = rdd.combineByKey(lambda value: (value, 1),
                            mergeValue,
                            lambda x, y: (x[0] + y[0], x[1] + y[1])
    )

sumCount.collect()
#[(&apos;A&apos;, (15.0, 3)), (&apos;B&apos;, (30.0, 2)), (&apos;Z&apos;, (28.0, 4))]

averageByKey = sumCount.map(lambda (key, (totalSum, count)): (key, totalSum / count))

averageByKey.collectAsMap()
#{A: 5.0, B: 15.0,Z: 7.0}
</code></pre><p>我们按步骤解读一下：<br>1.Create a Combiner<br>lambda value:(value,1)<br>这个步骤定义了C的数据结构，也就是(sum,count)。</p>
<p>如果是一个新的元素，此时使用createCombiner()来创建那个键对应的累加器的初始值。（注意：这个过程会在每个分区第一次出现各个键时发生，而不是在整个RDD中第一次出现一个键时发生。）</p>
<p>2.Merge a Value<br>lambda x, value: (x[0] + value, x[1] + 1)<br>这个方法告诉combineByKey当给到一个新value的时候要做什么。方法的参数是一个combiner和一个新的value。combiner的数据结构在上一个方法定义了(sum,count)。</p>
<p>3.Merge two Combiners<br>lambda x, y: (x[0] + y[0], x[1] + y[1])<br>这个方法告诉combineByKey怎么合并两个combiners</p>
<p>内部流程如下：</p>
<!-- python -->
<pre><code>data = [
        (&quot;A&quot;, 2.), (&quot;A&quot;, 4.), (&quot;A&quot;, 9.), 
        (&quot;B&quot;, 10.), (&quot;B&quot;, 20.), 
        (&quot;Z&quot;, 3.), (&quot;Z&quot;, 5.), (&quot;Z&quot;, 8.), (&quot;Z&quot;, 12.) 
       ]

Partition 1: (&quot;A&quot;, 2.), (&quot;A&quot;, 4.), (&quot;A&quot;, 9.), (&quot;B&quot;, 10.)
Partition 2: (&quot;B&quot;, 20.), (&quot;Z&quot;, 3.), (&quot;Z&quot;, 5.), (&quot;Z&quot;, 8.), (&quot;Z&quot;, 12.) 


Partition 1 
(&quot;A&quot;, 2.), (&quot;A&quot;, 4.), (&quot;A&quot;, 9.), (&quot;B&quot;, 10.)

A=2. --&gt; createCombiner(2.) ==&gt; accumulator[A] = (2., 1)
A=4. --&gt; mergeValue(accumulator[A], 4.) ==&gt; accumulator[A] = (2. + 4., 1 + 1) = (6., 2)
A=9. --&gt; mergeValue(accumulator[A], 9.) ==&gt; accumulator[A] = (6. + 9., 2 + 1) = (15., 3)
B=10. --&gt; createCombiner(10.) ==&gt; accumulator[B] = (10., 1)

Partition 2
(&quot;B&quot;, 20.), (&quot;Z&quot;, 3.), (&quot;Z&quot;, 5.), (&quot;Z&quot;, 8.), (&quot;Z&quot;, 12.) 

B=20. --&gt; createCombiner(20.) ==&gt; accumulator[B] = (20., 1)
Z=3. --&gt; createCombiner(3.) ==&gt; accumulator[Z] = (3., 1)
Z=5. --&gt; mergeValue(accumulator[Z], 5.) ==&gt; accumulator[Z] = (3. + 5., 1 + 1) = (8., 2)
Z=8. --&gt; mergeValue(accumulator[Z], 8.) ==&gt; accumulator[Z] = (8. + 8., 2 + 1) = (16., 3)
Z=12. --&gt; mergeValue(accumulator[Z], 12.) ==&gt; accumulator[Z] = (16. + 12., 3 + 1) = (28., 4)

Merge partitions together
A ==&gt; (15., 3)
B ==&gt; mergeCombiner((10., 1), (20., 1)) ==&gt; (10. + 20., 1 + 1) = (30., 2)
Z ==&gt; (28., 4)

最终结果为：
Array( [A, (15., 3)], [B, (30., 2)], [Z, (28., 4)])
</code></pre>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>浅谈SPARK 的 aggregate 算子</title>
    <url>/2016/spark_aggregate.html</url>
    <content><![CDATA[<h1 id="理解aggregate的原理"><a href="#理解aggregate的原理" class="headerlink" title="理解aggregate的原理"></a>理解aggregate的原理</h1><p>刚开始我觉得SPARK的aggregate算子比较难理解的。首先我们看看官网的example</p>
<!-- python -->
<pre><code>&gt;&gt;&gt; seqOp = (lambda x, y: (x[0] + y, x[1] + 1))
&gt;&gt;&gt; combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))
&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)
(10, 4)
&gt;&gt;&gt; sc.parallelize([]).aggregate((0, 0), seqOp, combOp)
(0, 0)    
</code></pre><p>看了后，我就一脸懵逼了，两个lambda函数中的x,y各自代表什么东西呢？</p>
<p>我们先看看官网的aggregate 用法说明：</p>
<p>Aggregate the elements of each partition, and then the results for all the partitions, using a given combine functions and a neutral “zero value.”</p>
<p>The functions op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object allocation; however, it should not modify t2.</p>
<p>The first function (seqOp) can return a different result type, U, than the type of this RDD. Thus, we need one operation for merging a T into an U and one operation for merging two U</p>
<a id="more"></a>
<p>zero value：初始值，就是你要结果的类型。上面例子的zero value是(0,0)</p>
<p>seqOp:对RDD里每个partition里要实施的操作。</p>
<p>combOp:对所有的partition汇总的操作</p>
<p>我们再进一步解读官网的那个例子，仔细说说那些x，y分别代表什么<br>这个例子的意图是算出一个列表里所有的元素的和，还有列表的长度。</p>
<p>在pyspark里，我们执行以下代码</p>
<!-- python -->
<pre><code>#我们创建一个有4个元素的list，并且分为2个partition
listRDD = sc.parallelize([1,2,3,4], 2)

#然后我们定义seqOp
seqOp = (lambda local_result, list_element: (local_result[0] + list_element, local_result[1] + 1) )
#上面你可以更直观的看到x，y分别代表什么了。local_result也就是初始值（0，0）；list_element就是每个元素1,2,3,4..

#然后是combOp：
combOp = (lambda some_local_result, another_local_result: (some_local_result[0] + another_local_result[0], some_local_result[1] + another_local_result[1]) )
#从变量的名字你应该可以直观的看出x，y分别代表什么了吧

#然后aggregate一下：
listRDD.aggregate( (0, 0), seqOp, combOp)
Out[8]: (10, 4)
</code></pre><p>第一个分区的子列表是[1,2]，我们实施seqOp的时候会对这个子列表产生一个本地（可以认为这是一个节点服务器上）的result，result也就是（sum,length)。第一个分区的local result 是(3,2)。</p>
<p>是这么算的：<br>(0+1,0+1)=&gt; (1,1)<br>(1+2,1+1)=&gt; (3,2)</p>
<p>第二个分区的子列表是[3,4]<br>同理也可以算出第二个分区的local result是(7,2)</p>
<p>然后是到combOp将两个本地结果汇总。计算过程是：<br>(3+7,2+2) =&gt;(10,4)</p>
<h3 id="这里有个特别注意的地方"><a href="#这里有个特别注意的地方" class="headerlink" title="这里有个特别注意的地方"></a>这里有个特别注意的地方</h3><p>如果你的zero value不是(0,0)，而是(1,0)结果会有点出乎意料，这个例子中，结果并不是(12,4)，而是(13,4)。如果你的partition为3的话结果是(14,4)。这应该是aggregate 会根据分区数在多运算几次</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>aggregate 可以用来先计算每个partition的本地结果，然后再汇总。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>pyspark</tag>
      </tags>
  </entry>
  <entry>
    <title>Python的yield 问题</title>
    <url>/2016/python_yield.html</url>
    <content><![CDATA[<h2 id="Python-yield示范"><a href="#Python-yield示范" class="headerlink" title="Python yield示范"></a>Python yield示范</h2><p>yield 关键词在python中不算常用。以前我只知道它的作用和return相似，就是返回一个值。并不知道它具体的用途和用法。</p>
<p>要理解yield，你必须知道generators（生成器）。而要知道生成器，需要了解iterables（可迭代对象）</p>
<p>生成器（Generators）就是迭代器的一种特殊形式，只不过它只能迭代一次。这是因为生成器不是将所有的数值存储在内存里，而是，立即生成数值。<br>和迭代器相似，我们可以通过使用next()来从generator中获取下一个值</p>
<p>顺便说说：集合数据类型，如list、tuple、dict、set、str等，这些可以用for循环的对象都是可迭代对象（Iterable）。<br>但list、dict、str虽然是Iterable，却不是迭代器（Iterator）。把list、dict、str等Iterable变成Iterator可以使用iter()函数。</p>
<p>迭代器对象可以被next()函数调用并不断返回下一个数据，直到没有数据时抛出StopIteration错误。</p>
<p>Python的for循环本质上就是通过不断调用next()函数实现的</p>
<p>我们看看下面例子：</p>
<a id="more"></a>
<p>测试环境是ubuntu;<br>Python版本为2.7<br><!-- python --></p>
<pre><code>def yield_test1():
    print &apos;one&apos;
    yield 1
    print &apos;two&apos;
    yield 2
    print &apos;three&apos;
    yield 3
    print &apos;is any thing?&apos;

&gt;y=yield_test()
&gt;y.next()   #第一次调用next 返回的是第一个yield，返回的是1
&gt;one
&gt;[out]1
&gt;y.next()   #第二次调用next 返回的是第二个yield，返回的是2
&gt;two
&gt;[out]2
&gt;y.next()   #第三次调用next 返回的是第三个yield，返回的是3
&gt;three
&gt;[out]3
&gt;y.next()   #第四次调用next，就不一样了，会报错StopIteration。因为已经没有东西可以yield了。但是注意，还是可以print &apos;is any thing&apos;的。
&gt;is any thing?
&gt;StopIteration                             Traceback (most recent call last)
</code></pre><p>我的理解：yield和return的却别在于，yield是有记忆的返回东西。因为它记忆了上一次yield的值。</p>
<p>当一个生成器函数调用yield，生成器函数的“状态”会被冻结，所有的变量的值会被保留下来，下一行要执行的代码的位置也会被记录，直到再次调用next()。一旦next()再次被调用，生成器函数会从它上次离开的地方开始。如果永远不调用next()，yield保存的状态就被无视了。</p>
<p>接下来，我们运用yield在做个实例：</p>
<p>求出所有小于2000000的质数的和1+2+3+5+7+11+13+….<br>通常，我们的思路是这样for i in [所有小于2000000的质数]<br>sum+=i</p>
<!-- python -->
<pre><code>#首先，我们定义 判断质数的方法
def is_prime(number):
if number &gt; 1:
    if number == 2:
        return True
    if number % 2 == 0:
        return False
    for current in range(3, int(math.sqrt(number) + 1), 2):
        if number % current == 0:
            return False
    return True
return False

all_prime=[]#用一个list来装载所有的质数
for i in range(1,2000000):
    if is_prime(i):
        all_prime.append（i）

len(all_prime) #len看了下大概有66W个质数
sum=0
for i in all_prime:
    sum+=i
</code></pre><p>这种方法也可以。只不过非常消耗资源。因为要把这么大一堆质数装载到一个容器里，这占了很多空间。</p>
<p>我们换成另一种方法：<br><!-- python --></p>
<pre><code>def sum_prime(maxnum):
    total = 0
    for next_prime in get_primes(2):#质数从2开始
        if next_prime &lt; int(maxnum):
            total += next_prime
        else:
            print(total)
            return

def get_primes(number):
    while True:
        if is_prime(number):
            yield number
        number += 1 

sum_prime(2000000)
</code></pre><p>这样就节省了很多资源，中间也不用管它到底是有哪些质数，我只要得出总的和，结果 就够了。</p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>聊聊python的unicode 问题</title>
    <url>/2016/python_unicode.html</url>
    <content><![CDATA[<h2 id="Python的中文字符处理"><a href="#Python的中文字符处理" class="headerlink" title="Python的中文字符处理"></a>Python的中文字符处理</h2><p>都说python对中文不太友好。</p>
<p>我们看看下面两个例子：<br>测试环境是ubuntu;<br>python版本为2.7<br><!-- python --></p>
<pre><code>》a = &apos;呵呵&apos;
》a 
》&apos;\xe5\x91\xb5\xe5\x91\xb5&apos;
》print a
》呵呵
</code></pre><p>为什么print 和 直接输入变量回车显示的东西不一样呢？<br>\xe5\x91\xb5\xe5\x91\xb5这些是什么东西啊。一定很奇怪。其实bytes字符编码，是8进制的东西。</p>
<a id="more"></a>
<p>首先’呵呵‘ 字符串赋值给a。然而python 内部是以’\xe5\x91\xb5\xe5\x91\xb5’保存的，这应该是utf-8（因为ubuntu默认为uhf-8）的unicode 对应的8进制字符编码</p>
<p>我们再看看：<br><!-- python --><br>    》a=u’呵呵’  #带上u表示将这字符串转为unicode<br>    》a<br>    》u’\u5475\u5475’<br>    》print a<br>    》呵呵</p>
<p>我们发现print真是厉害，不管是字符串 还是 unicode 它都能显示为中文的字符。而回车显示的u’\u5475\u5475’是什么呢？这是’呵呵’对应的uncode编码</p>
<p>我们知道unicode可以 通过encode 编码来转为str</p>
<!-- python -->
<pre><code>》a.encode(&apos;utf-8&apos;)
》&apos;\xe5\x91\xb5\xe5\x91\xb5&apos;
》print a.encode(&apos;utf-8&apos;)
》呵呵
</code></pre><p>我们看到encode后显示的str和我们最开始显示的’\xe5\x91\xb5\xe5\x91\xb5’一样。这说吗中文的字符串在python里就是用\x这种字符串编码保存的</p>
<p>print 可以把这种编码自动地解码并且显示为对应的中文字符</p>
<p>另外，有些时候会碰到这种情况，请看下面：</p>
<!-- python -->
<pre><code>》a=u&apos;呵呵&apos;
》l=[a]
》s=str(l)
》s
》&quot;[u&apos;\\u5475\\u5475&apos;]&quot;
</code></pre><p>哟，怎么多了个\<br>有时候，python会以这种编码保存，这不知道是什么编码，多了一个斜杠的<br>遇到这种情况我们可以用如下办法<br><!-- python --></p>
<pre><code>》s.decode(&apos;unicode_escape&apos;)
》s
》u&quot;[u&apos;\u5475\u5475&apos;]&quot;
</code></pre><p>这样就可以把它变回来了。此外我们可以print s一下看看会出现什么呢？<br><!-- python --></p>
<pre><code>》print s
》[u&apos;\u5475\u5475&apos;]
</code></pre><p>哈，其实print 就是把不好读的编码优化为比这种编码更好读的一种形式</p>
<h3 id="总结：print就是让显示出来的东西尽量友好。没那么多乱七八糟"><a href="#总结：print就是让显示出来的东西尽量友好。没那么多乱七八糟" class="headerlink" title="总结：print就是让显示出来的东西尽量友好。没那么多乱七八糟"></a>总结：print就是让显示出来的东西尽量友好。没那么多乱七八糟</h3>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>聊聊python的变量问题</title>
    <url>/2016/python_func_params.html</url>
    <content><![CDATA[<h2 id="Python的参数作用范围"><a href="#Python的参数作用范围" class="headerlink" title="Python的参数作用范围"></a>Python的参数作用范围</h2><p>我们看看下面两个例子：</p>
<!-- python -->
<pre><code>a = &apos;hello&apos;
def fun(a):
    a = &apos;lol&apos;
fun(a)
print a  # &apos;hello&apos;

a=[]
def fun(a):
    a.append(1)
fun(a)
print a  # [1]
</code></pre><p>为什么第一个例子print出来的是’hello’而不是’lol 呢?<br>为什么第二个例子print出来却是[1] 呢?</p>
<a id="more"></a>
<p>我们通过id() 来查看内存的引用：</p>
<!-- python -->
<pre><code>a = &apos;hello&apos;
def fun(a):
    print &quot;point&quot;,id(a)   # point 213224723696912
    a = 2
    print &quot;re-point&quot;,id(a), id(2)   # re-point 2132244845844160 2132244845844160
print &quot;point&quot;,id(a), id(1)  # point 213224723696912 213224723696912
fun(a)
print a  # &apos;hello&apos;
</code></pre><p>可以看到，在执行完a=’hello’之后，a的引用中保存的值，即内存地址发生变化，由原来’hello’对象的所在的地址变成了’lol这个实体对象的内存地址。</p>
<p>而第2个例子a引用保存的内存值就不会发生变化：<br><!-- python --></p>
<pre><code>a = []
def fun(a):
    print &quot;point&quot;,id(a)  # point 53629256
    a.append(1)
print &quot;point the same&quot;,id(a)     # point the same 53629256
fun(a)
print a  # [1]
</code></pre><p>这里记住的是类型是属于对象的，而不是变量。而对象有两种,“可更改”（mutable）与“不可更改”（immutable）对象。在python中，strings, tuples, 和numbers是不可更改的对象，而list,dict等则是可以修改的对象。</p>
<p>当一个引用传递给函数的时候,函数自动复制一份引用,这个函数里的引用和外边的引用没有半毛关系了.所以第一个例子里函数把引用指向了一个不可变对象,当函数返回的时候,外面的引用没半毛感觉.<br>而第二个例子就不一样了,函数内的引用指向的是可变对象,对它的操作就和定位了指针地址一样,在内存里进行修改.</p>
<p>在类变量和实例变量中也有类似的例子：<br><!-- python --></p>
<pre><code>class Person:
    name=&quot;aaa&quot;

p1=Person()
p2=Person()
p1.name=&quot;bbb&quot;
print p1.name  # bbb
print p2.name  # aaa
print Person.name  # aaa
</code></pre><p>类变量就是供类使用的变量,实例变量就是供实例使用的.</p>
<p>这里p1.name=”bbb”是实例调用了类变量,这其实和上面第一个问题一样,就是函数传参的问题,p1.name一开始是指向的类变量name=”aaa”,<br>但是在实例的作用域里把类变量的引用改变了,就变成了一个实例变量,self.name不再引用Person的类变量name了.</p>
<p>可以看看下面的例子:<br><!-- python --></p>
<pre><code>class Person:
    name=[]

p1=Person()
p2=Person()
p1.name.append(1)
print p1.name  # [1]
print p2.name  # [1]
print Person.name  # [1]
</code></pre>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark简易入门</title>
    <url>/2016/spark_guide.html</url>
    <content><![CDATA[<h1 id="Spark简易入门"><a href="#Spark简易入门" class="headerlink" title="Spark简易入门"></a>Spark简易入门</h1><p>Spark，搞大数据的人都应该用过吧。spark作为主流的大数据处理框架，到底为什么这么多人用呢？我们少扯淡，直接动手写大数据界的HelloWorld：WordCount。</p>
<p>先贴上代码（Scala版本):</p>
<!-- java -->
<pre><code>import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
object WordCount 
  def main(args: Array[String]) {
    val conf = new SparkConf()
    conf.setAppName(&quot;WorkCount&quot;)
    val sc = new SparkContext(conf)

    val file = &quot;hdfs://127.0.0.1:9000/file.txt&quot;
    val lines = sc.textFile(file)
    val words = lines.flatMap(_.split(&quot;\\s+&quot;))
    val wordCount = words.countByValue()
    println(wordCount)
}
</code></pre><p>短短10多行代码，就已经完成了，比大家想象的要简单，完全看不出Spark背后做了什么处理。分布式，容错处理，这就是Spark给我们带来的福利。</p>
<a id="more"></a>
<h3 id="接下来我们了解下Spark的核心："><a href="#接下来我们了解下Spark的核心：" class="headerlink" title="接下来我们了解下Spark的核心："></a>接下来我们了解下Spark的核心：</h3><ul>
<li>Spark上下文</li>
</ul>
<p>Spark集群的执行单位是<strong>Application</strong>，任何提交的任务都会产生一个Application。一个Application只会关联上一个Spark上下文，也就是SparkContext。构建SparkContext时可以传入Spark相关配置，也就是SparkConf，它可以用来指定Application的名称，任务需要集群的CPU核数/内存大小，调优需要的配置等等。</p>
<p><code>val conf = new SparkConf()</code><br><br><code>conf.setAppName(&quot;WorkCount&quot;)</code><br><br><code>val sc = new SparkContext(conf)</code><br></p>
<p>这三行语句创建了一个Spark上下文，并且运行时这个Application的名字就叫WordCount。</p>
<ul>
<li>弹性分布式数据集RDD</li>
</ul>
<p>Spark中最主要的编程概念就是弹性分布式数据集 (resilient distributed dataset,RDD)，它是元素的集合，划分到集群的各个节点上，可以被并行操作。RDD的创建可以从HDFS(或者任意其他支持Hadoop文件系统) 上的一个文件开始，或者通过转换Master中已存在的Scala集合而来。</p>
<p><code>val file = &quot;hdfs://127.0.0.1:9000/file.txt&quot;</code><br><br><code>val lines = sc.textFile(file)</code><br></p>
<p>这两行语句从HDFS文件中创建了叫lines的RDD，它的每个元素就对应文件中的每一行，有了RDD我们就可以通过它提供的各种API来完成需要的业务功能。</p>
<p>RDD提供的API分为两类：转换（Transformation）和动作（Action）。</p>
<ul>
<li>转换</li>
</ul>
<p>顾名思义，转换就是把一个RDD转换成另一个RDD。当然，光是拷贝产生一个新的RDD出来是没有太大意义的，这里的转换实际上是RDD中元素的映射和转换。有一点必须要注意的是，RDD是只读的，一旦执行转换，一定会生成一个新的RDD。<br></p>
<p><code>val words = lines.flatMap(_.split(&quot;\\s+&quot;))</code><br></p>
<p>flatMap是RDD众多转换中的一种，它的功能是把源RDD中的元素映射成目的RDD中的0个或者多个元素。上面语句把以文本行为元素的RDD转换成了以单个单词为元素的RDD。</p>
<ul>
<li>动作</li>
</ul>
<p>“动作”就不好望文生义了，可以简单地理解成想要获得结果时调用的API。</p>
<p><code>val wordCount = words.countByValue()</code><br></p>
<p>countByValue就是一个“动作”，它的功能是统计RDD中每个元素出现的次数，得到一个元素及其出现次数的Map。</p>
<p><strong>提示：返回结果为RDD的API是转换，返回结果不为RDD的API是动作。</strong></p>
<ul>
<li>运行</li>
</ul>
<p>要运行Spark任务，首先要把代码打成JAR包，额。。。这个不需要多言。</p>
<p>打包后，就只需在Spark集群上以命令行的方式用spark-submit提交就OK。</p>
<p><code>spark-submit --class &quot;demo.WordCount&quot; SparkDemo-1.0-SNAPSHOT.jar</code><br></p>
<p>其中demo.WordCount是main函数所在的ojbect，而SparkDemo-1.0-SNAPSHOT.jar就是打出来的jar包。</p>
<p>Spark 最简单的流程，就这样Finish了。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Streaming Dstream ForeachRDD的理解</title>
    <url>/2016/spark_foreachRDD.html</url>
    <content><![CDATA[<h1 id="通俗理解Spark-Streaming-Dstream-的ForeachRDD"><a href="#通俗理解Spark-Streaming-Dstream-的ForeachRDD" class="headerlink" title="通俗理解Spark Streaming Dstream 的ForeachRDD"></a>通俗理解Spark Streaming Dstream 的ForeachRDD</h1><p>在SparkStreaming中，数据中的每个batch都是只有一个RDD，为什么我们还要用ForeachRDD 在每一个RDD呢？ 不是只有一个RDD吗？</p>
<p>Dstream也就是离散stream，就是把连续的数据分成一小团一小团。我们用专业术语“microbatching”来描述。每个microbatch 变成一个RDD以便Spark的后续处理。在每一个batch interval中，每个DStream有且仅有一个RDD。</p>
<p><img src="/images/2016/spark_foreachRDD/spark-streaming.png" alt="Sample Image Added via Markdown"></p>
<p>然而RDD是什么呢，RDD是一个分布式数据集合。你可以认为它是一个告诉你实际数据在集群中具体什么地方的指南者（pointer）。</p>
<a id="more"></a>
<p>Dstream.foreachRDD在SparkStreaming中是一个“output operator”，它让你直接基于Dstream的RDDs去处理数据。比如说，用foreachRDD方法将数据写入数据库</p>
<p>这里有个容易疑惑的地方，DStream是和时间有关的集合。我们来对比一下传统的集合。我们用一群客户（users）来举个例子：</p>
<!-- scala -->
<pre><code>val userDStream：DStream[User]=???
userDStream.foreachRDD{usersRDD =&gt;
    usersRDD.foreach{user =&gt; serveCoffee(user)}
}
</code></pre><p>注意：</p>
<ul>
<li>DStream.foreachRDD返回给你的是RDD[user],不是单单一个user。回到上面咖啡的这个例子，这个客户集合 是指某个时间区间(比如下午一点到两点)的集合</li>
<li>为了处理单个集合的元素，需要进一步操作(operate) RDD。在这个案例中，我们用RDD.foreach来对每个客户serve coffee</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>怎么变得更有效率 ————Aaron Swarts</title>
    <url>/2016/be_more_productive.html</url>
    <content><![CDATA[<h1 id="How-to-be-More-Productive–Aaron-Swartz"><a href="#How-to-be-More-Productive–Aaron-Swartz" class="headerlink" title="How to be More Productive–Aaron Swartz"></a>How to be More Productive–Aaron Swartz</h1><p>原文在 <a href="http://www.aaronsw.com/weblog/productivity" target="_blank" rel="external">http://www.aaronsw.com/weblog/productivity</a> </p>
<p>有人跟我说：“你花在看电视上的时间足够用来写本书了。”毫无疑问，把时间花在写书上花在看电视上更好。但这里隐含了一个假设，即时间是“可互换的”。也就是说，看电视的时间可以轻松地用来写书。但悲催的是事实并非如此。</p>
<p>不同的时间有不同的质量等级。如果我正走向地铁站而且忘带笔记本了，我就很难写什么文章。同样，如果你不停地被打断，也很难集中注意力。另外还有些心理和情感上的因素：有时候我心情不错，就愿意去主动做一些事；也有些时候我心情郁闷，就只能看看电视了。</p>
<p>如果你想变的更加有效，你必须意识到这个事实，并很好的处理它。首先，你得利用好不同类型的时间。其次，你得提高时间的质量。</p>
<h2 id="1-更有效的利用你的时间"><a href="#1-更有效的利用你的时间" class="headerlink" title="1. 更有效的利用你的时间"></a>1. 更有效的利用你的时间</h2><p>1.1 选择好问题</p>
<a id="more"></a>
<p>生命是如此的短暂（别人这么告诉我），为什么浪费时间去做那些没有意义的事呢？做一些让你感到舒适的事是很容易的，但你应该不断地问自己为什么要做这些事呢？有没有更重要的事情等着你去做呢？为什么你不去做那些事呢？这些问题很难回答（如果你遵循这个规则，慢慢地你就不得不问自已为什么没去做那些世界上最重要的事了），但是每一个小小的进步都会让你更加有效。</p>
<p>这不是说你所有的时间都必须用来做那些最重要的事。我的时间肯定就不是这样的（毕竟，我现在还在写这篇文章呢）。但这是我衡量自己生活的明确标准。</p>
<p>1.2  收集很多问题</p>
<p>另一个公开的秘密是：如果你认准一件事并集中精力只做这一件事，你的效率就是最高的。但我发现这是很不现实的。就以现在为例吧，我正在调整坐姿，锻炼身体，喝水，清理桌面，和我弟弟聊天，同时还在写这篇文章。今天一整天，我写了现在这篇文章，读了本书，吃了点东西，回了几封邮件，和一些朋友聊了聊天，买了点东西，改了改其他几篇文章，备份了硬盘，还整理了一下图书列表。在过去的一周里，我做了好几个不同的软件项目，读了好几本书，学习了好几种不同的编程语言，等等。</p>
<p>有很多不同的项目能让我能在不同质量的时间下做不同的工作。而且，在你卡壳或是厌烦的时候有其他的一些事可以做（你可以给你的大脑一些时间来放松）。</p>
<p>同时这会让你变得更有创造力。创造力就是把你自己从其他地方学到的东西用到你正在做的工作中。如果你同时做许多不同方向的工作，那你就会得到更多的想法和创意。</p>
<p>1.3  列出清单</p>
<p>找一些不同的事同时做并不困难，大部分人都有很多很多事要做。但是如果你想把它们全部记在脑袋里的话，它们很快就会消失的。想记住所有这些事给你带来的心理压力会把你逼疯的。解决办法很简单：把它们写下来。<br>一旦你把要做的事列成清单，你就可以更好地进行分类组织了。例如，我的清单包括：编程、写作、思考、跑腿、阅读，聆听、观看等。</p>
<p>大部分项目都包括很多不同的任务。以写这篇文章为例，除了纯粹的文字写作外，还包含阅读其他关于拖延的文章，构思文章的结构，润色语句，写邮件向别人请教问题等。每一项任务都属于清单的不同部分，所以你可以在合适的时间再去做。</p>
<p>1.4  把清单和生活结合起来</p>
<p>一旦你有了这个清单，你就要经常记得看它。记得看它的最好方法是把它放在你能看到的地方。例如，我总是在桌子上堆一摞书，最上面的那一本就是我最近在读的。当我想要读书的时候，我就直接把最上面的那一本抓过来。</p>
<p>我看电视和电影时也这么做。如果想看某部电影，我就会把它放在电脑中一个专门的文件夹里。当我想休息一下看看电影的时候，我就会打开那个文件夹。</p>
<p>我也想过一些更强制性的方法，比如说我想查看博客，会弹开一个页面，列出我“待读”文件夹里的文章。或者当我不小心犯了错时，就弹开一个窗口，提出工作建议。</p>
<h2 id="2-提高你时间的质量"><a href="#2-提高你时间的质量" class="headerlink" title="2 提高你时间的质量"></a>2 提高你时间的质量</h2><p>像上面那样最大限度的利用时间还远远不够，更重要的是提高你自己的时间的质量。大多数人的时间都被上学、工作之类的事情吃掉了。如果你属于其中之一，你必须停下来。但你还能做什么呢？</p>
<p>2.1 减轻身体上的约束</p>
<p>2.1.1 携带纸和笔</p>
<p>我认识的很多人都有随身携带笔记本之类东西的的习惯。纸和笔在很多时候都是非常有用的，你可以给某人写点什么东西、针对什么做点记录、写下自己的想法等。我甚至在地铁上写过一整篇文章。（我以前是这样的，但我现在只用带智能手机。它不用让我给人物理信息，但可以一直给我提供读的东西，我可以把笔记直接写在收件箱里）</p>
<p>2.1.2  避免被打扰</p>
<p>对于那些需要集中注意力的任务，你应该避免被打扰。一个很简单的方法是去一个没人能打扰你的地方，另一个方法是告诉周围的人“关门的时候不要打扰”或“我戴耳机时给我发消息”（然后你在有空的时候再看消息）<br>这一点不要做过了。当你浪费时间的时候你反倒应该被打扰一下，帮助别人解决问题肯定比坐在那里看新闻更好的利用了时间。所以可以达成一个专门的协议：当你没有集中精力的时候你可以被打扰。</p>
<p>2.2  减轻心理上的约束</p>
<p>2.2.1  吃、睡和锻炼</p>
<p>当你感到很饿、很累、很焦躁的时候，你的时间的质量会很低。解决这个问题很简单，就是：吃、睡和锻炼。但我有时候做得不好，虽然觉得很饿了，但我还是一直工作而不想吃东西，结果最后实在太累了都没法吃东西了。</p>
<p>对自己说“虽然我很累了，但我不能休息，因为我必须要工作”会让你觉得自己很努力，但事实上休息之后你的效率会更高。既然你迟早都要睡觉，还不如先休息好，再来提高剩余时间内的效率。</p>
<p>我锻炼其实不多，所以不好给出建议，但我仍尽力做好。我躺着读书时，我就做仰卧起坐。我要步行去什么地方时，我就跑步。</p>
<p>2.2.2 与快乐的人谈话</p>
<p>减轻精神负担是很难的，与快乐的人做朋友可起到帮助。比如，我在和Palu Graham或Dan Connolly交谈后总是更乐于工作，他们总是释放正能量。也许有人愿意关在屋子里埋头苦干，不与其他人接触，他们觉得这样时间才没有被“浪费”，但事实上这会让他们变得情绪低落，工作效率也会大大下降。</p>
<p>2.2.3 分担压力</p>
<p>即便你的朋友不能给你带来快乐，和其他人一起做事也会让难题变简单。一方面，精神上的压力大家可以互相分担，另一方面，和其他人在一起可以让你专注于工作而不是时常分心。</p>
<p>2.3  拖延和精神力场</p>
<p>上面所说的那些并不是问题的核心，关于效率大家最大的问题还是“拖延”。虽然很多人不承认，但是几乎所有人都会或多或少拖延，不只是你。但这不意味着你不必避免它。</p>
<p>拖延是什么？从旁观者来看，你在玩（如玩游戏，看新闻）而不是在做事，这让别人以为你很懒、很糟糕。但问题的关键是：为什么会这样呢？你的脑子里究竟是怎么想的？</p>
<p>我花了很多时间来研究这件事，我能给出的最好解释是你的大脑赋予每项任务一种“精神力场”。你玩过两块磁铁相互作用吗？如果你让它们异极相对，他们就会相互排斥，你会感到他们之间的磁场力。你越是想要把它们合在一起，越会感到它们之间的排斥。</p>
<p>精神上也是类似。你看不见摸不着它，但你却可以感受到它的存在。你越是想要接近它，它会离你越远。</p>
<p>你不可能通过蛮力来克服两个场之间的排斥力，你一不用力了它们就会转过来。我也从来没有通过纯粹的自制力来克服这种精神力场。其实，你不应该强制，你应该悄悄地调转方向。</p>
<p>那又是什么产生了这种精神力场呢？似乎有两个主要原因：任务是否艰巨，任务是否是被指派的。</p>
<p>2.3.1 艰巨的任务</p>
<p>2.3.1.1 把任务细分</p>
<p>一个任务很艰巨的原因之一是这个任务很庞大。比如说你想要做一个菜谱构造程序，没有人能坐下来就完成一个菜谱构造器。这是一个目标，不是一项任务。任务是使你能够朝向目标更迈进的具体步骤。一个好的任务应该类似于“画出展示菜谱的屏幕的模型”，这是你能够立即做的。</p>
<p>当你完成了一个任务后，下一步就会变得更加清晰。你将会考虑一个菜谱由什么构成，你需要什么样的搜索机制，如何构建菜谱的数据库，等等。这样你就构建了一个引擎，每一个任务都会通向下一个任务。</p>
<p>对于每一个大项目，我都会考虑我需要完成一系列什么样的任务，并且将这些任务加入到我分类的待办事项列表中去。同样，当我做完一些任务之后，我会把接下来需要完成的任务再加入任务列表中去。</p>
<p>2.3.1.2  简化任务</p>
<p>另一个让任务变得艰巨的原因就是它太复杂了。写书这个任务会放你无所适从，那么就先从写文章开始吧。如果一篇文章也觉得太复杂了，那么就先写一个段落的概要吧。最重要的是真正做了一些工作，真正的有进展。</p>
<p>一旦你明确了你的任务之后，你就可以更清楚的判断它，更容易的理解它。完善现有的东西比从头创建东西更容易。如果你的一个段落写好了，那么一点一点积累，它会变成一篇文章，最终变成一本书。</p>
<p>2.3.1.3  认真考虑它</p>
<p>通常来说解决一个难题需要些灵感。如果你对那个领域并不熟悉，你应该从研究这个领域开始，借鉴一下其他人的经验，慢慢的理解这个领域，并且做一些小的尝试看看你能否搞定这个领域。</p>
<p>2.3.2  被指派的任务</p>
<p>被指派的任务是那些别人要求你做的任务。很多心理学实验都表明：当你“刺激”其他人做什么事的时候，他们反倒不容易做好那个事。奖励，惩罚等外部刺激会扼杀“内在动机”——你对于某个问题发自内心的兴趣。（这是社会心理学中最完全重复的发现之一了——70多项研究发现奖励会减弱任务的兴趣） 。人类的大脑对于被要求做的事有先天的抗拒力 。</p>
<p>奇怪的是这种现象不仅局限于其他人要求你做的事，当你向自己分配任务时仍然会出现这种现象。如果你对自己说“我应该好好做X工作了，这是我现在最重要的事”，之后你就会感到X突然变成了世界上最困难的事情了。然而一旦当Y变成了“最重要的事”，原来的那个X又变得简单了。</p>
<p>2.3.2.1  虚构一个任务</p>
<p>如果你要完成X，那就告诉自己做Y。然而不幸的是，这样欺骗自己也很难，因为你清楚你究竟要做什么。所以你必须悄悄地做。</p>
<p>一个方法是让别人给你分配点什么事情。最著名的例子就是毕业生必须要写篇论文才能毕业，这是一项很难的任务，为了不做这件事，研究生结果做了很多其他很难的工作。</p>
<p>这项工作必须看上去非常重要（不做就不能毕业）也非常艰巨（你最好的工作写上百页），但实际上没那么重要，放下来也不会成为什么灾难。</p>
<p>2.3.2.2  不要自己给自己布置任务</p>
<p>给自己布置任务看起来很诱人，比如对自己说“好吧，我要把这些放在一边，坐下来将这篇文章写完”。更糟糕的是给自己一些奖励，比如“如果我写完这篇文章，我就吃点糖果。”最糟糕的是让别人假装布置给你一些任务。</p>
<p>这些方式都很诱人——我自己完成了——但是这些都会让你变得更没有效。在这三种情况下，你还是在给自己布置任务，你的大脑只会去逃避它。</p>
<p>2.3.2.3 把事情变得有趣</p>
<p>困难的工作听起来不会令人感到愉悦，但事实上这可能就是最能让我感到高兴的事。一个困难的问题不但能让你集中全部注意力，而且当你完成它的时候你会感到非常棒，非常有成就感。</p>
<p>所以帮助自己完成一件事的秘密不是说服自己必须完成它，而是说服自己这件事确实非常有意思。如果一件事没有意思的话，你需要做的就是让它变得有意思。</p>
<p>我最开始认真考虑这这个问题是我在大学与论文的时候。写论文不是一个特别难的任务，但的确是被分配的。谁愿意去写什么两本毫无关系的书有什么关系呢。所以我开始将论文编到我自己的小把戏中。比如，我决定每一段都写出自己的小风格，尽力去模仿各种形式的演讲。</p>
<p>让事情更有趣的另一个方法是解决元问题。不要去构造一个WEB应用，而是构造一个WEB应用框架，将之作为一个示范应用。这种任务不但更有趣，而且更有用。</p>
<h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h2><p>高效有很多神话——什么时间是可替换的、集中精力是好的、奖励你自己是好的、艰巨的工作是不爽的，拖延是不自然的——但它们都有一个共同的主题：真实的工作违反了你内心的倾向。</p>
<p>对大多数人在大多数工作中，这也许成立。你没有理由写无聊的文章或归档无意义的纪要，如果社会强迫你做，你需要学会关闭你头脑中让你停止的声音。</p>
<p>但如果你正在做一些有意义的有创造性的事情，关闭你的大脑就是错误的。效率的真正秘密在于“聆听自己”，在你饿的时候吃饭，在你疲惫的时候睡觉，当你厌烦的时候休息一下，做那些有趣好玩的项目。</p>
<p>这看起来太容易了。它不包含任何花哨的缩写、或自制力、或个人的成功经验。但是社会上的一些观念正在把我们向相反的方向引导。要想变得更加有效，我们需要做的就是转过头来聆听自己。</p>
<p>嗯。</p>
]]></content>
      <categories>
        <category>study</category>
      </categories>
      <tags>
        <tag>daily</tag>
      </tags>
  </entry>
</search>
