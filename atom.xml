<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Dennis&#39; Blog</title>
  
  <subtitle>欢迎各位领导前来指导工作</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://mikolaje.github.io/"/>
  <updated>2019-11-20T06:09:57.423Z</updated>
  <id>http://mikolaje.github.io/</id>
  
  <author>
    <name>Dennis</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Tmux简明教程</title>
    <link href="http://mikolaje.github.io/2019/tmux_guide.html"/>
    <id>http://mikolaje.github.io/2019/tmux_guide.html</id>
    <published>2019-09-02T04:07:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>你经常可能会遇到这样的情况：你在vim编辑你在远程服务器上面的代码，然后你要新开一个窗口再次ssh到<br>这个远程服务器来测试运行你的代码。此外，如果你的WIFI断线了，你的所有session都会挂掉，GG。<br>然后你又要麻烦地重新开两个session。</p><p>其实，上述问题都可以用tmux解决，一个可以提供如下功能的命令行工具：</p><ol><li>在一个terminal window内开启多个windows，panes。</li><li>在一个session内(这个session会一直保持着，即使断网的时候也会保持着)保持windows和panes。</li><li>能够共享session(这功能对结对编程来说真是太妙了！)，就是说，你在session上做得所有操作，<br>在另一台电脑上连接到同一个session，会看到同样的输入和输出。</li></ol><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>Ubuntu用户只需要执行<code>sudo apt-get install tmux</code><br>Mac 用户只需要执行<code>brew install tmux</code></p><p>安装完后，执行如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tmux</div></pre></td></tr></table></figure></p><p><img src="/images/2019/tmux_guide/1dec3830.png" alt=""><br>这看起来和普通的终端差不多，除了底下有条绿色的状态bar。在这个bar上，我们可以运行tmux命令去控制管理终端windows和sessions</p><h3 id="Tmux基本命令："><a href="#Tmux基本命令：" class="headerlink" title="Tmux基本命令："></a>Tmux基本命令：</h3><p>当你进入到tmux后，你可以通过先运行一个prefix key来执行各种命令。默认来说，tmux的prefix key是 ctrl + b。也就是说在执行<br>其他任何命令之前你都要先同时按下ctrl + b。 </p><p>tmux的命令有很多很多，但首先我们了解下基本的就好了，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&lt;prefix&gt; c: 创建一个新的window（在status bar中显示）</div><div class="line">&lt;prefix&gt; 0: 切换到 window 0</div><div class="line">&lt;prefix&gt; 1: 切换到 window 1</div><div class="line">&lt;prefix&gt; 2: 切换到 window 2 (etc.)</div><div class="line">&lt;prefix&gt; x: Kill 当前的window </div><div class="line">&lt;prefix&gt; w: 切换 window</div><div class="line">&lt;prefix&gt; d: detach 到tmux（也就是说当你离开后，再次回到session中）</div></pre></td></tr></table></figure></p><p>如果你把在tmux session的所有的window都kill后，它将会kill整个session并且回到普通的终端上。<br>如果你用<prefix> d 去 detach到tmux，你将会回到你的tmux session，而且你可以看到你之前运行的东西已然在那里。<br>要查看所有的tmux sessions，你可以使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tmux ls</div></pre></td></tr></table></figure></prefix></p><p>要attach到最后一个使用的session，你可以用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tmux a</div></pre></td></tr></table></figure></p><p>要attach到指定的某个session，你可以用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tmux a -t &lt;session-name&gt;</div></pre></td></tr></table></figure></p><p>这里补充说明下 session 和 window之间的关系，我们看看这个图，<br>一个window相当于你的显示器能看到的所有东西，然后一个window上可以分成一块块的拼图，也就是各个panes：<br><img src="/images/2019/tmux_guide/5a861515.png" alt=""></p><p>这就这么一些。现在你可以通过多个终端window和永久的sessions来在你的远程电脑上工作了。<br>你甚至可以通过让两个人attach到同一个session来进行结对编程！</p><h3 id="下一步"><a href="#下一步" class="headerlink" title="下一步"></a>下一步</h3><p>上面的命令对掌握tmux真的非常有用。通常，我尽量少开窗口，一般是一个window用于vim，另一个window做开发，第3个窗口去运行命令。<br>大多数人倾向于用panes来在同一个屏幕上展现多个内容。</p><p>Panes是个特别炫酷的东西而且很值得学习，但是推荐你们先放一放，不要急着学习太多panes的东西，因为它会增加你的tmux学习曲线。<br>当你用panes时候，你要记住水平和垂直创建panes的命令，还有切换不同panes的命令，还有改变panes大小，关闭panes的命令，等等等等。<br>所以我建议先把其它的玩熟练再去用panes这些花哨的东西。</p><p>你也可以通过在home目录下添加<code>.tmux.conf</code>文件来指定tmux的一些配置。<br>比如通常可以在里面改变 prefix的快捷键（很多人会使用ctrl+a 作为prefix）。<br>初始window数设置为1，而不是0;然后还可以设置下颜色。<br>如果你打算重新绑定prefix，很有可能这将会为你以后形成肌肉记忆了，哈哈。</p><p>下面是一个精简版的.tmux.conf, 只改变了window数和 prefix:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># remap prefix from &apos;C-b&apos; to &apos;C-a&apos;</div><div class="line">unbind C-b</div><div class="line">set-option -g prefix C-a</div><div class="line">bind-key C-a send-prefix</div><div class="line"># Start window numbering at 1</div><div class="line">set -g base-index 1</div><div class="line"></div><div class="line"># 建议加上这个，用鼠标会有很大惊喜，呵呵。在多个panes的时候切换用鼠标点一下就好了！</div><div class="line"># 同时也解决了scroll的问题，可以轻松地用鼠标在不同的panes内滚动！</div><div class="line">set -g mouse on</div></pre></td></tr></table></figure></p><p>tmux有非常多的可选配置，但是你得由浅到深，刚开始尽可能精简。当你想尝试一些更高级的功能时，不要在网上复制粘贴。<br>你必须清楚<code>.tmux.conf</code>的每一个配置的作用是什么。</p><p>一旦你感觉你掌握了基础，将会有很多好玩的高级tmux游戏等着你。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;你经常可能会遇到这样的情况：你在vim编辑你在远程服务器上面的代码，然后你要新开一个窗口再次ssh到&lt;br&gt;这个远程服务器来测试运行你的代码
      
    
    </summary>
    
    
      <category term="tmux" scheme="http://mikolaje.github.io/categories/tmux/"/>
    
    
      <category term="tmux" scheme="http://mikolaje.github.io/tags/tmux/"/>
    
  </entry>
  
  <entry>
    <title>Hexo博客SEO优化，添加robots.txt</title>
    <link href="http://mikolaje.github.io/2019/hexo_seo.html"/>
    <id>http://mikolaje.github.io/2019/hexo_seo.html</id>
    <published>2019-07-30T03:55:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近一时兴起，想提高自己博客的点击率，就尝试做了一些SEO优化，并且加入了Google Adsense广告。呵呵，<br>写博客这么辛苦，赚点钱是应该的嘛。</p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>废话不说了，直接进入主题。都知道要想在百度搜索，Google搜索结果页中排得靠前，你得让人家的爬虫搜你嘛，<br>所以我们得通过一个叫<code>robots.txt</code>的文件放在根目录上。这文件的目的，就是告诉搜索引擎应该搜索我这网站的那些内容。<br>我们当然希望是搜索我们文章内容本身，不要去搜那些JavaScript和CSS代码。</p><h3 id="配置-robots-txt"><a href="#配置-robots-txt" class="headerlink" title="配置 robots.txt"></a>配置 robots.txt</h3><p>我们在hexo 根目录下的 <code>public</code> 目录下新建一个<code>robots.txt</code>文件，内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">User-agent: *</div><div class="line">Allow: /</div><div class="line">Allow: /archives/</div><div class="line">Allow: /categories/</div><div class="line">Allow: /tags/</div><div class="line">Allow: /about/</div><div class="line"></div><div class="line">Disallow: /vendors/</div><div class="line">Disallow: /js/</div><div class="line">Disallow: /css/</div><div class="line">Disallow: /fonts/</div><div class="line">Disallow: /fancybox/</div><div class="line"></div><div class="line">Sitemap: https://mikolaje.github.io/sitemap.xml</div><div class="line">Sitemap: https://mikolaje.github.io/baidu_sitemap.xml</div></pre></td></tr></table></figure></p><p>最后面两行是site-map</p><p>这里要注意的是如果js和fonts这些加了disallow的话，会出现谷歌抓取问题。<br><img src="/images/2019/hexo_seo/google_spyder.png" alt=""><br>因为现在（2019-09以后）Google Search默认是用智能手机引擎来抓取，<br>所以如果js和css这样被disallow的话会有问题，建议还是把上面的disallow去掉。</p><h3 id="Sitemap即网站地图"><a href="#Sitemap即网站地图" class="headerlink" title="Sitemap即网站地图"></a>Sitemap即网站地图</h3><p>它的作用在于便于搜索引擎更加智能地抓取网站。<br>最简单和常见的sitemap形式，是XML文件，在其中列出网站中的网址以及关于每个网址的其他元数据（上次更新时间、更新的频率及相对其他网址重要程度等）。</p><p>要使用<code>sitemap</code>我们需要安装两个hexo的插件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">npm install hexo-generator-sitemap --save</div><div class="line">npm install hexo-generator-baidu-sitemap --save</div></pre></td></tr></table></figure></p><p>然后，我们要在根目录下的<code>_config.yml</code> 的最后面添加如下内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="attr">sitemap:</span></div><div class="line"><span class="attr">  path:</span> sitemap.xml</div><div class="line"><span class="attr">baidusitemap:</span></div><div class="line"><span class="attr">  path:</span> baidusitemap.xml</div></pre></td></tr></table></figure><h3 id="配置-google-analytics"><a href="#配置-google-analytics" class="headerlink" title="配置 google analytics"></a>配置 google analytics</h3><p>在theme/next/_config.yml文件下添加如下配置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">google_analytics: UA-146421499-1</div></pre></td></tr></table></figure></p><p>Track ID要到你自己的GA的页面里找</p><h3 id="配置ads-txt"><a href="#配置ads-txt" class="headerlink" title="配置ads.txt"></a>配置ads.txt</h3><blockquote><p>ads.txt是干什么用的？</p><p>授权数字卖方 (ads.txt) 是一项 IAB 计划，可帮助确保您的数字广告资源只通过您认定为已获得授权的卖家（如 AdSense）进行销售。创建自己的 ads.txt 文件后，您可以更好地掌控允许谁在您的网站上销售广告，并可防止向广告客户展示仿冒广告资源。</p></blockquote><p>在Google Adsense找到相应的页面下载 ads.txt，然后同样放在根目录的<code>public</code>目录下面。<br><img src="/images/2019/hexo_seo/adsense.png" alt=""></p><h3 id="修改博文链接"><a href="#修改博文链接" class="headerlink" title="修改博文链接"></a>修改博文链接</h3><p>HEXO默认的文章链接形式为<code>domain/year/month/day/postname</code>，默认就是四级url，并且可能造成url过长，对搜索引擎是不太不友好，<br>我们可以改成domain/postname 的形式。编辑站点的_config.yml文件，<br>修改其中的permalink字段改为<code>permalink: :title.html</code>即可。</p><blockquote><p>配置完成后，重新部署hexo：<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近一时兴起，想提高自己博客的点击率，就尝试做了一些SEO优化，并且加入了Google Adsense广告。呵呵，&lt;br&gt;写博客这么辛苦，赚
      
    
    </summary>
    
    
      <category term="hexo" scheme="http://mikolaje.github.io/categories/hexo/"/>
    
    
      <category term="hexo" scheme="http://mikolaje.github.io/tags/hexo/"/>
    
      <category term="SEO" scheme="http://mikolaje.github.io/tags/SEO/"/>
    
  </entry>
  
  <entry>
    <title>PySpark 会比Scala或Java慢吗（译）？</title>
    <link href="http://mikolaje.github.io/2019/pyspark_slower.html"/>
    <id>http://mikolaje.github.io/2019/pyspark_slower.html</id>
    <published>2019-07-04T04:07:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<p>首先，你必须知道不同类型的API（RDD API，MLlib 等），有它们不同的性能考虑。</p><h2 id="RDD-API"><a href="#RDD-API" class="headerlink" title="RDD API"></a>RDD API</h2><p>（带JVM编排的Python结构）</p><p>这是一个会被Python代码性能和PySpark实施影响最大的组件。虽然Python性能很可能不会是个问题，至少有几个因素你要考虑下：</p><ul><li><p>JVM 通信的额外开销。所有进出Python executor的数据必须通过一个socket和一个JVM worker. 尽管这过程相当高效，以为走的都是本地通信，<br>但多少依然还是要付出点代价。</p></li><li><p>基于进程的Python executor 对比基于线程（单JVM多线程）的 Scala executors。每个Python executor在它独自的进程里运行。<br>它的副作用是，虽然它有着比JVM更强的隔离性，并且对executor生命周期的一些控制也比JVM更强，但是潜在地会比JVM的executor消耗更多的内存。<br>比如：</p><ul><li>解析器内存footprint</li><li>加载模块的footprint</li><li>更低效的广播（因为每个进程需要独自的广播复制）</li></ul></li><li><p>Python本身的性能。总的来说Scala会比Python更快，但不同的task有有所不同。此外，你有其它的选项包括JITs<br>比如Numba，C扩展Cython或者其它专业的lib比如Theano。最后，可以考虑用PyPy作为解析器。</p></li><li><p>PySpark configuration提供<code>spark.python.worker.reuse</code>参数， 这可以用来对每个task在 forking Python进程和复用已有的进程中作出选择。<br>后者似乎在避免昂贵的垃圾回收方面上更有用（这更多的是一个印象而不是系统测试的结果）</p></li><li><p>在CPython里首选的垃圾回收方法，引用计数法，和典型的Spark 作业（比如流式处理，没有引用循环）结合得挺好，并且减少了长时间垃圾回收等待的风险。</p></li></ul><h2 id="MLlib"><a href="#MLlib" class="headerlink" title="MLlib"></a>MLlib</h2><p>（结合Python和JVM执行）</p><p>基本上要考虑的和前面说的那些差不多，这里再补充一些。尽管MLlib所用的基础架构是Python RDD，所有的算法都是直接用Scala来执行的。这意味着需要额外的开销来将Python 对象转为Scala对象，<br>增长的内存使用率和一些其它的限制我们将来再说。</p><p>现在的Spark2.x，基于RDD的API是以一个维护模式存在，Spark3.0计划会移除RDD API。</p><h2 id="DataFrame-API-和-Spark-ML"><a href="#DataFrame-API-和-Spark-ML" class="headerlink" title="DataFrame API 和 Spark ML"></a>DataFrame API 和 Spark ML</h2><p>（限制在driver的用Python代码的JVM执行）<br>这些可能是对标准数据处理task最好的选择。因为Python代码在driver端大多被限制在高层次的逻辑操作，在这方面上Scala和Python基本上没有什么区别。<br>有个例外是，按行的Python UDF相对来说会比Scala慢很多。尽管有很多改进的机会（在Spark2.0有着大量的改进），最大的限制还是JVM和Python解析器之间数据传送。</p><p>尽量习惯于用Spark内置的一些函数比如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">from pyspark.sql.functions import col, lower, trim</div><div class="line"></div><div class="line">exprs = [</div><div class="line">    lower(trim(col(c))).alias(c) if t == &quot;string&quot; else col(c) </div><div class="line">    for (c, t) in df.dtypes</div><div class="line">]</div><div class="line"></div><div class="line">df.select(*exprs)</div></pre></td></tr></table></figure></p><p>应该用Spark的lower而不是Python String的lower，这样做有几个好处：</p><ul><li>这操作直接将数据到JVM而不用到Python解析器</li><li>只需要投影一次，而不用对字段的每个字符串进行投影</li></ul><p>对了，避免在DataFrame和RDD之间的转换，因为这需要耗费很大的序列化和反序列化工作，更别说JVM和Python之间的数据传输了。</p><p>值得注意的是，调用Py4J会有非常高的延迟。这包括这样的调用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">from pyspark.sql.functions import col</div><div class="line"></div><div class="line">col(&quot;foo&quot;)</div></pre></td></tr></table></figure></p><p>通常，这不应该是个问题（overhead是固定的，不取决于数据量，但假如是实时程序，你可能考虑对 Java wrapper进行 缓存/复用 。</p><h2 id="GraphX-和-Spark-DataSets"><a href="#GraphX-和-Spark-DataSets" class="headerlink" title="GraphX 和 Spark DataSets"></a>GraphX 和 Spark DataSets</h2><p>对于 Spark 1.6 和 2.1，GraphX和Spark DataSets都不提供Python接口，所以你可以说PySpark比Scala差多了。</p><h3 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h3><p>实践里，GraphX开发几乎完全停滞了，项目目前在维护模式，在JIRA上一些tickets都已经关掉了，不再fix。GraphFrames库提供了Python结合，你可以选它作为一个 graph处理的办法。</p><h3 id="DataSets"><a href="#DataSets" class="headerlink" title="DataSets"></a>DataSets</h3><p>主观来说，Python在统计类型的DataSets没有什么空间，即使现有的Scala实施过于简单，并且不提供和DataFrame一样的性能优势。</p><h2 id="Streaming"><a href="#Streaming" class="headerlink" title="Streaming"></a>Streaming</h2><p>从我之前说来看，我都会强烈推荐Scala，而不是Python。未来如果PySpark在structured streams上得到支持的话，可能会改变，但是现在来说，还是为时过早。再者，基于RDD的API<br>在Databricks文档里（ 2017-03-03）已经被定为“streaming遗产”，所以，可以期许下在未来进行统一。</p><h2 id="非性能考虑"><a href="#非性能考虑" class="headerlink" title="非性能考虑"></a>非性能考虑</h2><h3 id="功能平等"><a href="#功能平等" class="headerlink" title="功能平等"></a>功能平等</h3><p>不是所有的Spark特性、功能在PySpark上都有。需要确保下你需要的那部分已经实现了，并且尝试了解可能的限制。</p><p>有点特别重要的是，当你使用MLlib，和其它类似的混合Context（比如在task里调用Java/Scala 方法)。公平来讲，一些PySpark API，比如mllib.linalg，提供比Scala更加复杂的方法。</p><h3 id="API设计"><a href="#API设计" class="headerlink" title="API设计"></a>API设计</h3><p>PySpark API的设计和Scala类似，并不那么Pythonic。 这意味着很容易地可以在两种语言之间切换，但同时，Python可能会变得难以理解</p><h3 id="架构的复杂性"><a href="#架构的复杂性" class="headerlink" title="架构的复杂性"></a>架构的复杂性</h3><p>PySpark数据处理流程相当复杂比起纯粹的JVM执行来说。PySpark程序非常难去debug或找出出错原因。此外，至少在基本对Scala和JVM总体的理解上是必须要有的。</p><h3 id="Spark2-0-及以后"><a href="#Spark2-0-及以后" class="headerlink" title="Spark2.0 及以后"></a>Spark2.0 及以后</h3><p>随着RDD API被冻结，正在进行迁移到DataSet API对Python用户同时带来机会和挑战。尽管高级层次部分的API用Python包装会容易很多，但更高级的直接被使用的可能性很低。</p><p>此外，在SQL的世界里，原生Python function依然是二等公民。但值得期待的是，在将来伴随着Apache Arrow序列化，Python的地位会提高（目前侧重仍然是数据收集，UDF序列化以及反序列化仍然是个长远的目标）。<br>对于那些Python代码依赖性很强的项目，还可以选择纯Python的框架，比如Dask或Ray等等，也挺有意思的。</p><h2 id="不必和其它比较"><a href="#不必和其它比较" class="headerlink" title="不必和其它比较"></a>不必和其它比较</h2><p>Spark DataFrame（SQL，DataSets）API提供了一个在PySpark程序里整合Java/Scala代码优雅的方式。<br>你可以用<code>DataFrames</code> 去输送数据给原生JVM代码，然后返回结果。<br>我已经在其它地方解释了我的看法 <a href="https://stackoverflow.com/q/31684842" target="_blank" rel="external">这里</a> ，你可以在这 <a href="https://stackoverflow.com/q/36023860/1560062" target="_blank" rel="external">https://stackoverflow.com/q/36023860/1560062</a> 找到一个Python-Scala的工作案例 。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;首先，你必须知道不同类型的API（RDD API，MLlib 等），有它们不同的性能考虑。&lt;/p&gt;
&lt;h2 id=&quot;RDD-API&quot;&gt;&lt;a href=&quot;#RDD-API&quot; class=&quot;headerlink&quot; title=&quot;RDD API&quot;&gt;&lt;/a&gt;RDD API&lt;/h2&gt;
      
    
    </summary>
    
    
      <category term="大数据" scheme="http://mikolaje.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://mikolaje.github.io/tags/spark/"/>
    
      <category term="pyspark" scheme="http://mikolaje.github.io/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>自建家用服务器集群，打造一个私有云</title>
    <link href="http://mikolaje.github.io/2019/private_IDC.html"/>
    <id>http://mikolaje.github.io/2019/private_IDC.html</id>
    <published>2019-07-02T14:07:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="我的战神"><a href="#我的战神" class="headerlink" title="我的战神"></a>我的战神</h3><p>还记得17年的时候，那时深度学习刚火起来，幼稚的我居然打算往这个领域去研究（根本没有意识到这领域需要的数学功底要多深）。俗话说，工欲善其事，必先利其器，于是乎，<br>一股脑地跑到广州的岗顶那买了台神舟战神 超级本（GTX-1060）。当时，经常都听到东哥在提到GPU，CUDA，GTX1080提到这些关键词。哈，为什当时会选择买一台笔记本，而不是台式呢？</p><p>其实，当时页考虑过买台式的，但是，考虑到当时在外面租房子，台式机搬家不方便。于是就想搞一台显卡性能好的笔记本，但是，完全不知道台式机的1060和笔记本的1060完全不是一个概念，<br>虽然都要同一个型号。至于为什么要买神舟，而不是华硕，或者其它，很简单，—-因为它性价比高（其实不高，买回来不久就感觉上当了，买回来玩FIFA OL会经常卡卡的，小毛病很多，而且<br>偶尔还会蓝屏。。）</p><p>那时候7400左右的价格在岗顶那买回来，真正用它就用了一年的时间吧。后来，在今年5月份，我以3000的价格在闲鱼上低价卖了。<br><img src="/images/2019/private_IDC/IMG_3434.jpg" alt="Sample Image Added via Markdown"></p><h3 id="第一台组装台式机"><a href="#第一台组装台式机" class="headerlink" title="第一台组装台式机"></a>第一台组装台式机</h3><p>前面算是有点扯远了，不过也算是记录一下我早期的一些想法吧，正是有了曾经的各种因素，导致了我今天打算搭建私有集群的IDEA。在尝到了神舟笔记本的坑后，我意识到，其实买一台台式机的必要性。<br>台式机在散热，硬件稳定性上都明显强于笔记本，而且，性价比也挺高的。对了，对于购买台式机的一个很重要的因素是，17年到18年这段时间里，我在云服务上花了不少钱，刚开始在阿里云，<br>然后在Google Cloud上。陆陆续续大概花了有六七千块钱吧，也将近买一台机器的钱了。<br>18年年初，我跑去深圳华强北，组装了自己第一台组装机（华硕主板，16G内存，1T硬盘），不加显示器，一共5000多人民币。背回来立马装了ubuntu系统，顿时感到有了自己的服务器，那感觉就像<br>在外面租房久了，终于买了属于自己的房子，再也不要担心服务器没续费，登陆不上；再也不用担心资源不够用，要去计算云服务的成本和开支了。自己的机子想怎么扩展都行。</p><p>其实刚开始，由于技术原因，我并没有把它当做一台服务器在用，只是把它当成一台PC安装了一些自己熟悉的开发环境和软件。在配置了Ngrok后，基本上把它当成一台云服务器了，24小时运行着。<br>关于ngrok的配置我前面有一篇专门的文章有讲得很详细。</p><p>最近发现我的台式机风扇声音有点大（比起Dell T30来说），从长远来看，毕竟是要7*24 running的，考虑到耗电量和稳定性，还是服务器好啊。我打算将再购置<br>2台服务器。其实，服务器完全可以当PC来用，但PC不能当服务器来用。</p><h3 id="第一台真正的服务器"><a href="#第一台真正的服务器" class="headerlink" title="第一台真正的服务器"></a>第一台真正的服务器</h3><p>虽然我的台式机现在配置有32G内存，在安装了一些软件（Cloudera Manager， Elasticsearch）后，内存就不太够用了。我是做数据这一块的，平时也要用分布式的框架，像Hadoop生态圈的<br>所以组件。我非常需要有一个真正的分布式平台去练手，于是就有了我的第一台服务器————戴尔T30<br><img src="/images/2019/private_IDC/t30.jpg" alt="Sample Image Added via Markdown"></p><p>也是在华强北的赛格广场，3200买的。配置是E3-1225，16G DDR4，2T*2 带阵列。T30 我没理解错的话应该是戴尔服务器的入门版。我对服务器不太理解，刚入门，挺好奇的，只知道一般PC的CPU<br>是分I7，I5等等，服务器的CPU是E开头的。然后，服务器用的内存和PC用的也不一样，是不能共用的。</p><p>购机的时候和那些卖电脑的老板闲聊也学到了好多专业知识。别看他们穿着没有写字楼的白领干净，没有程序员工资高，<br>但他们对计算机硬件知识还是很有经验的。比如，老板跟我说，你知道磁盘是怎么运行的吗？他说并不是说高转速的磁盘就一定好，如果突然断电的话，<br>转速高的磁盘容易被划伤。并介绍了下SAS，SATA，SSD之间的一些区别。他说SAS这种算是比较老的格式了。现在基本都用SATA。</p><p>那天我看了两种服务器，一种是机架式服务器，另一种是踏实。机架式的风扇声比较大（虽然单路的会比双路的小一些，但比起塔式服务器会大挺多的。）</p><h4 id="坑人的阵列卡"><a href="#坑人的阵列卡" class="headerlink" title="坑人的阵列卡"></a>坑人的阵列卡</h4><p>作为一个服务器装机菜鸟，我从网上下好的ubuntu服务器放在U盘里，然后改好U盘启动引导，开始安装。前几步没有啥问题，但到了选择安装磁盘路径的时候，找不到任何磁盘，<br>从而无法点击下一步继续。后来在老板的指导下，我才得知，原来是因为有阵列卡，BIOS没有设置对还是啥的，所以才读不到磁盘。于是，我在老板的远程指导下成功地拆卸掉阵列卡，<br>并重新接线好两块磁盘。之后，便正常地顺利正常地安装好ubuntu server了。</p><p><img src="/images/2019/private_IDC/raid.jpg" alt="Sample Image Added via Markdown"></p><h4 id="交换机"><a href="#交换机" class="headerlink" title="交换机"></a>交换机</h4><p>哈哈，没钱买千兆网卡，千兆交换机，只能用路由器充当了。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>玩硬件组装也是个超级无底洞，各个部件都可以扩展起来，内存，硬盘，CPU，可以水平扩展，也可以横向扩展。以后准备好好赚钱升级设备吧。如果比较缺钱的话可以上淘宝或闲鱼买点二手的配件，<br>比如内存条，硬盘这些。最好是可以和个人卖家交易，因为个人的话经常可以买到一些物美价廉的二手物品。另外，V2EX网站也有专门二手转让的版块，可以去看看。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;h3 id=&quot;我的战神&quot;&gt;&lt;a href=&quot;#我的战神&quot; class=&quot;headerlink&quot; title=&quot;我的战神&quot;&gt;&lt;/a&gt;我的战神&lt;/h
      
    
    </summary>
    
    
      <category term="运维" scheme="http://mikolaje.github.io/categories/%E8%BF%90%E7%BB%B4/"/>
    
    
      <category term="ubuntu" scheme="http://mikolaje.github.io/tags/ubuntu/"/>
    
      <category term="NAS" scheme="http://mikolaje.github.io/tags/NAS/"/>
    
  </entry>
  
  <entry>
    <title>SQLAchemy的多进程实践</title>
    <link href="http://mikolaje.github.io/2019/sqlalchemy_with_multiprocess.html"/>
    <id>http://mikolaje.github.io/2019/sqlalchemy_with_multiprocess.html</id>
    <published>2019-07-02T04:07:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近上头说我写的ETL工具从MySQL导出为CSV的速度太慢了，需要性能优化。的确，有部分数据因为在MySQL里面做了<br>分表分库，而我目前的导出实现是一个一个对小表进行导出。其实，这一步完全是可以并发多个表同时导出的。<br>理论上如果网络IO没有瓶颈的话，多个表同时从MySQL里dump可以大大提升效率。</p><h2 id="正题"><a href="#正题" class="headerlink" title="正题"></a>正题</h2><h3 id="查阅文档"><a href="#查阅文档" class="headerlink" title="查阅文档"></a>查阅文档</h3><p>为了实现并发地使用sqlalchemy我花了不少时间在网上找资料，也在StackOverflow寻求帮助。<br>其实刚开始我是想用threading结合SQLAlchemy来实现多线程导出的。去SQLAlchemy官网一看，没有找到相关的实现文档，<br>却找到了multiprocessing与SQLAlchemy的实践:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Using Connection Pools with Multiprocessing</div><div class="line">It’s critical that when using a connection pool, and by extension when using an Engine created via create_engine(), that the pooled connections are not shared to a forked process. TCP connections are represented as file descriptors, which usually work across process boundaries, meaning this will cause concurrent access to the file descriptor on behalf of two or more entirely independent Python interpreter states.</div><div class="line"></div><div class="line">There are two approaches to dealing with this.</div><div class="line"></div><div class="line">The first is, either create a new Engine within the child process, or upon an existing Engine, call Engine.dispose() before the child process uses any connections. This will remove all existing connections from the pool so that it makes all new ones. Below is a simple version using multiprocessing.Process, but this idea should be adapted to the style of forking in use:</div></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">engine = create_engine(<span class="string">"..."</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_in_process</span><span class="params">()</span>:</span></div><div class="line">  engine.dispose()</div><div class="line"></div><div class="line">  <span class="keyword">with</span> engine.connect() <span class="keyword">as</span> conn:</div><div class="line">      conn.execute(<span class="string">"..."</span>)</div><div class="line"></div><div class="line">p = Process(target=run_in_process)</div></pre></td></tr></table></figure><h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><p>于是，我就打算用multiprocessing试试了。multiprocessing有个比较坑爹的地方就是它会用pickle来序列化一些数据，<br>因为要把数据复制到新spawn出来的进程。</p><h4 id="关于pickle"><a href="#关于pickle" class="headerlink" title="关于pickle"></a>关于pickle</h4><p>首先我们要了解下pickle，虽然pickle挺坑的。哪些内容可以pickle呢，官网上说：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">What can be pickled and unpickled?</div><div class="line">The following types can be pickled:</div><div class="line"></div><div class="line">None, True, and False</div><div class="line"></div><div class="line">integers, floating point numbers, complex numbers</div><div class="line"></div><div class="line">strings, bytes, bytearrays</div><div class="line"></div><div class="line">tuples, lists, sets, and dictionaries containing only picklable objects</div><div class="line"></div><div class="line">functions defined at the top level of a module (using def, not lambda)</div><div class="line"></div><div class="line">built-in functions defined at the top level of a module</div><div class="line"></div><div class="line">classes that are defined at the top level of a module</div><div class="line"></div><div class="line">instances of such classes whose __dict__ or the result of calling __getstate__() is picklable (see section Pickling Class Instances for details).</div></pre></td></tr></table></figure></p><p>比如, 我们举个简单的例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> pymysql</div><div class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</div><div class="line"><span class="comment">#engine = create_engine(f'mysql+pymysql://root:ignorance@localhost:3306/', server_side_cursors=True, pool_size=20)</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Client</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment">#self.engine = create_engine(f'mysql+pymysql://root:ignorance@localhost:3306/', server_side_cursors=True, pool_size=20)</span></div><div class="line">        self.pool = Pool(<span class="number">5</span>)</div><div class="line">        self.connection = pymysql.connect(host=<span class="string">'localhost'</span>,</div><div class="line">                             user=<span class="string">'root'</span>,</div><div class="line">                             password=<span class="string">'ignorance'</span>,</div><div class="line">                             port=<span class="number">3306</span>,</div><div class="line">                             charset=<span class="string">'utf8mb4'</span>,</div><div class="line">                             cursorclass=pymysql.cursors.DictCursor)</div><div class="line">        self.engine = create_engine(f<span class="string">'mysql+pymysql://root:ignorance@localhost:3306/'</span>, server_side_cursors=<span class="keyword">True</span>, pool_size=<span class="number">20</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run_in_process</span><span class="params">(self, x, y=<span class="string">'hehe'</span>)</span>:</span></div><div class="line">        print(<span class="string">'run in process'</span>)</div><div class="line">        self.engine.dispose()</div><div class="line">        conn = self.engine.connect()</div><div class="line">        res = conn.execute(<span class="string">'select count(1) from zhihu.zhihu_answer_meta limit 10'</span>)</div><div class="line">        print(res.fetchall())</div><div class="line">        time.sleep(<span class="number">5</span>)</div><div class="line">        print(conn)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></div><div class="line">        x = <span class="string">'x'</span> </div><div class="line">        res_list = []</div><div class="line">        res = self.pool.apply_async(self.run_in_process, args=(x,), kwds=&#123;<span class="string">'y'</span>:<span class="string">'shit'</span>&#125;)</div><div class="line">        res_list.append(res)</div><div class="line">        <span class="comment">#[each.get(3) for each in res_list]</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run_pool</span><span class="params">(self)</span>:</span></div><div class="line">        self.pool.close()</div><div class="line">        self.pool.join()</div><div class="line"></div><div class="line">client = Client()</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    client.run()</div><div class="line"></div><div class="line">client.run_pool()</div></pre></td></tr></table></figure></p><p>这段代码我的目的是想用多个进程多个connector连接到MySQL，然后各自进程同时去查询，这样便可以实现并行处理，<br>提升效率。然而，实际上这段代码不能正常地执行，而且没有任何报错提示。这是为何呢？</p><p>有些时候可能会看到这样的报错:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">TypeError: can&apos;t pickle _thread._local objects</div></pre></td></tr></table></figure></p><p><a href="https://stackoverflow.com/questions/58022926/cant-pickle-the-sqlalchemy-engine-in-the-class" target="_blank" rel="external">https://stackoverflow.com/questions/58022926/cant-pickle-the-sqlalchemy-engine-in-the-class</a></p><p>下面我把上面的代码修改下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> pymysql</div><div class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Client</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.pool = Pool(<span class="number">5</span>)</div><div class="line">        self.connection = pymysql.connect(host=<span class="string">'localhost'</span>,</div><div class="line">                             user=<span class="string">'root'</span>,</div><div class="line">                             password=<span class="string">'ignorance'</span>,</div><div class="line">                             port=<span class="number">3306</span>,</div><div class="line">                             charset=<span class="string">'utf8mb4'</span>,</div><div class="line">                             cursorclass=pymysql.cursors.DictCursor)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run_in_process</span><span class="params">(self, x, y=<span class="string">'hehe'</span>)</span>:</span></div><div class="line">        <span class="comment"># 这里我们把engine改为了方法的内部变量，而不是实例变量。这样就不报错了</span></div><div class="line">        engine = create_engine(f<span class="string">'mysql+pymysql://root:ignorance@localhost:3306/'</span>, server_side_cursors=<span class="keyword">True</span>, pool_size=<span class="number">20</span>)</div><div class="line">        engine.dispose()</div><div class="line">        conn = engine.connect()</div><div class="line">        res = conn.execute(<span class="string">'select count(1) from zhihu.zhihu_answer_meta limit 10'</span>)</div><div class="line">        print(res.fetchall())</div><div class="line">        time.sleep(<span class="number">5</span>)</div><div class="line">        print(conn)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></div><div class="line">        x = <span class="string">'x'</span></div><div class="line">        res_list = []</div><div class="line">        res = self.pool.apply_async(self.run_in_process, args=(x,), kwds=&#123;<span class="string">'y'</span>:<span class="string">'shit'</span>&#125;)</div><div class="line">        res_list.append(res)</div><div class="line">        <span class="comment">#[each.get(3) for each in res_list]</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run_pool</span><span class="params">(self)</span>:</span></div><div class="line">        self.pool.close()</div><div class="line">        self.pool.join()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getstate__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># 这里我增加了这个魔术方法</span></div><div class="line">        self_dict = self.__dict__.copy()</div><div class="line">        <span class="keyword">del</span> self_dict[<span class="string">'pool'</span>]</div><div class="line">        <span class="keyword">del</span> self_dict[<span class="string">'connection'</span>]  <span class="comment"># if conenction is not deleted, it would be silent without any errors</span></div><div class="line">        <span class="keyword">return</span> self_dict</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span><span class="params">(self, state)</span>:</span></div><div class="line">        <span class="comment"># 这里我增加了这个魔术方法</span></div><div class="line">        self.__dict__.update(state)</div><div class="line"></div><div class="line"></div><div class="line">client = Client()</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    client.run()</div><div class="line"></div><div class="line">client.run_pool()</div></pre></td></tr></table></figure></p><h4 id="我们看到，这段代码有一些变化："><a href="#我们看到，这段代码有一些变化：" class="headerlink" title="我们看到，这段代码有一些变化："></a>我们看到，这段代码有一些变化：</h4><blockquote><p>增加了<strong>getstate</strong>， <strong>getstate</strong>这两个魔术方法。为什么要加呢？<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">首先，我们 self.pool.apply_async(self.run_in_process) 可以看出apply_async调用的是实例的方法，所以Python需要pickle整个Client对象，</div><div class="line">包括它的所有实例变量。在第一个代码片段中我们可以看到它的实例变量有pool, connection, engine等等。然而这些对象都是不可以被pickle的，</div><div class="line">所以代码执行的时候会有问题。所以就有了__getstate__， __getstate__ 这两个东西。</div></pre></td></tr></table></figure></p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">__getstate__ 总是在对象pickle之前调用， 同时，它让你可以指定你想pickle的对象状态。然后unpickle的时候，</div><div class="line">如果__setstate__被实现了，则__setstate__(state)会被调用。如果没有被实现的话，__getstate__返回的dict将会被</div><div class="line">unpickle的实例使用。在上面例子中，__setstate__ 其实没有实际效果，不写也可以。</div></pre></td></tr></table></figure><h3 id="multiprocessing的一些细节"><a href="#multiprocessing的一些细节" class="headerlink" title="multiprocessing的一些细节"></a>multiprocessing的一些细节</h3><h4 id="传多个参数在target函数"><a href="#传多个参数在target函数" class="headerlink" title="传多个参数在target函数"></a>传多个参数在target函数</h4><p>有时候当我们想在apply_async 的 target函数上传指定参数的时候, 可以用kwds传进去,比如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_in_process</span><span class="params">(self, x, y=<span class="string">'hehe'</span>)</span>:</span></div><div class="line">    <span class="comment"># 这里我们把engine改为了方法的内部变量，而不是实例变量。</span></div><div class="line">    print(x)</div><div class="line">    print(y)</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></div><div class="line">    x = <span class="string">'x'</span></div><div class="line">    res = self.pool.apply_async(self.run_in_process, args=(x,), kwds=&#123;<span class="string">'y'</span>:<span class="string">'shit'</span>&#125;)</div></pre></td></tr></table></figure></p><h4 id="map和apply的区别"><a href="#map和apply的区别" class="headerlink" title="map和apply的区别"></a>map和apply的区别</h4><p>map执行的顺序是和参数的顺序一致的，apply_async的顺序是随机的<br>map一般用来切分参数执行在同一个方法上。而apply_async可以调用不同的方法。</p><p>此外,<br>map相当于 map_async().get()<br>apply相当于 apply_async().get()</p><h5 id="子进程报错没有提示"><a href="#子进程报错没有提示" class="headerlink" title="子进程报错没有提示"></a>子进程报错没有提示</h5><p>有个很坑的地方，有些时候逻辑明明没有执行，但又没有任何报错！<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment">#@staticmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">work</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"error"</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    pool = mp.Pool()</div><div class="line">    foo = Foo()</div><div class="line">    res = pool.apply_async(foo.work)</div><div class="line">    pool.close()</div><div class="line">    pool.join()</div><div class="line">    <span class="comment">#print(res.get())</span></div></pre></td></tr></table></figure></p><p> 比如这个，如果我不get() 一下 apply_async后的返回的话，看不到任何报错信息，解决办法就是用get()后才<br> 能得知报错的信息。</p><h4 id="加装饰器后报错"><a href="#加装饰器后报错" class="headerlink" title="加装饰器后报错"></a>加装饰器后报错</h4><p> 比如我们在<code>run_in_process</code>方法上了个装饰器，然后就报错了。<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeit</span><span class="params">(method)</span>:</span></div><div class="line">   <span class="function"><span class="keyword">def</span> <span class="title">timed</span><span class="params">(*args, **kw)</span>:</span></div><div class="line">       ts = time.time()</div><div class="line">       result = method(*args, **kw)</div><div class="line">       te = time.time()</div><div class="line">       <span class="keyword">if</span> <span class="string">'log_time'</span> <span class="keyword">in</span> kw: </div><div class="line">           name = kw.get(<span class="string">'log_name'</span>, method.__name__.upper())</div><div class="line">           kw[<span class="string">'log_time'</span>][name] = int((te - ts) * <span class="number">1000</span>)</div><div class="line">       <span class="keyword">else</span>:</div><div class="line">           print(<span class="string">'%r 执行时长  %2.2f s'</span> % (method.__name__, (te - ts) ))</div><div class="line">       <span class="keyword">return</span> result</div><div class="line">   <span class="keyword">return</span> timed</div><div class="line">   </div><div class="line"><span class="meta">   @timeit</span></div><div class="line">   <span class="function"><span class="keyword">def</span> <span class="title">run_in_process</span><span class="params">(self, x, y=<span class="string">'hehe'</span>)</span>:</span></div><div class="line">       <span class="comment"># 这里我们把engine改为了方法的内部变量，而不是实例变量。</span></div><div class="line">       print(x)</div><div class="line">       print(y)</div><div class="line">       </div><div class="line">   <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></div><div class="line">       x = <span class="string">'x'</span></div><div class="line">       res = self.pool.apply_async(self.run_in_process, args=(x,), kwds=&#123;<span class="string">'y'</span>:<span class="string">'shit'</span>&#125;)</div></pre></td></tr></table></figure></p><p>报错说:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Traceback (most recent call last):</div><div class="line">  File &quot;/data/software/miniconda3/lib/python3.7/multiprocessing/process.py&quot;, line 297, in _bootstrap</div><div class="line">    self.run()</div><div class="line">  File &quot;/data/software/miniconda3/lib/python3.7/multiprocessing/process.py&quot;, line 99, in run</div><div class="line">    self._target(*self._args, **self._kwargs)</div><div class="line">  File &quot;/data/software/miniconda3/lib/python3.7/multiprocessing/pool.py&quot;, line 110, in worker</div><div class="line">    task = get()</div><div class="line">  File &quot;/data/software/miniconda3/lib/python3.7/multiprocessing/queues.py&quot;, line 354, in get</div><div class="line">    return _ForkingPickler.loads(res)</div><div class="line">AttributeError: &apos;Client&apos; object has no attribute &apos;timed&apos;</div></pre></td></tr></table></figure></p><p>解决办法：<br>比较麻烦，不能用@了。可以以这种办法代替装饰器：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">decorate_func</span><span class="params">(f)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_decorate_func</span><span class="params">(*args, **kwargs)</span>:</span></div><div class="line">        <span class="keyword">print</span> <span class="string">"I'm decorating"</span></div><div class="line">        <span class="keyword">return</span> f(*args, **kwargs)</div><div class="line">    <span class="keyword">return</span> _decorate_func</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">actual_func</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> x ** <span class="number">2</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">wrapped_func</span><span class="params">(*args, **kwargs)</span>:</span></div><div class="line">    <span class="keyword">return</span> decorate_func(actual_func)(*args, **kwargs)</div><div class="line"></div><div class="line">my_swimming_pool = Pool()</div><div class="line">result = my_swimming_pool.apply_async(wrapped_func,(<span class="number">2</span>,))</div><div class="line"><span class="keyword">print</span> result.get()</div></pre></td></tr></table></figure></p><h4 id="关于处理ctrl-c-的报错"><a href="#关于处理ctrl-c-的报错" class="headerlink" title="关于处理ctrl-c 的报错:"></a>关于处理ctrl-c 的报错:</h4><p>有时候我们用multiprocessing处理一些任务，当我们想终止任务时候，用Ctrl+C 然后会看到一堆的报错，有时候还得连续按很多CTRL+C完全终止掉。<br>下面是最佳解决方案：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">解决办法是 首先防止子进程接收KeyboardInterrupt，然后完全交给父进程catch interrupt然后清洗进程池。通过这种方法可以</div><div class="line">避免在子进程写处理异常逻辑，并且防止了由idle workers 生成的无止尽的Error。</div></pre></td></tr></table></figure></p><p>解决方案代码:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> multiprocessing</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> signal</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_worker</span><span class="params">()</span>:</span></div><div class="line">    signal.signal(signal.SIGINT, signal.SIG_IGN)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_worker</span><span class="params">()</span>:</span></div><div class="line">    time.sleep(<span class="number">15</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"Initializng 5 workers"</span></div><div class="line">    pool = multiprocessing.Pool(<span class="number">5</span>, init_worker)</div><div class="line"></div><div class="line">    <span class="keyword">print</span> <span class="string">"Starting 3 jobs of 15 seconds each"</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">        pool.apply_async(run_worker)</div><div class="line"></div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        print(<span class="string">"Waiting 10 seconds"</span>)</div><div class="line">        time.sleep(<span class="number">10</span>)</div><div class="line"></div><div class="line">    <span class="keyword">except</span> KeyboardInterrupt:</div><div class="line">        print(<span class="string">"Caught KeyboardInterrupt, terminating workers"</span>)</div><div class="line">        pool.terminate()</div><div class="line">        pool.join()</div><div class="line"></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">print</span> <span class="string">"Quitting normally"</span></div><div class="line">        pool.close()</div><div class="line">        pool.join()</div></pre></td></tr></table></figure></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://docs.python.org/3/library/pickle.html" target="_blank" rel="external">https://docs.python.org/3/library/pickle.html</a><br><a href="https://stackoverflow.com/questions/25382455/python-notimplementederror-pool-objects-cannot-be-passed-between-processes/25385582#25385582" target="_blank" rel="external">https://stackoverflow.com/questions/25382455/python-notimplementederror-pool-objects-cannot-be-passed-between-processes/25385582#25385582</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近上头说我写的ETL工具从MySQL导出为CSV的速度太慢了，需要性能优化。的确，有部分数据因为在MySQL里面做了&lt;br&gt;分表分库，而我
      
    
    </summary>
    
    
      <category term="python" scheme="http://mikolaje.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mikolaje.github.io/tags/python/"/>
    
      <category term="sqlalchemy" scheme="http://mikolaje.github.io/tags/sqlalchemy/"/>
    
  </entry>
  
  <entry>
    <title>Hive 使用orc进行事务操作(update)</title>
    <link href="http://mikolaje.github.io/2019/hive_orc.html"/>
    <id>http://mikolaje.github.io/2019/hive_orc.html</id>
    <published>2019-06-22T03:01:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h2><p>需求方需要用Hive来进行一些update操作。以往一般用Parquet这种格式作为Hive的存储格式，查文档得知Parquet不支持<br>update，orc格式可以支持update。</p><h2 id="开始试验"><a href="#开始试验" class="headerlink" title="开始试验"></a>开始试验</h2><h3 id="创建测试数据"><a href="#创建测试数据" class="headerlink" title="创建测试数据"></a>创建测试数据</h3><p>首先我们在Hive上简单地创建一个表作为测试：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">create table test_format (</div><div class="line">    car_name string,</div><div class="line">    series string,</div><div class="line">    e_series string,</div><div class="line">    model string,</div><div class="line">    variant string</div><div class="line">) clustered by (car_name) into 5 buckets stored as orc TBLPROPERTIES(&apos;transactional&apos;=&apos;true&apos;);</div><div class="line"></div><div class="line">insert into test_format values</div><div class="line">(&apos;2017款宝马3系320i M运动型&apos;,&apos;3&apos;,&apos;F30&apos;,&apos;320i&apos;,&apos;320i M Sport Package&apos;),</div><div class="line">(&apos;2018款宝马3系320i M运动套装&apos;,&apos;3&apos;,&apos;F30&apos;,&apos;320i&apos;,&apos;320i M Sport Package&apos;),</div><div class="line">(&apos;2018款宝马3系320i M运动曜夜版&apos;,&apos;3&apos;,&apos;F30&apos;,&apos;320i&apos;,&apos;320i M Sport Dark Edition&apos;),</div><div class="line">(&apos;2019款宝马3系320i M 运动套装&apos;,&apos;3&apos;,&apos;F30&apos;,&apos;320i&apos;,&apos;320i M Sport Package&apos;);</div></pre></td></tr></table></figure></p><p>刚开始会有报错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">This command is not allowed on an ACID table auto_projects.test_format with a non-ACID transaction manager. Failed command: insert into test_format value</div></pre></td></tr></table></figure></p><p>这是因为有些配置文件需要修改才能支持transaction操作。</p><p>在hive-site.xml里面添加如下配置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.optimize.sort.dynamic.partition&lt;/name&gt;</div><div class="line">    &lt;value&gt;false&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.support.concurrency&lt;/name&gt;</div><div class="line">    &lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.enforce.bucketing&lt;/name&gt;</div><div class="line">    &lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.exec.dynamic.partition.mode&lt;/name&gt;</div><div class="line">    &lt;value&gt;nonstrict&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.txn.manager&lt;/name&gt;</div><div class="line">    &lt;value&gt;org.apache.hadoop.hive.ql.lockmgr.DbTxnManager&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.compactor.initiator.on&lt;/name&gt;</div><div class="line">    &lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.compactor.worker.threads&lt;/name&gt;</div><div class="line">    &lt;value&gt;1&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.in.test&lt;/name&gt;</div><div class="line">    &lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure></p><p>Cloudera Manager的话则可以在WebUI上完成。</p><ol><li>在WebUI上先点击Hive集群。</li><li>点击配置，然后找到一个叫hive-site.xml 的 HiveServer2 高级配置代码段（安全阀）的tag中</li><li>将上述配置内容复制在文本框内。</li><li>重启集群</li></ol><h3 id="测试更新数据"><a href="#测试更新数据" class="headerlink" title="测试更新数据"></a>测试更新数据</h3><p>测试更新和删除部分数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">update test_format set model=&apos;test&apos; where series=&apos;3&apos;;</div><div class="line"></div><div class="line">delete  from test_format where model=&apos;test&apos;;</div></pre></td></tr></table></figure></p><p>这里需要注意的一个地方就是 要更新的字段不能是设置为bucket的那个字段，不然会报错：</p><p>成功执行！</p><h3 id="踩的一些坑"><a href="#踩的一些坑" class="headerlink" title="踩的一些坑"></a>踩的一些坑</h3><p>因为我是用的单节点部署的CDH集群。刚开始，在Hive上执行select count(*) ; 和 insert操作时候，没有任何报错，<br>但会一直卡在那里。后来查看到cloudera community说单节点要改个mapred-site.xml的一个参数：<br>mapreduce.framework.name</p><p>默认是用的yarn，单机的话要改为local，然后重启集群insert 和 select count(*) 就不会卡住了,<br>CDH也是在WebUI上的Yarn集群上的配置上修改。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.cnblogs.com/qifengle-2446/p/6424620.html" target="_blank" rel="external">https://www.cnblogs.com/qifengle-2446/p/6424620.html</a><br><a href="https://community.cloudera.com/t5/Batch-SQL-Apache-Hive/Hive-server-query-hanging-when-issue-ing-select-count-on-CLI/m-p/67119#M2662" target="_blank" rel="external">https://community.cloudera.com/t5/Batch-SQL-Apache-Hive/Hive-server-query-hanging-when-issue-ing-select-count-on-CLI/m-p/67119#M2662</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;需求背景&quot;&gt;&lt;a href=&quot;#需求背景&quot; class=&quot;headerlink&quot; title=&quot;需求背景&quot;&gt;&lt;/a&gt;需求背景&lt;/h2&gt;&lt;p&gt;需求方需要用Hive来进行一些update操作。以往一般用Parquet这种格式作为Hive的存储格式，查文档得知Parqu
      
    
    </summary>
    
    
      <category term="大数据" scheme="http://mikolaje.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hive" scheme="http://mikolaje.github.io/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>经典的大数据面试题</title>
    <link href="http://mikolaje.github.io/2019/typical_bigdata_interview_question.html"/>
    <id>http://mikolaje.github.io/2019/typical_bigdata_interview_question.html</id>
    <published>2019-06-02T04:07:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近面试大数据工程师岗位，同一个问题被连续问了两次。题目大概是这样的：<br>如果你有一台机器，内存是有限的，要你统计一个很大的日志文件里的数据，比如统计UV top-N；<br>另外一个公司是这么问：<br>如果你有一台机器，内存有限，要你对某个指标做一个全局排序。比如对单词频次进行全局排序。</p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>第一种问题是比较简单的。这种问题一般都问在某个条件下如何在大数据中求出top-N。这种问题的解决方案如下步骤：</p><ol><li>分而治之/hash映射。</li><li>hash统计。</li><li>堆/快速/归并排序；</li></ol><p>首先，为什么药hash映射呢？<br>因为，我们的目的要将一个大文件切割成N个小文件，去进行分而治之。而hash 加 取模 这种方式可以帮我们把数据<br>均匀随机地分布在N个地方。更重要的是，hash可以让我们把相同的东西放在同一个地方！</p><p>其实这种做法在有些地方出现过，比如Hive的bucket实现原理就是这个中道理。用hash加取模的方式保证同样的值被<br>分配到同一个地方。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://blog.csdn.net/v_july_v/article/details/7382693" target="_blank" rel="external">https://blog.csdn.net/v_july_v/article/details/7382693</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近面试大数据工程师岗位，同一个问题被连续问了两次。题目大概是这样的：&lt;br&gt;如果你有一台机器，内存是有限的，要你统计一个很大的日志文件里的
      
    
    </summary>
    
    
      <category term="大数据" scheme="http://mikolaje.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="interview" scheme="http://mikolaje.github.io/tags/interview/"/>
    
      <category term="大数据" scheme="http://mikolaje.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hive bucket和partition的区别</title>
    <link href="http://mikolaje.github.io/2019/hive_bucket_partition.html"/>
    <id>http://mikolaje.github.io/2019/hive_bucket_partition.html</id>
    <published>2019-06-02T03:01:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Hive-partition和bucket的区别"><a href="#Hive-partition和bucket的区别" class="headerlink" title="Hive partition和bucket的区别"></a>Hive partition和bucket的区别</h2><ul><li>翻译文</li></ul><p>为了更好地阐述partition和bucket的区别，我们先看看数据是怎么保存在Hive上面的。比如，你有一个表：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE mytable ( </div><div class="line">         name string,</div><div class="line">         city string,</div><div class="line">         employee_id int ) </div><div class="line">PARTITIONED BY (year STRING, month STRING, day STRING) </div><div class="line">CLUSTERED BY (employee_id) INTO 256 BUCKETS</div></pre></td></tr></table></figure></p><p>然后，Hive会将数据保存为如下的层级：<br>/user/hive/warehouse/mytable/y=2015/m=12/d=02<br>所以，用partition的时候必须小心，因为如果你用employee_id来partition的话，如果有上百万个employee，那么你会看到有上百万个目录被创建在你的系统上。<br>“cardinality” 这个术语被用来表示不同字段值的数量。比如，你有country这个字段，世界上会有300个国家，所以这里的”cardinality”是300这样。<br>对于像’timestamp_ms’的字段，它的’cardinality’会有几十亿。<br>总的来说，当我们用partition的时候，不要用在cardinality很高的字段上。因为它会导致生成太多的目录。</p><p>说说bucket了，在指定了bucket数后，会使得文件的数量固定。Hive会做的是计算字段的hash值，然后分发一个记录给那个bucket。<br>但是比如说你用总共256个bucket在一个较低的cardinality的字段上会发生什么呢？（比如，美国的州，只有50个）<br>只有50个bucket有数据，其它206个bucket是没有数据的。</p><p>有人已经提到partition可以极大地减少查询数据的量。<br>那在我这个例子中，如果你想在某个日期上进一步查询，对 yearn/month/day的partition会大大地减少IO。<br>我觉得有人已经提到bucket可以加速和其它恰好在同一个bucket的表的join操作。那么在我的案例中，如果你正在通过employee_id来join两个表，<br>Hive能够在每个每个bucket地内部进行join（如果它们已经通过employee_id排序好的话，效果会更好！）</p><p>总结，<br>bucket在字段有很高的cardinality，和在数据在不同bucket中均匀地分布的时候，会表现出优越性。<br>partition在cardinality partition的字段不是很多的时候，会表现出优越性。</p><p>另外，你可以按顺序同时partiton多个字段，比如（yearn/month/day），但bucket只能取一个字段。</p><ul><li>原文<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">There are a few details missing from the previous explanations. To better understand how partitioning and bucketing works,</div><div class="line"> you should look at how data is stored in hive. Let&apos;s say you have a table</div><div class="line"></div><div class="line">CREATE TABLE mytable ( </div><div class="line">         name string,</div><div class="line">         city string,</div><div class="line">         employee_id int ) </div><div class="line">PARTITIONED BY (year STRING, month STRING, day STRING) </div><div class="line">CLUSTERED BY (employee_id) INTO 256 BUCKETS</div><div class="line">then hive will store data in a directory hierarchy like</div><div class="line"></div><div class="line">/user/hive/warehouse/mytable/y=2015/m=12/d=02</div><div class="line">So, you have to be careful when partitioning, because if you for instance partition by employee_id and you have millions of employees, </div><div class="line">you&apos;ll end up having millions of directories in your file system. The term &apos;cardinality&apos; refers to the number of possible value a field can have. </div><div class="line">For instance, if you have a &apos;country&apos; field, the countries in the world are about 300, so cardinality would be ~300. </div><div class="line">For a field like &apos;timestamp_ms&apos;, which changes every millisecond, cardinality can be billions. </div><div class="line">In general, when choosing a field for partitioning, it should not have a high cardinality, because you&apos;ll end up with way too many directories in your file system.</div><div class="line"></div><div class="line">Clustering aka bucketing on the other hand, will result with a fixed number of files, since you do specify the number of buckets. </div><div class="line">What hive will do is to take the field, calculate a hash and assign a record to that bucket. </div><div class="line">But what happens if you use let&apos;s say 256 buckets and the field you&apos;re bucketing on has a low cardinality (for instance, it&apos;s a US state, so can be only 50 different values) ? </div><div class="line">You&apos;ll have 50 buckets with data, </div><div class="line">and 206 buckets with no data.</div><div class="line"></div><div class="line">Someone already mentioned how partitions can dramatically cut the amount of data you&apos;re querying. </div><div class="line">So in my example table, if you want to query only from a certain date forward, the partitioning by year/month/day is going to dramatically cut the amount of IO. </div><div class="line">I think that somebody also mentioned how bucketing can speed up joins with other tables that have exactly the same bucketing, </div><div class="line">so in my example, if you&apos;re joining two tables on the same employee_id, </div><div class="line">hive can do the join bucket by bucket (even better if they&apos;re already sorted by employee_id since it&apos;s going to mergesort parts that are already sorted, </div><div class="line">which works in linear time aka O(n) ).</div><div class="line"></div><div class="line">So, bucketing works well when the field has high cardinality and data is evenly distributed among buckets. </div><div class="line">Partitioning works best when the cardinality of the partitioning field is not too high.</div><div class="line"></div><div class="line">Also, you can partition on multiple fields, with an order (year/month/day is a good example), while you can bucket on only one field.</div></pre></td></tr></table></figure></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://stackoverflow.com/questions/19128940/what-is-the-difference-between-partitioning-and-bucketing-a-table-in-hive" target="_blank" rel="external">https://stackoverflow.com/questions/19128940/what-is-the-difference-between-partitioning-and-bucketing-a-table-in-hive</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Hive-partition和bucket的区别&quot;&gt;&lt;a href=&quot;#Hive-partition和bucket的区别&quot; class=&quot;headerlink&quot; title=&quot;Hive partition和bucket的区别&quot;&gt;&lt;/a&gt;Hive partition
      
    
    </summary>
    
    
      <category term="大数据" scheme="http://mikolaje.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hive" scheme="http://mikolaje.github.io/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Spark什么时候用 persist</title>
    <link href="http://mikolaje.github.io/2019/spark_when_to_cache.html"/>
    <id>http://mikolaje.github.io/2019/spark_when_to_cache.html</id>
    <published>2019-05-02T04:07:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在用Spark做一些数据统计，有个任务要跑几个小时，所以需要优化一下。首先想到的是用 persist或者cache(persist的其中一种方式)</p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="场景一"><a href="#场景一" class="headerlink" title="场景一"></a>场景一</h3><h4 id="首先看看在Stackoverflow的一个回答"><a href="#首先看看在Stackoverflow的一个回答" class="headerlink" title="首先看看在Stackoverflow的一个回答"></a>首先看看在Stackoverflow的一个回答</h4><p>Spark很多惰性算子，它并不会立即执行，persist就是惰性的。只有当被action trigger的时候，叫 lineage的RDD 链才会被执行。<br>如果是线性的lineage的话，persist是没用的。但如果RDD的lineage被分流出去的话，那么persist就有用了。</p><p>举个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># 比如这种情况，有个两个action算子count；</div><div class="line">val positiveWordsCount = wordsRDD.filter(word =&gt; isPositive(word)).count()</div><div class="line">val negativeWordsCount = wordsRDD.filter(word =&gt; isNegative(word)).count()</div><div class="line"></div><div class="line"></div><div class="line"># 可以在公用的RDD上加个cache，让这个flatMap只计算一次</div><div class="line">val textFile = sc.textFile(&quot;/user/emp.txt&quot;)</div><div class="line">val wordsRDD = textFile.flatMap(line =&gt; line.split(&quot;\\W&quot;))</div><div class="line">wordsRDD.cache()</div><div class="line">val positiveWordsCount = wordsRDD.filter(word =&gt; isPositive(word)).count()</div><div class="line">val negativeWordsCount = wordsRDD.filter(word =&gt; isNegative(word)).count()</div></pre></td></tr></table></figure><p>另外一个案例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_url</span><span class="params">(value)</span>:</span></div><div class="line">    url = <span class="string">'http://domain.com/'</span> + str(value)</div><div class="line">    <span class="keyword">if</span> value == <span class="string">'1134021255504498689'</span>:</div><div class="line">        print(value)  <span class="comment"># 这里print出来可以作为一个标记，知道 这个udf执行了几次</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> url </div><div class="line">    </div><div class="line">url_udf = udf(make_url, StringType())</div><div class="line">df = spark.read.csv(<span class="string">'file:///Users/xiaofeilong/Documents/twitter_tweet_keyword.csv'</span>)</div><div class="line"></div><div class="line">df = df.withColumn(<span class="string">'url'</span>, url_udf(<span class="string">'_c0'</span>))</div><div class="line"><span class="comment"># df.cache() </span></div><div class="line">df1 = df.filter(df._c1.isin([<span class="string">'petcare'</span>]))</div><div class="line">df2 = df.filter(df._c1.isin([<span class="string">'hound'</span>]))</div><div class="line"></div><div class="line">df_union = df1.union(df2)</div><div class="line">df_union = df_union.orderBy(<span class="string">'url'</span>)  <span class="comment"># 这里加order by url的原因是，要让它执行url_udf。不然如果这个字段没用上的话，Spark并不会执行url_udf</span></div><div class="line">print(df_union.count())</div></pre></td></tr></table></figure></p><p>上述例子中，如果不加cache的话。log中可以看到打印了两次 1134021255504498689。而且会看到有两条这样的信息：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">INFO FileScanRDD: Reading File path: file:///Users/xiaofeilong/Documents/twitter_tweet_keyword.csv, range: 0-2315578, partition values: [empty row]</div><div class="line">.</div><div class="line">.</div><div class="line">.</div><div class="line">FileScanRDD: Reading File path: file:///Users/xiaofeilong/Documents/twitter_tweet_keyword.csv, range: 0-2315578, partition values: [empty row]</div></pre></td></tr></table></figure></p><h4 id="测试结论！"><a href="#测试结论！" class="headerlink" title="测试结论！"></a>测试结论！</h4><ul><li><p>说明csv文件被load了两次！！！</p></li><li><p>如果用了cache的话，只会出现一次！！！</p></li></ul><h3 id="场景二"><a href="#场景二" class="headerlink" title="场景二"></a>场景二</h3><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://stackoverflow.com/questions/28981359/why-do-we-need-to-call-cache-or-persist-on-a-rdd" target="_blank" rel="external">https://stackoverflow.com/questions/28981359/why-do-we-need-to-call-cache-or-persist-on-a-rdd</a><br><a href="https://blog.csdn.net/ainidong2005/article/details/53152605" target="_blank" rel="external">https://blog.csdn.net/ainidong2005/article/details/53152605</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近在用Spark做一些数据统计，有个任务要跑几个小时，所以需要优化一下。首先想到的是用 persist或者cache(persist的其中
      
    
    </summary>
    
    
      <category term="大数据" scheme="http://mikolaje.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://mikolaje.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Git删除误add的文件</title>
    <link href="http://mikolaje.github.io/2019/git_filter_branch.html"/>
    <id>http://mikolaje.github.io/2019/git_filter_branch.html</id>
    <published>2019-05-02T03:07:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h4 id="很多时候，我们经常会不小心把一个不应该add进来的文件-add到-Repo里了。比如："><a href="#很多时候，我们经常会不小心把一个不应该add进来的文件-add到-Repo里了。比如：" class="headerlink" title="很多时候，我们经常会不小心把一个不应该add进来的文件 add到 Repo里了。比如："></a>很多时候，我们经常会不小心把一个不应该add进来的文件 add到 Repo里了。比如：</h4><p>password.txt，<em>.log  </em>.mp4 等等。</p><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><h4 id="这种情况下，我们可以选择用-git-filter-branch-来将历史的所有commit都重新过滤一下。"><a href="#这种情况下，我们可以选择用-git-filter-branch-来将历史的所有commit都重新过滤一下。" class="headerlink" title="这种情况下，我们可以选择用 git filter-branch 来将历史的所有commit都重新过滤一下。"></a>这种情况下，我们可以选择用 git filter-branch 来将历史的所有commit都重新过滤一下。</h4><h4 id="查看Git的文档得知，filter-branch是一个核弹级操作"><a href="#查看Git的文档得知，filter-branch是一个核弹级操作" class="headerlink" title="查看Git的文档得知，filter-branch是一个核弹级操作:"></a>查看Git的文档得知，filter-branch是一个核弹级操作:</h4><blockquote><p>如果你想用脚本的方式修改大量的提交，还有一个重写历史的选项可以用——例如，全局性地修改电子邮件地址或者将一个文件从所有提交中删除。<br>这个命令是filter-branch，这个会大面积地修改你的历史，所以你很有可能不该去用它，除非你的项目尚未公开，没有其他人在你准备修改的提交的基础上工作。<br>尽管如此，这个可以非常有用。你会学习一些常见用法，借此对它的能力有所认识。<br><br>从所有提交中删除一个文件<br>这个经常发生。有些人不经思考使用git add .，意外地提交了一个巨大的二进制文件，你想将它从所有地方删除。<br>也许你不小心提交了一个包含密码的文件，而你想让你的项目开源。filter-branch大概会是你用来清理整个历史的工具。<br>要从整个历史中删除一个名叫password.txt的文件，你可以在filter-branch上使用–tree-filter选项</p></blockquote><h4 id="Example：把历史中不小心添加进来的所有mp4文件清除。"><a href="#Example：把历史中不小心添加进来的所有mp4文件清除。" class="headerlink" title="Example：把历史中不小心添加进来的所有mp4文件清除。"></a>Example：把历史中不小心添加进来的所有mp4文件清除。</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">删除所有后缀为mp4 的历史提交文件</div><div class="line"></div><div class="line">git filter-branch --index-filter &apos;git rm -r --cached --ignore-unmatch *.mp4&apos; --prune-empty -f</div><div class="line">git for-each-ref --format=&apos;delete %(refname)&apos; refs/original | git update-ref --stdin </div><div class="line">git reflog expire --expire=now --all &amp;&amp; git gc --aggressive --prune=now </div><div class="line"></div><div class="line">如果要push到remote Repo的话，需要加-f 强推</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;h4 id=&quot;很多时候，我们经常会不小心把一个不应该add进来的文件-add到-Repo里了。比如：&quot;&gt;&lt;a href=&quot;#很多时候，我们经常会
      
    
    </summary>
    
    
      <category term="git" scheme="http://mikolaje.github.io/categories/git/"/>
    
    
      <category term="git" scheme="http://mikolaje.github.io/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>Celery让某个task一个一个地执行</title>
    <link href="http://mikolaje.github.io/2019/celery_lock_task.html"/>
    <id>http://mikolaje.github.io/2019/celery_lock_task.html</id>
    <published>2019-03-02T03:07:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>最近有个需求是这样子的，某个task要求一定要一个一个地执行，不能并发执行。<br>比较简单的办法是直接将 celery worker启动为 一个进程: “-c 1”。 但是，这种方法会导致其它的task也只能单进程了。</p><p>后来通过Google，查找了很多例子，最普遍的一个做法是参考官方文档的做法， 代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> celery <span class="keyword">import</span> task</div><div class="line"><span class="keyword">from</span> celery.five <span class="keyword">import</span> monotonic</div><div class="line"><span class="keyword">from</span> celery.utils.log <span class="keyword">import</span> get_task_logger</div><div class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> contextmanager</div><div class="line"><span class="keyword">from</span> django.core.cache <span class="keyword">import</span> cache</div><div class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</div><div class="line"><span class="keyword">from</span> djangofeeds.models <span class="keyword">import</span> Feed</div><div class="line"></div><div class="line">logger = get_task_logger(__name__)</div><div class="line"></div><div class="line">LOCK_EXPIRE = <span class="number">60</span> * <span class="number">10</span>  <span class="comment"># Lock expires in 10 minutes</span></div><div class="line"></div><div class="line"><span class="meta">@contextmanager</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">memcache_lock</span><span class="params">(lock_id, oid)</span>:</span></div><div class="line">    timeout_at = monotonic() + LOCK_EXPIRE - <span class="number">3</span></div><div class="line">    <span class="comment"># cache.add fails if the key already exists</span></div><div class="line">    status = cache.add(lock_id, oid, LOCK_EXPIRE)  </div><div class="line">    <span class="comment"># 如果存在lock_id的话会返回False，不存在的话会返回True。这个也可以换成用Redis实现，比如用 setnx</span></div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        <span class="keyword">yield</span> status</div><div class="line">    <span class="keyword">finally</span>:</div><div class="line">        <span class="comment"># memcache delete is very slow, but we have to use it to take</span></div><div class="line">        <span class="comment"># advantage of using add() for atomic locking</span></div><div class="line">        <span class="keyword">if</span> monotonic() &lt; timeout_at <span class="keyword">and</span> status:</div><div class="line">            <span class="comment"># don't release the lock if we exceeded the timeout</span></div><div class="line">            <span class="comment"># to lessen the chance of releasing an expired lock</span></div><div class="line">            <span class="comment"># owned by someone else</span></div><div class="line">            <span class="comment"># also don't release the lock if we didn't acquire it</span></div><div class="line">            cache.delete(lock_id)</div><div class="line"></div><div class="line"><span class="meta">@task(bind=True)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">import_feed</span><span class="params">(self, feed_url)</span>:</span></div><div class="line">    <span class="comment"># The cache key consists of the task name and the MD5 digest</span></div><div class="line">    <span class="comment"># of the feed URL.</span></div><div class="line">    feed_url_hexdigest = md5(feed_url).hexdigest()</div><div class="line">    lock_id = <span class="string">'&#123;0&#125;-lock-&#123;1&#125;'</span>.format(self.name, feed_url_hexdigest)</div><div class="line">    logger.debug(<span class="string">'Importing feed: %s'</span>, feed_url)</div><div class="line">    <span class="keyword">with</span> memcache_lock(lock_id, self.app.oid) <span class="keyword">as</span> acquired:</div><div class="line">        <span class="keyword">if</span> acquired:</div><div class="line">            <span class="keyword">return</span> Feed.objects.import_feed(feed_url).url</div><div class="line">    logger.debug(</div><div class="line">        <span class="string">'Feed %s is already being imported by another worker'</span>, feed_url)</div></pre></td></tr></table></figure><p>但是，上面的逻辑只是在有task正执行的时候忽略了新增task。比如说有个import_feed task 正在运行，还没有运行完，<br>再调用apply_async的时候就会不做任何操作。</p><p>所以得在上面代码的基础上改一改。</p><h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><p>这里我用了一个上厕所的例子。假设有一个公共厕所。如果有人在用着这个厕所的时候其他人就不能使用了，得在旁边排队等候。<br>一次只能进去一个人。废话少说直接上代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="meta">@app.task(bind=True, base=ShitTask, max_retries=10)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">shit_task</span><span class="params">(self, toilet_id)</span>:</span></div><div class="line">    lock_id = <span class="string">'&#123;0&#125;-lock'</span>.format(toilet_id)</div><div class="line">    shit_queue = <span class="string">'&#123;0&#125;-queue'</span>.format(self.name)</div><div class="line">    <span class="comment"># 这里我选用了Redis来处理队列</span></div><div class="line">    <span class="keyword">with</span> memcache_lock(lock_id, self.app.oid) <span class="keyword">as</span> acquired:</div><div class="line">        <span class="keyword">if</span> acquired:</div><div class="line">            <span class="comment"># 当进入到厕所的时候，先把门锁上（其他人就进不来了，然后再拉</span></div><div class="line">            print(<span class="string">'Oh yes, Lock the door and it is my time to shit. '</span>)</div><div class="line">            time.sleep(<span class="number">5</span>)</div><div class="line">            <span class="keyword">return</span> <span class="string">'I finished shit'</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># 有人在用厕所，得在外面等着 </span></div><div class="line">            print(<span class="string">'Oops, somebody engaged the toilet, I have to queue up'</span>)</div><div class="line">            rdb.lpush(shit_queue, json.dumps(list(self.request.args)))</div><div class="line">            <span class="comment"># 将其他人放到Redis里排队等候</span></div><div class="line">            <span class="keyword">raise</span> Ignore()</div></pre></td></tr></table></figure><p>上面代码是主要的task处理所及。另外，要先重写一下Task类的 after_return 方法，使得当没能执行的task（在门口排队的人）<br>在正在执行task（正在用厕所的人）成功执行完后，接着执行下一个task（下个人接着用厕所）。</p><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding=u8</span></div><div class="line"><span class="keyword">from</span> celery <span class="keyword">import</span> Celery, Task</div><div class="line"><span class="keyword">from</span> celery.exceptions <span class="keyword">import</span> Ignore</div><div class="line"><span class="keyword">from</span> celery <span class="keyword">import</span> task</div><div class="line"><span class="keyword">import</span> redis</div><div class="line"><span class="keyword">import</span> pickle</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">from</span> celery.five <span class="keyword">import</span> monotonic</div><div class="line"><span class="keyword">from</span> celery.utils.log <span class="keyword">import</span> get_task_logger</div><div class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> contextmanager</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line">app = Celery(<span class="string">'tasks'</span>, broker=<span class="string">'redis://localhost:6379/10'</span>)</div><div class="line"></div><div class="line">logger = get_task_logger(__name__)</div><div class="line"></div><div class="line">LOCK_EXPIRE = <span class="number">60</span> * <span class="number">10</span>  <span class="comment"># Lock expires in 10 minutes</span></div><div class="line"></div><div class="line">rdb = redis.Redis(db=<span class="number">11</span>)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShitTask</span><span class="params">(Task)</span>:</span></div><div class="line">    abstract = <span class="keyword">True</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">after_return</span><span class="params">(self, status, retval, task_id, args, kwargs, einfo)</span>:</span></div><div class="line">        print(status)</div><div class="line">        <span class="keyword">if</span> retval:</div><div class="line">            <span class="comment"># 这个retval的内容就是task return过来的内容</span></div><div class="line">            print(<span class="string">'somebody finished shit, calling the next one to shit'</span>)</div><div class="line">            shit_queue = <span class="string">'&#123;0&#125;-queue'</span>.format(self.name)</div><div class="line">            task_args = rdb.rpop(shit_queue)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> task_args:</div><div class="line">                task_args = json.loads(task_args)</div><div class="line">                self.delay(*task_args)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="meta">@contextmanager</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">memcache_lock</span><span class="params">(lock_id, oid)</span>:</span></div><div class="line">    timeout_at = monotonic() + LOCK_EXPIRE - <span class="number">3</span></div><div class="line">    <span class="comment"># status = cache.add(lock_id, oid, LOCK_EXPIRE)  # 如果lock_id 存在则返回False，如果不存在则返回True</span></div><div class="line">    status = rdb.setnx(lock_id, oid)</div><div class="line">    rdb.expire(lock_id, LOCK_EXPIRE)</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        <span class="keyword">yield</span> status</div><div class="line">    <span class="keyword">finally</span>:</div><div class="line">        <span class="keyword">if</span> monotonic() &lt; timeout_at <span class="keyword">and</span> status:</div><div class="line">            <span class="comment"># 设置一个时间限制，一个人不能占用厕所太久，而且只有占用厕所的那人才能开锁把厕所门打开</span></div><div class="line">            print(<span class="string">'release the lock and open the door of the toilet %s'</span> % lock_id)</div><div class="line">            rdb.delete(lock_id)</div><div class="line"></div><div class="line"></div><div class="line"><span class="meta">@app.task(bind=True, base=ShitTask, max_retries=10)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">shit_task</span><span class="params">(self)</span>:</span></div><div class="line">    print(<span class="string">'task name %s'</span> % self.name)</div><div class="line">    </div><div class="line">    lock_id = <span class="string">'&#123;0&#125;-lock'</span>.format(self.name)</div><div class="line">    shit_queue = <span class="string">'&#123;0&#125;-queue'</span>.format(self.name)</div><div class="line">    print(lock_id)</div><div class="line">    <span class="keyword">with</span> memcache_lock(lock_id, self.app.oid) <span class="keyword">as</span> acquired:</div><div class="line">        print(<span class="string">'acquired'</span>, acquired)</div><div class="line">        <span class="keyword">if</span> acquired:</div><div class="line">            print(<span class="string">'Oh yes, Lock the door and it is my time to shit. '</span>)</div><div class="line">            time.sleep(<span class="number">5</span>)</div><div class="line">            <span class="keyword">return</span> <span class="string">'I finished shit'</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            print(<span class="string">'Oops, somebody engaged the toilet, I have to queue up'</span>)</div><div class="line">            <span class="comment">#pending_task = pickle.dumps(self)</span></div><div class="line">            <span class="comment">#rdb.lpush(shit_queue, pending_task)</span></div><div class="line">            <span class="comment"># 不能用pickle 的去序列化task。在after_return load的时候会出现很诡异的现象。load出的task是第一个acquired的task</span></div><div class="line">            <span class="comment"># 改为用json来做序列化</span></div><div class="line">            rdb.lpush(shit_queue, json.dumps(list(self.request.args)))</div><div class="line">            <span class="keyword">raise</span> Ignore()</div></pre></td></tr></table></figure><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>可以新建一个test_celery项目来检验一下。新建一个目录名叫 test_celery<br>然后新建一个tasks.py文件，内容就是上面代码。<br>用下面命令来启动Celery worker，这里用了8个进程来处理。设置多点可以增加task的并行执行任务数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">celery -A tasks worker -l info -c 8</div></pre></td></tr></table></figure></p><p>然后，可以启动ipython 进行调用task。<br><img src="/images/2019/celery_lock_task/1d60a617.png" alt=""><br>快速地敲几个task.delay</p><p>在celery日志中我们可以看到两个shit_tasks 是一个接一个来运行的。而不是并行执行。<br><img src="/images/2019/celery_lock_task/50ef5fbc.png" alt=""></p><p>代码放在了：<br><a href="https://github.com/mikolaje/celery_toys/tree/master/test_lock" target="_blank" rel="external">https://github.com/mikolaje/celery_toys/tree/master/test_lock</a></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="http://docs.celeryproject.org/en/latest/tutorials/task-cookbook.html#ensuring-a-task-is-only-executed-one-at-a-time" target="_blank" rel="external">http://docs.celeryproject.org/en/latest/tutorials/task-cookbook.html#ensuring-a-task-is-only-executed-one-at-a-time</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;最近有个需求是这样子的，某个task要求一定要一个一个地执行，不能并发执行。&lt;br&gt;比较简单的办法是直接将 celery worker启动为
      
    
    </summary>
    
    
      <category term="python" scheme="http://mikolaje.github.io/categories/python/"/>
    
    
      <category term="celery" scheme="http://mikolaje.github.io/tags/celery/"/>
    
  </entry>
  
  <entry>
    <title>新西兰十二日自由行攻略</title>
    <link href="http://mikolaje.github.io/2019/nz_travel.html"/>
    <id>http://mikolaje.github.io/2019/nz_travel.html</id>
    <published>2019-02-20T03:00:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<p>新西兰:flag-nz:十二日自由行攻略<br>更多详情可上新西兰旅游局官方网站查询<a href="https://www.newzealand.com/cn/" target="_blank" rel="external">https://www.newzealand.com/cn/</a></p><p>:memo:签证：提前三个月办理即可，一般都能拿到有效期五年的签证。现在新西兰是电子签证，无需贴签，本人是找淘宝旅行社办理，如果个人不嫌麻烦完全可以自己登陆相关网站申请签证，蜂窝网有很全面的介绍，我是懒得自己弄。新西兰签证比欧美签证简单，银行卡半年流水只需三万，无需按指纹面试等，两周内可以出签。签证出来后留电子版或者打印出来，到新西兰过关时直接刷护照即可过关，都没人检查电子签，因为系统已有纪录。</p><p>另外如果是要转机的话，需要提前确认是否需要过境签。比如，在澳大利亚转机就要申请过境签。澳大利亚的过境签申请挺简单的，在官网注册个账号然后填个表就好了，免费的。</p><p><a href="https://immi.homeaffairs.gov.au/visas/getting-a-visa/visa-listing/transit-771" target="_blank" rel="external">https://immi.homeaffairs.gov.au/visas/getting-a-visa/visa-listing/transit-771</a></p><p>:airplane:飞机：建议提前买，签证没出来之前也可以买了（因为签证很容易过），越早买越便宜！直飞大概要11.5个小时，飞奥克兰的班次多且便宜。</p><p>我们来回买的都是在澳大利亚悉尼转机的。去的是维珍航空，回的是澳大利亚航空。维珍的机票比较便宜，便宜的缺点就是他们的refreshment好难吃，有一餐我根本就没吃饱，另外要了一小碗泡面。相比之下，澳大利亚航空的伙食就好太多了，吃得都好。</p><p>:dollar:现金：提前去银行换好新西兰币，汇率大概是1:4.6。准备好visa或者master卡，新西兰旅游景点很多可以刷微信、支付宝、银联。</p><p>:phone:电话卡：建议提前在淘宝买电话卡，比较便宜。朋友建议买Vodafone的卡，价格比较高，也可以买2degree的卡，华为背景价格便宜。我买了Vodafone和2degree的卡，感觉没啥区别，到了偏远地区都是没有信号。如果要自驾游的朋友最好提早在谷歌下载离线地图，免得走错路线。</p><p>:moneybag:保险：建议出发前买个基础保险，新西兰看病非常贵，发生拉肚子阑尾炎等情况必须看病，买了保险可以减轻负担。我们买的是安联保险，性价比很高。</p><p>:house:住宿：为了深入了解当地文化以及省钱，我们全程在Airbnb上订民宿，大家一定要订超赞房东，看住客的评论，以免入坑。</p><p>:bus:交通：新西兰交通费很贵，出租车尤其贵，Uber稍微比的士便宜点。公交车也比国内贵很多，如果是在皇后镇没办卡的情况下，每个人是五刀。办卡的话大概是2刀一次这样。自驾行虽然方便但是难度大，新西兰驾驶方向和国内相反，靠左行驶，山路多，当地人开车速度快。如果对国外驾驶规则不是很熟的话，不建议自驾。</p><p>具体行程:car:<br>前四天：奥克兰:boat:<br>奥克兰是新西兰最大的城市，一半是海水，一半是都市，是著名的风帆之都。这是我们在新西兰最喜欢的城市，大部分华人都住在奥克兰。我们参观了</p><p>【奥克兰战争纪念博物馆】</p><p><img src="/images/2019/nz_travel/DSC00223.JPG" alt="Sample Image Added via Markdown"></p><p>【伊甸山】</p><p>【天空塔】</p><p><img src="/images/2019/nz_travel/DSC00091.JPG" alt="Sample Image Added via Markdown"></p><p>【皇后大街】【海边码头】<br>强烈推荐新西兰唯一官方旅行社isite，在天空塔楼下就有一家，我们报了三次一日游，大巴包接送。最后一天，我们报了一日游前往【玛塔玛塔霍比特村】<br><img src="/images/2019/nz_travel/DSC00356.JPG" alt="Sample Image Added via Markdown"></p><p>，中土世界的童话小镇，满眼都是绿油油的草地。在奥克兰的交通方式就是uber和走路。<br>:fork_and_knife:推荐餐厅：Depot（天空塔下）、Giapo冰淇淋</p><p>中间四天：皇后镇:crown:<br>皇后镇是非常出名的旅游城市啦，超级多游客，中国游客尤其多。皇后镇衣食住行都很贵。皇后镇风光旖旎，生态环境也很好，新西兰的自来水可以直接喝。我们参观了</p><p>【格林诺奇】</p><p>【箭镇】</p><p>【瓦卡蒂普湖】</p><p><img src="/images/2019/nz_travel/IMG_3222.JPG" alt="Sample Image Added via Markdown"></p><p>【坐蒸汽船看牧场喂动物】</p><p><img src="/images/2019/nz_travel/IMG_3200.JPG" alt="Sample Image Added via Markdown"></p><p>皇后镇可以买优惠公交卡，10纽币可以坐5次，非常划算！最后一天，我们报了I-site的一日游，从皇后镇坐大巴到基督城，沿途经过库克山/蒂卡波湖/好牧羊人教堂。<br>:fork_and_knife:推荐餐厅：Fergburger汉堡</p><p><img src="/images/2019/nz_travel/IMG_3081.JPG" alt="Sample Image Added via Markdown"></p><p>最后两天：基督城:latin_cross:<br>基督城于2011年发生大地震，现在城市还没恢复过来，比较萧条，没什么人气。可以购买有轨复古电车:railway_car:一日游，随上随下，</p><p><img src="/images/2019/nz_travel/DSC00670.JPG" alt="Sample Image Added via Markdown"></p><p>在站点下车可参观【基督城博物馆】</p><p>【超级漂亮的海格利公园】</p><p><img src="/images/2019/nz_travel/DSC00650.JPG" alt="Sample Image Added via Markdown"></p><p>【基督城大教堂】</p><p><img src="/images/2019/nz_travel/IMG_3253.jpg" alt="Sample Image Added via Markdown"></p><p>【图书馆】</p><p>【纸板大教堂】</p><p>【皇后广场】基</p><p>督城内很多涂鸦，比较文艺。</p><p><img src="/images/2019/nz_travel/IMG_3266.JPG" alt="Sample Image Added via Markdown"></p><p>留一两天就够了。<br>newzealand.com<br>新西兰旅游局欢迎您 | 最全面最官方的新西兰旅游攻略<br>欢迎您来新西兰旅游。敬请浏览新西兰旅游局官方网站，了解新西兰旅游攻略、经典观光线路推荐、特色景点介绍、当地美酒佳肴、酒店住宿推荐、航班与机场信息等内容。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;新西兰:flag-nz:十二日自由行攻略&lt;br&gt;更多详情可上新西兰旅游局官方网站查询&lt;a href=&quot;https://www.newzealand.com/cn/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.newzealand.
      
    
    </summary>
    
    
      <category term="life" scheme="http://mikolaje.github.io/categories/life/"/>
    
    
      <category term="life" scheme="http://mikolaje.github.io/tags/life/"/>
    
      <category term="travel" scheme="http://mikolaje.github.io/tags/travel/"/>
    
  </entry>
  
  <entry>
    <title>Ngrok服务器部署</title>
    <link href="http://mikolaje.github.io/2019/ngrok_deployment.html"/>
    <id>http://mikolaje.github.io/2019/ngrok_deployment.html</id>
    <published>2019-01-02T04:07:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>自己使用Google Cloud，阿里云，AWS有好几年了，在云服务器上花了不少钱，有段时间做爬虫，需要较大的磁盘空间（几百G），每个月大概要5，600元。<br>一年下来就6，7钱了。想想，还是直接买一台主机或服务器在家里放着好了，也就几千块钱。于是我买了台主机在家里放着，配置远远比原来ECS的配置要高，<br>而且还更便宜。把数据全部迁移到了自己的主机，但很多时候自己都不在家，不可能把笨重的主机带出去外面。于是得想办法在其它地方通过网络连接到主机。<br>刚开选择用Teamviewer，可以远程连接到主机的桌面，我的主机的操作系统是Ubuntu的，感觉还行。但是，慢慢感觉效率不是很高，因为是用桌面远程，</p><p>所以网速要求，还是有点高，更大的问题是Teamviewer是收费的，而且价格不菲，这样成本算下来一年下来也要花费挺多钱。<br>于是，我想到了以前的花生壳 内网穿透映射，但那个也是收费的。终于找到了一个令我满意的解决方案—-Ngrok！</p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>Ngrok到底好在哪里呢？ 首先，也是最重要的是它的成本底。<br>主机买服务的话也就10多块钱一个月，网上一搜就很多Ngrok的提供商；自己搭建的话，也就买一台虚拟机的价格，我买的是某云的ECS。大概也就几十来块钱。</p><p>其次，Ngrok可以把你家里的电脑当成服务器来使用，远程登陆SSH；或者作为数据库，暴露对应的端口提供服务；或者直接作为Web服务，直接提供http访问。<br>这使得不用买昂贵的云服务，省了不少钱。不够配置的话可以自己买内存，SSD增加机器配置。</p><h3 id="准备工作："><a href="#准备工作：" class="headerlink" title="准备工作："></a>准备工作：</h3><h4 id="需要购买的服务"><a href="#需要购买的服务" class="headerlink" title="需要购买的服务"></a>需要购买的服务</h4><ol><li>有一个外网的虚拟机，我用的是国内的阿里云的，操作系统是Ubuntu。</li><li>有个自己注册的域名。<br>需要添加2条域名解析：<br><img src="/images/2019/ngrok_deployment/647f55bb.png" alt=""></li></ol><h4 id="部署工作："><a href="#部署工作：" class="headerlink" title="部署工作："></a>部署工作：</h4><ol><li><p>apt-get install golang<br>因为ngrok是用Go 来写的，所以要安装一下Go环境</p></li><li><p>git clone <a href="https://github.com/inconshreveable/ngrok.git" target="_blank" rel="external">https://github.com/inconshreveable/ngrok.git</a><br>然后把ngrok的源码clone下来。</p></li></ol><h4 id="证书安装："><a href="#证书安装：" class="headerlink" title="证书安装："></a>证书安装：</h4><blockquote><p>使用ngrok.com官方服务时，我们使用的是官方的SSL证书。自己建立ngrok服务，需要我们生成自己的证书，并提供携带该证书的ngrok客户端。首先指定域名：</p></blockquote><p>进入ngrok目录后，运行下面的指令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> NGROK_DOMAIN=<span class="string">"ngrok.xfl.host"</span>  </div><div class="line">//这里换成你的自己注册的域名，比如你注册的域名为abc.com。这里可以填 ngrok.abc.com</div><div class="line"></div><div class="line">openssl genrsa -out rootCA.key 2048</div><div class="line">openssl req -x509 -new -nodes -key rootCA.key -subj <span class="string">"/CN=<span class="variable">$NGROK_DOMAIN</span>"</span> -days 5000 -out rootCA.pem</div><div class="line">openssl genrsa -out device.key 2048</div><div class="line">openssl req -new -key device.key -subj <span class="string">"/CN=<span class="variable">$NGROK_DOMAIN</span>"</span> -out device.csr</div><div class="line">openssl x509 -req -in device.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out device.crt -days 5000</div></pre></td></tr></table></figure></p><p>我们在编译可执行文件之前，需要把生成的证书分别替换到 assets/client/tls和assets/server/tls中，这两个目录分别存放着ngrok和ngrokd的默认证书。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cp rootCA.pem assets/client/tls/ngrokroot.crt</div><div class="line">cp device.crt assets/server/tls/snakeoil.crt</div><div class="line">cp device.key assets/server/tls/snakeoil.key</div></pre></td></tr></table></figure><h3 id="ngrok部署安装"><a href="#ngrok部署安装" class="headerlink" title="ngrok部署安装"></a>ngrok部署安装</h3><h4 id="编译ngrokd-和-ngrok"><a href="#编译ngrokd-和-ngrok" class="headerlink" title="编译ngrokd 和 ngrok"></a>编译ngrokd 和 ngrok</h4><p>首先需要知道，ngrokd 为服务端的执行文件，ngrok为客户端的执行文件。</p><p>有没有release的区别是，包含release的编译结果会把assets目录下的内容包括进去，从而可以独立执行。<br>如果你今后还要更换证书，建议编译不包含release的版本。。首先编译ngrok服务端（ngrokd），默认为Linux版本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">make clean</div><div class="line">make release-server</div></pre></td></tr></table></figure></p><p>编译ngrokd后，我们来编译ngrok(客户端)<br>在编译客户端的时候需要指明对应的操作系统和构架：</p><p>Linux 平台 32 位系统：GOOS=linux GOARCH=386<br>Linux 平台 64 位系统：GOOS=linux GOARCH=amd64<br>Windows 平台 32 位系统：GOOS=windows GOARCH=386<br>Windows 平台 64 位系统：GOOS=windows GOARCH=amd64<br>MAC 平台 32 位系统：GOOS=darwin GOARCH=386<br>MAC 平台 64 位系统：GOOS=darwin GOARCH=amd64<br>ARM 平台：GOOS=linux GOARCH=arm</p><p>我的Ubuntu属于linux 64，所以我执行如下指令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">GOOS=linux GOARCH=amd64 make release-client</div></pre></td></tr></table></figure></p><h4 id="启动ngrokd"><a href="#启动ngrokd" class="headerlink" title="启动ngrokd"></a>启动ngrokd</h4><p>编译后生成两个文件分别为服务端（ngrokd）和客户端(ngrok)。切换到对应的文件夹，运行服务端：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">./ngrokd -domain=<span class="string">"<span class="variable">$NGROK_DOMAIN</span>"</span> -httpAddr=<span class="string">":801"</span> -httpsAddr=<span class="string">":802"</span></div><div class="line">//801 是访问的http端口，比如，在本例子中，访问http://ngrok.xfl.host:801 就可以看到映射出的网页</div></pre></td></tr></table></figure><p>参数-domain表示服务器域名，请改成你自己的域名；-httpAddr表示默认监听的HTTP端口，-httpsAddr表示默认监听的HTTPS端口，<br>因为我用不到所以都设置成空字符串”“来关闭监听，如果需要打开的话记得格式是:12345（冒号+端口号）这样的；-tunnelAddr表示服务器监听客户端连接的隧道端口号，格式和前面一样；<br>-log表示日志文件位置；还有个-log-level用来控制日志记录的事件级别，选项有DEBUG、INFO、WARNING、ERROR。</p><p>如果编译的是不带release的版本，还可以通过-tlsCrt和-tlsKey选项来指定证书文件的位置。</p><p>出现类似以下内容，则说明我们的服务器端ngrokd正常运行了:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[16:41:56 CST 2017/04/20] [INFO] (ngrok/log.(*PrefixLogger).Info:83) [registry] [tun] No affinity cache specified</div><div class="line">[16:41:56 CST 2017/04/20] [INFO] (ngrok/log.(*PrefixLogger).Info:83) [metrics] Reporting every 30 seconds</div><div class="line">[16:41:57 CST 2017/04/20] [INFO] (ngrok/log.Info:112) Listening for public http connections on [::]:80</div><div class="line">[16:41:57 CST 2017/04/20] [INFO] (ngrok/log.Info:112) Listening for public https connections on [::]:443</div><div class="line">[16:41:57 CST 2017/04/20] [INFO] (ngrok/log.Info:112) Listening for control and proxy connections on [::]:4443</div><div class="line">[16:41:57 CST 2017/04/20] [INFO] (ngrok/log.(*PrefixLogger).Info:83) [tun:627acc92] New connection from 42.53.196.242:9386</div><div class="line">[16:41:57 CST 2017/04/20] [DEBG] (ngrok/log.(*PrefixLogger).Debug:79) [tun:627acc92] Waiting to read message</div><div class="line">[16:41:57 CST 2017/04/20] [DEBG] (ngrok/log.(*PrefixLogger).Debug:79) [tun:627acc92] Reading message with length: 159</div></pre></td></tr></table></figure><h4 id="配置ngrok客户端"><a href="#配置ngrok客户端" class="headerlink" title="配置ngrok客户端"></a>配置ngrok客户端</h4><p>将之前编译好的客户端文件(ngrok 文件)拷贝到需要使用服务的设备（自己买的那台笨重的主机）上。</p><h4 id="启动ngrok客户端"><a href="#启动ngrok客户端" class="headerlink" title="启动ngrok客户端"></a>启动ngrok客户端</h4><p>在ngrok同路径下建立配置文件ngrok.yml<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="attr">server_addr:</span> <span class="string">"ngrok.xfl.host:4443"</span></div><div class="line"><span class="attr">trust_host_root_certs:</span> <span class="literal">false</span></div><div class="line"><span class="attr">tunnels:</span></div><div class="line"><span class="attr">  ssh:</span></div><div class="line"><span class="attr">    remote_port:</span> <span class="number">6666</span></div><div class="line"><span class="attr">    proto:</span></div><div class="line"><span class="attr">      tcp:</span> <span class="number">22</span></div></pre></td></tr></table></figure></p><p>server_addr端口默认4443，可通过ngrokd服务端启动修改端口。在tunnels里配置隧道信息，<br>注意http和https隧道可设置subdomain和auth，而tcp里只能设置remote_port。</p><p>使用如下命令启动ngrok客户端：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ngrok -log=stdout -config=./ngrok.yml start ssh</div></pre></td></tr></table></figure></p><p>正常启动，你将会看到如下日志：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">ngrok                                                                                                                                                                                     (Ctrl+C to quit)</div><div class="line">                                                                                                                                                                                                          </div><div class="line">Tunnel Status                 online                                                                                                                                                                      </div><div class="line">Version                       1.7/1.7                                                                                                                                                                     </div><div class="line">Forwarding                    http://demo.ngrok.xfl.host -&gt; 127.0.0.1:19999                                                                                                                               </div><div class="line">Forwarding                    https://demo.ngrok.xfl.host -&gt; 127.0.0.1:19999                                                                                                                              </div><div class="line">Web Interface                 127.0.0.1:4040                                                                                                                                                              </div><div class="line"># Conn                        0                                                                                                                                                                           </div><div class="line">Avg Conn Time                 0.00ms</div></pre></td></tr></table></figure></p><p>Notice:如果显示reconnecting说明连接有错，在运行时加入-log=stdout来进行debug。</p><ol><li>有可能是因为你的Ngrok服务器没有开 4443端口</li><li>有可能是域名没有解析成功</li></ol><h4 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h4><p>ngrok 的配置文件是完全可选的非常简单 YAML 格式文件，他可以允许你使用 ngrok 一些更高级的功能，例如：</p><p>同时运行多个隧道</p><ul><li>连接到自定义的 ngrok 服务器</li><li>调整 ngrok 一些很神秘的功能</li><li><p>ngrok 的配置文件默认从 ~/.ngrok 加载。你可以通过 -config 参数重写配置文件的地址</p></li><li><p>同时运行多个隧道<br>为了运行多个隧道，你需要在配置文件当中使用 tunnels 参数配置每个隧道。隧道的参数以字典的形式配置在配置文件当中。<br>举个例子，让我们来定义三个不同的隧道。第一个隧道是一个有认证的只转发 https 的隧道。第二个隧道转发我们自己机器的 22 端口以便让我可以通过隧道连接到自己的电脑。<br>最后，我们使用自己的域名创造了一个隧道，我们将要在黑客马拉松中展示这个。</p></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="attr">tunnels:</span></div><div class="line"><span class="attr">  client:</span></div><div class="line"><span class="attr">    auth:</span> <span class="string">"user:password"</span></div><div class="line"><span class="attr">    proto:</span></div><div class="line"><span class="attr">      https:</span> <span class="number">8080</span></div><div class="line"><span class="attr">  ssh:</span></div><div class="line"><span class="attr">    proto:</span></div><div class="line"><span class="attr">      tcp:</span> <span class="number">22</span></div><div class="line">  hacks.inconshreveable.com:</div><div class="line"><span class="attr">    proto:</span></div><div class="line"><span class="attr">      http:</span> <span class="number">9090</span></div></pre></td></tr></table></figure><p>通过 ngrok start 命令，我们可以同时运行三个隧道，后面要接上我们要启动的隧道名。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ngrok start client ssh hacks.inconshreveable.com</div></pre></td></tr></table></figure><ul><li>隧道设置<br>每一个隧道都可以设置以下五个参数：proto，subdomain，auth，hostname 以及 remote_port。<br>每一个隧道都必须定义 proto ，因为这定义了协议的类型以及转发的目标。当你在运行 http/https 隧道时， auth 参数是可选的，<br>同样， remote_port 也是可选的，他声明了某个端口将要作为远程服务器转发的端口，请注意这只适用于 TCP 隧道。<br>ngrok 使用每个隧道的名字做到子域名或者域名，但你可以重写他：</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="attr">tunnels:</span></div><div class="line"><span class="attr">  client:</span></div><div class="line"><span class="attr">    subdomain:</span> <span class="string">"example"</span></div><div class="line"><span class="attr">    auth:</span> <span class="string">"user:password"</span></div><div class="line"><span class="attr">    proto:</span></div><div class="line"><span class="attr">      https:</span> <span class="number">8080</span></div></pre></td></tr></table></figure><p>对于 TCP 隧道，你可以会通过 remote_port 参数来指定一个远程服务器的端口作为映射。如果没有声明，服务器将会给你随机分配一个端口。</p><h3 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h3><p>如果使用国内IP或者国内的域名的话，用http的话貌似因为要备案，所以访问不了。前两天我是可以正常访问的，但是过了两天后就突然访问不了了。<br>可以改为用TCP的方式比如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">netdata:</div><div class="line">  remote_port: 801 </div><div class="line">  proto:</div><div class="line">    tcp: 19999</div></pre></td></tr></table></figure></p><p>访问 ngnrok.xfl.host:801 即可。</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://morongs.github.io/2016/12/28/dajian-ngrok/" target="_blank" rel="external">https://morongs.github.io/2016/12/28/dajian-ngrok/</a><br><a href="https://luozm.github.io/ngrok" target="_blank" rel="external">https://luozm.github.io/ngrok</a><br><a href="https://imlonghao.com/28.html" target="_blank" rel="external">https://imlonghao.com/28.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;自己使用Google Cloud，阿里云，AWS有好几年了，在云服务器上花了不少钱，有段时间做爬虫，需要较大的磁盘空间（几百G），每个月大概
      
    
    </summary>
    
    
      <category term="运维" scheme="http://mikolaje.github.io/categories/%E8%BF%90%E7%BB%B4/"/>
    
    
      <category term="ngrok" scheme="http://mikolaje.github.io/tags/ngrok/"/>
    
  </entry>
  
  <entry>
    <title>CDH 在ubuntu上的部署和安装，以及一些坑</title>
    <link href="http://mikolaje.github.io/2019/cdh_install.html"/>
    <id>http://mikolaje.github.io/2019/cdh_install.html</id>
    <published>2019-01-02T02:24:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<p>最近两天在自己电脑上搭建一个Cloudera Manager来玩玩。本来以为挺简单的，只是在Web UI上无脑下一步就好了，<br>但其实还是遇到挺多问题的。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="在服务器上的操作"><a href="#在服务器上的操作" class="headerlink" title="在服务器上的操作"></a>在服务器上的操作</h3><p>刚开始基本上，就是按照官网的步骤来走，首先做一些前置工作:</p><ol><li>配置下apt</li><li>安装JDK.</li><li>安装下NTP时间同步的程序；</li><li>安装好Mysql，MariaDB，Posgres。<br>其中的一个数据库，刚开始以为都要安装。。。然后又把MariaDB这些一个个卸载了；</li><li>在Mysql中创建一些CM所需的数据库和表。如下所是。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">CREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON scm.* TO &apos;scm&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON rman.* TO &apos;rman&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON hue.* TO &apos;hue&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON metastore.* TO &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON sentry.* TO &apos;sentry&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE nav DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON nav.* TO &apos;nav&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE navms DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON navms.* TO &apos;navms&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div><div class="line"></div><div class="line">CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</div><div class="line">GRANT ALL ON oozie.* TO &apos;oozie&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;</div></pre></td></tr></table></figure><ol><li>通过scm_prepare_database.sh 脚本来进一步设置Manager Database<br>sudo /opt/cloudera/cm/schema/scm_prepare_database.sh [options] <databasetype> <databasename> <databaseuser> <password><br>只需要执行一次<code>sudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm mypassword</code>就好了。</password></databaseuser></databasename></databasetype></li></ol><h3 id="在Web-UI上的安装"><a href="#在Web-UI上的安装" class="headerlink" title="在Web UI上的安装"></a>在Web UI上的安装</h3><ol><li>首先记得在每台机器上配置好/etc/hosts</li><li>在Web上的安装基本上就是点继续。有些地方要注意。<br>这一步会等待比较长的时间，会下载安装一些parcels。</li></ol><p><img src="/images/2019/cdh_install/323205c4.png" alt="Sample Image Added via Markdown"></p><ol><li>然后，基本上就是下一步了。到这一步，我是创建了一个叫cloudera的用户，要给与它sudo以及password-exempt<br><img src="/images/2019/cdh_install/cloudera_install.jpg" alt="Sample Image Added via Markdown"></li></ol><p>对了，因为我的是单机版的，所以HDFS那边会报错一个叫：副本不足的块 存在隐患。<br>这是因为只有一个节点，Block块无法，分配到其它的节点作为备份。默认是有2个备份Block分发到其它节点。</p><h3 id="启动CDH"><a href="#启动CDH" class="headerlink" title="启动CDH"></a>启动CDH</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sudo systemctl start cloudera-scm-server</div><div class="line"></div><div class="line"># 查看scm server日志，scm的全称是：The Service and Configuration Manager </div><div class="line">sudo tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log</div></pre></td></tr></table></figure><h3 id="停止CDH"><a href="#停止CDH" class="headerlink" title="停止CDH"></a>停止CDH</h3><p>有些时候我们要停机检修一下电脑，所以要停止Cluster。很简单，首先进入到CDH的Web UI的Cluster主界面, 左上角有个Action<br>，点一下弹出下拉条，然后选停止。等几分钟后集群的所有组件就会停止了。</p><p>然后进入到主节点的终端，输入<code>sudo systemctl stop cloudera-scm-server</code>，就全部停止了。</p><h2 id="CDH的一些配置"><a href="#CDH的一些配置" class="headerlink" title="CDH的一些配置"></a>CDH的一些配置</h2><h3 id="Yarn-RM-NM共用一个host"><a href="#Yarn-RM-NM共用一个host" class="headerlink" title="Yarn: RM,NM共用一个host"></a>Yarn: RM,NM共用一个host</h3><p>默认情况下Resource Manager会单独用一个节点。但是我的RM host内存和CPU都有剩余，跑app的时候把资源压在<br>device2上有点浪费了，我利用起device1的资源来。<br>首先进入到Yarn的版块，Action下拉框，点击Add Role Instance<br><img src="/images/2019/cdh_install/yarn_rm_nm1.png" alt="Sample Image Added via Markdown"><br><img src="/images/2019/cdh_install/yarn_rm_nm1.png" alt="Sample Image Added via Markdown"></p><h3 id="增加服务"><a href="#增加服务" class="headerlink" title="增加服务"></a>增加服务</h3><p>如果我们想新增加一些组件，比如Kafka或Spark，然后我们可以点击Cluster版块的Action下拉框，选中第一个 Add Service<br>进入新增Service的页面。</p><h2 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h2><h3 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h3><p>我增加一个节点的时候遇到如下报错<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Host with invalid Cloudera Manager GUID is detected</div><div class="line">...</div><div class="line">Error, CM server guid updated, expected c3b5fe15-5f29-434b-ae0a-4750b56c72ab, received dc1d28d4-4c78-4a07-919b-a9eaf7190d41</div></pre></td></tr></table></figure></p><p>解决方法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">验证如下配置文件，确定hostname是否正确</div><div class="line">$ nano /etc/cloudera-scm-agent/config.ini</div><div class="line">so that the hostname where the same as the command $ hostname returned.</div><div class="line">Then rm /var/lib/cloudera-scm-agent/cm_guid</div><div class="line">然后删除每个节点的cm_guid</div><div class="line">then I restarted the agent and the server of cloudera:</div><div class="line">然后重启</div><div class="line">$ service cloudera-scm-agent restart</div><div class="line">$ service cloudera-scm-server restart</div></pre></td></tr></table></figure></p><h3 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h3><p>在NameNode Format的时候遇到如下报错<br>Running in non-interactive mode, and data appears to exist in Storage Directory /dfs/nn. Not formatting.</p><p>解决方案：<br>删除/dfs/nn 以及 /dfs/dn里面的所有数据<br>因为之前我安装了一个单机集群，HDFS里面放了一些数据</p><h3 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h3><p>Cloudera 在Validate Hive Metastore schema的时候出现如下错误，发现metastore里面没有VERSION table<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">Fri Jul 19 14:06:33 CST 2019 WARN: Establishing SSL connection without server&apos;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&apos;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &apos;false&apos;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</div><div class="line">org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version, Cause:Table &apos;metastore.VERSION&apos; doesn&apos;t exist</div><div class="line">org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version, Cause:Table &apos;metastore.VERSION&apos; doesn&apos;t exist</div><div class="line">at org.apache.hadoop.hive.metastore.CDHMetaStoreSchemaInfo.getMetaStoreSchemaVersion(CDHMetaStoreSchemaInfo.java:342)</div><div class="line">at org.apache.hive.beeline.HiveSchemaTool.validateSchemaVersions(HiveSchemaTool.java:685)</div><div class="line">at org.apache.hive.beeline.HiveSchemaTool.doValidate(HiveSchemaTool.java:578)</div><div class="line">at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1142)</div><div class="line">at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</div><div class="line">at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</div><div class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class="line">at java.lang.reflect.Method.invoke(Method.java:498)</div><div class="line">at org.apache.hadoop.util.RunJar.run(RunJar.java:313)</div><div class="line">at org.apache.hadoop.util.RunJar.main(RunJar.java:227)</div><div class="line">*** schemaTool failed ***</div></pre></td></tr></table></figure></p><p>解决方案：<br><code>dennis@device1:/opt/cloudera/parcels/CDH/lib/hive/bin$ schematool -dbType mysql -initSchema -passWord password -userName hive</code></p><h3 id="问题4"><a href="#问题4" class="headerlink" title="问题4"></a>问题4</h3><p>在Hue上的hive上运行一些 insert 和count(*) 操作时候会一直卡住（stuck, hang），没有任何反应，也没报错。<br>看日志是说MR 还没有启动。在Cloudera的community上查到 要mapred-site.xml的参数 mapreduce.framework.name 设置为 local</p><p>于是我在CDH中的Yarn集群下修改了mapreduce.framework.name 为 local，然后重启集群后就成功了。 select count(*) 和 insert就不会卡住了。</p><h3 id="问题5"><a href="#问题5" class="headerlink" title="问题5"></a>问题5</h3><p>在hue上面可以正常地使用Hive。在device2下用hive cli没有问题。但在device1 下的bash执行hive command，里面输入show databases后报错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</div></pre></td></tr></table></figure></p><p> 解决方案：<br> 先用 hive -hiveconf hive.root.logger=DEBUG,console<br> 在调试，查看到更多有价值的报错信息。<br> 果然，查到如下信息：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"> Caused by: java.io.IOException: Keystore was tampered with, or password was incorrect</div><div class="line">at com.sun.crypto.provider.JceKeyStore.engineLoad(JceKeyStore.java:865) ~[sunjce_provider.jar:1.8.0_112]</div><div class="line">at java.security.KeyStore.load(KeyStore.java:1445) ~[?:1.8.0_121]</div><div class="line">at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.locateKeystore(AbstractJavaKeyStoreProvider.java:322) ~[hadoop-common-3.0.0-cdh6.2.0.jar:?]</div><div class="line">at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.&lt;init&gt;(AbstractJavaKeyStoreProvider.java:86) ~[hadoop-common-3.0.0-cdh6.2.0.jar:?]</div><div class="line">at org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider.&lt;init&gt;(LocalJavaKeyStoreProvider.java:58) ~[hadoop-common-3.0.0-cdh6.2.0.jar:?]</div><div class="line"></div><div class="line">at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:237) ~[hive-exec-2.1.1-cdh6.2.0.jar:2.1.1-cdh6.2.0]</div><div class="line">... 23 more</div><div class="line">Caused by: java.security.UnrecoverableKeyException: Password verification failed</div></pre></td></tr></table></figure></p><p>按日志的报错信息来说是我的源数据库密码不对，于是我查看hive-site.xml配置文件，发现<br>/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/hive/conf/hive-site.xml<br>也就是/etc/hive/conf/hive-site.xml(我猜CDH会把上面目录的所有配置文件复制一遍到 /etc/hive/conf/下)<br>我之前把它改了，所以那配置有问题，我把它改为默认的配置重启后就恢复正常了！</p><h3 id="问题6"><a href="#问题6" class="headerlink" title="问题6"></a>问题6</h3><p>启动 cloudera-scm-server 时候报错如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table &apos;metastore.CM_VERSION&apos; doesn&apos;t exist</div></pre></td></tr></table></figure></p><p> 解决方案：<br><code>sudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm mypassword</code><br>重新执行下这条命令。之前我把所有的服务都执行了一遍（amon, rman, … metastore, … etc)，是我误解了scm_prepare_database.sh的作用。<br>按官网所说的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Cloudera Manager Server includes a script that can create and configure a database for itself. The script can:</div><div class="line">Create the Cloudera Manager Server database configuration file.</div><div class="line">(MariaDB, MySQL, and PostgreSQL) Create and configure a database for Cloudera Manager Server to use.</div><div class="line">(MariaDB, MySQL, and PostgreSQL) Create and configure a user account for Cloudera Manager Server.</div></pre></td></tr></table></figure></p><p>这个脚本只需要执行一次就好了，就是<code>sudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm mypassword</code><br>然后重启cloudera-scm-server解决问题。此外，通过<code>/etc/cloudera-scm-server/db.properties</code> 也可以确定目前scm用的是哪个数据库。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.cloudera.com/documentation/enterprise/6/6.2/topics/introduction.html" target="_blank" rel="external">https://www.cloudera.com/documentation/enterprise/6/6.2/topics/introduction.html</a><br><a href="https://blog.csdn.net/qq_24409555/article/details/76139886" target="_blank" rel="external">https://blog.csdn.net/qq_24409555/article/details/76139886</a><br><a href="https://community.cloudera.com/t5/Batch-SQL-Apache-Hive/Hive-Errors-happend-when-execute-hive-service-metastore/m-p/93050#M3282" target="_blank" rel="external">https://community.cloudera.com/t5/Batch-SQL-Apache-Hive/Hive-Errors-happend-when-execute-hive-service-metastore/m-p/93050#M3282</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近两天在自己电脑上搭建一个Cloudera Manager来玩玩。本来以为挺简单的，只是在Web UI上无脑下一步就好了，&lt;br&gt;但其实还是遇到挺多问题的。&lt;/p&gt;
&lt;h2 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安
      
    
    </summary>
    
    
      <category term="大数据" scheme="http://mikolaje.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="cloudera" scheme="http://mikolaje.github.io/tags/cloudera/"/>
    
      <category term="CDH" scheme="http://mikolaje.github.io/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>启德教育被坑记</title>
    <link href="http://mikolaje.github.io/2018/study_abroad_agency.html"/>
    <id>http://mikolaje.github.io/2018/study_abroad_agency.html</id>
    <published>2018-12-31T04:07:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>留学一直都是我的梦想，但因为种种原因毕业后没能实现，工作了几年后，还是放不下，于是打算去香港留学，因为离家里比较近，年纪大了，不方便出走太远。<br>但由于平时要上班工作，没有时间，也不想花太多时间在申请上，于是就打算找中介。在国内，留学中介中知名度<br>最高的是启德吧。我看他们家的广告随处可见，之前也些朋友找的启德做中介的。于是我就选了启德。</p><p>当时中介告诉我，已经是年底了，申请的时间比较晚，要赶紧行动。于是中介耐心地给我介绍了我这条件能去的学校和专业，就赶紧交钱申请。<br>当时交了1.5万（还不包括申请费，申请费还得要近3000RMB），6各专业好像。他们是按专业数量收费的。加专业还要加钱。</p><p>当时我完全不知道他们是怎么操作的，对留学申请我也完全没有经验和概念。我只是在很多年前考过一次雅思，6分。就这样，我被中介牵着鼻子走了。<br>中介让我干嘛我就干嘛。</p><p>中介说让我先准备考雅思。然后在用中文写下各专业的文书，再递给中介让她们翻译。</p><p>经过一两个月的努力，我把雅思考到6.5(低分6)了。于是就把中文的文书交给中介，结果他们等了大半个月才翻译出来。翻译出来的质量当时我根本没有看。<br>于是他们就帮我投递了。最先投的是浸会大学，很快就有回复了，于是安排我面试。面试时候，面试官只是简单跟我聊聊，为什么选择这个专业呀，对选择这个program有什么挑战，<br>等等。很快，一个礼拜后就给我发offer了。</p><p>收到offer，意味着不能退款了。1.5万就没了。后来等了很久也没等到，其它专业的offer，城大，理工的都没有任何回复，连面试通知都没。很失落，最后也没有接收浸会的offer，<br>没有交留位费。就放弃了当年的入学机会。</p><p>时隔半年后，我再次打算申请。我又交了1.2万给启德，当时也是怪自己没有花精力去了解留学申请，太过于依赖中介了。并且，自己的留学欲望又很强。这就导致我很轻易地把这钱交给中介了。<br>我都没怎么犹豫，也不觉得一两万前很多，很果断地打钱过去了。</p><p>去年的入学失败，使得我今年必须得抓住机会，不然，很有可能以后都没有机会出去了。于是，我今年的申请盯得特别紧。我第一次认真看启德写的PS文书，对比地看了下985学生申请的文书，<br>那差距啊。。。启德写的文书这是什么玩意啊，简直就是笑话。写的基本上通篇都是废话。开头就是说自己的人生格言，然后是出生家庭什么的。说了一大堆跟自己的能力展现没任何关系的<br>废话内容。于是，我基本上把所有专业的PS文书都重新写了遍。然后是进行艰苦的修改阶段， 改了又改。自己还另外掏了千百块块找了其他人帮忙修改查看。花了好多心血才把终稿定下来了。</p><p>TM的我这一切努力都和中介没毛关系啊，我出这么钱，中介没帮我啥事情啊。从备战雅思，去学校申请资料，修改文书PS，都是我自己的努力和艰辛。中介做的就是帮你递交申请资料而已，<br>PS这些虽然他们写了，但质量太差，基本上用不了，还是要你自己重新再写一遍。这些都算了，最让我气愤的是：交完钱后，中介就对你不上心了。比如微信回复不及时回，而且回复的态度<br>完全和你交钱之前的不一样。带电话咨询她，她还说，明天有问题问助理，我明天休息不方便接听电话。这让人听起来感觉是非常不热情，不友好的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;正文&quot;&gt;&lt;a href=&quot;#正文&quot; class=&quot;headerlink&quot; title=&quot;正文&quot;&gt;&lt;/a&gt;正文&lt;/h2&gt;&lt;p&gt;留学一直都是我的梦想，但因为种种原因毕业后没能实现，工作了几年后，还是放不下，于是打算去香港留学，因为离家里比较近，年纪大了，不方便出走太远
      
    
    </summary>
    
    
      <category term="life" scheme="http://mikolaje.github.io/categories/life/"/>
    
    
      <category term="daily" scheme="http://mikolaje.github.io/tags/daily/"/>
    
  </entry>
  
  <entry>
    <title>华强北被坑记</title>
    <link href="http://mikolaje.github.io/2018/huaqiangbei_black_market.html"/>
    <id>http://mikolaje.github.io/2018/huaqiangbei_black_market.html</id>
    <published>2018-12-22T11:07:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<h2 id="好奇心"><a href="#好奇心" class="headerlink" title="好奇心"></a>好奇心</h2><p>之前有段时间一直为了好奇，想拍摄一些比较隐蔽的探访视频然后放YouTube上赚点广告费。想了很久，终于跑到忍不住<br>跑到华强北去寻找买一个眼镜摄像仪。就是带着摄像头在眼镜上的偷拍神器。去到华强北摄像头市场里，问了好多商铺，<br>他们都不卖，当然是私底下地问，因为这东西肯定不是不能明着卖的。后来还是找了个路边站街拉客的大妈，一问就有了。<br>她让我跟着她走，走到一个稍微人少点地方，她打电话给卖这东西的贩子，然后就在那等那贩子过来。</p><p>10分钟后，一个矮矮的中年男子（四五十岁），拿着个黑色大袋子走过来，说带我去一个更少人的角落验货。终于看到了<br>传说中的眼镜拍摄神器。这玩意超出我的预期，并没有我想象中的那么好，因为它一看就有些不太对头，不像一个正常的眼镜，<br>左右两根支架特别粗，因为里面有电路板，还要放置MINI-SD卡。它的摄像头在眼镜的最中间位置，也就是连接两个镜片<br>的地方，不仔细看的话，不会发现，不过稍微仔细点看还是能看出来它是有个摄像头在上面的。</p><p>接着，那个贩子拍摄了一段视频，我感觉视频的质量，分别率不是很好，大概就是几百块钱手机的拍摄效果吧。虽然不太满意，<br>但感觉跟那贩子看了有点久，有点骑虎难下，最后还是买下来了，花了750，他开价是900块。终于止住我那段时间以来的好奇心和欲望。</p><p>这东西还不带SD卡，于是我还花了几十块钱买了个SD卡。那天总共花了850左右吧，现在想想感觉还是挺不值的，因为拿回来后<br>也就拍了一下，就再也没用过了，感觉就是一个玩具而已，没有什么实际的用途。</p><p><img src="/images/2018/huaqiangbei_black_market/36750b47.png" alt=""></p><p><img src="/images/2018/huaqiangbei_black_market/8984d475.png" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>就当花钱买个教训吧，我记得，我不是我第一次在华强北被坑，以前还很想买一种录音手表，后来以色列人送了我一个，大概是一百多块钱吧，用了几次就扔了。<br>以后还是要注意下自己的消费欲望。买之前再三思考，这东西值不值。不过很多东西其实还是买来后才知道它值不值。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;好奇心&quot;&gt;&lt;a href=&quot;#好奇心&quot; class=&quot;headerlink&quot; title=&quot;好奇心&quot;&gt;&lt;/a&gt;好奇心&lt;/h2&gt;&lt;p&gt;之前有段时间一直为了好奇，想拍摄一些比较隐蔽的探访视频然后放YouTube上赚点广告费。想了很久，终于跑到忍不住&lt;br&gt;跑到华强北去
      
    
    </summary>
    
    
      <category term="life" scheme="http://mikolaje.github.io/categories/life/"/>
    
    
      <category term="life" scheme="http://mikolaje.github.io/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu的.so文件问题</title>
    <link href="http://mikolaje.github.io/2018/ubuntu_shared_object.html"/>
    <id>http://mikolaje.github.io/2018/ubuntu_shared_object.html</id>
    <published>2018-11-06T14:27:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>昨天在anaconda的虚拟环境py3 下装uwsgi</p><p>但执行uwsgi命令的时候报错提示：<br>uwsgi: error while loading shared libraries: libpcre.so.1: cannot open…</p><p>找不到libpcre.so.1 这个动态链接库。</p><p>这个东西叫做 动态链接库。</p><ul><li>An .so file is a compiled library file. It stands for “Shared Object” and is analogous to a Windows DLL</li></ul><h3 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h3><p>首先我学习了几个linux命令：</p><ol><li>ldd命令用于打印程序或者库文件所依赖的共享库列表。<br>2.ldconfig命令的用途主要是在默认搜寻目录/lib和/usr/lib以及动态库配置文件/etc/ld.so.conf内所列的目录下，搜索出可共享的动态链接库（格式如lib<em>.so</em>）,进而创建出动态装入程序(ld.so)所需的连接和缓存文件。<br>3.locate 让使用者可以很快速的搜寻档案系统内是否有指定的档案。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">ldconfig /opt/anaconda3/lib/</div><div class="line">ldd /data/software/anaconda2/bin/uwsgi</div><div class="line">locate libcrypto.so.1.0.0</div><div class="line"></div><div class="line">libcrypto.so.1.0.0 =&gt; /data/software/anaconda2/lib/libcrypto.so.1.0.0的  (0x00007f90b5695000)</div><div class="line">表示libcrypto.so.1.0.0 用的是 /data/software/anaconda2/lib/libcrypto.so.1.0.0的 动态 链接库</div><div class="line">用locate libcrypto.so.1.0.0 查看系统一共有几个 libcrypto.so.1.0.0 动态链接库</div></pre></td></tr></table></figure><p>ldconfig的几点注意事项：</p><ol><li>往/lib和/usr/lib里面加东西，是不用修改/etc/ld.so.conf的，但是完了之后要调一下ldconfig，不然这个library会找不到。</li><li>想往上面两个目录以外加东西的时候，一定要修改/etc/ld.so.conf，然后再调用ldconfig，不然也会找不到。</li><li>比如安装了一个mysql到/usr/local/mysql，mysql有一大堆library在/usr/local/mysql/lib下面，这时就需要在/etc/ld.so.conf下面加一行/usr/local/mysql/lib，保存过后ldconfig一下，新的library才能在程序运行时被找到。</li><li>如果想在这两个目录以外放lib，但是又不想在/etc/ld.so.conf中加东西（或者是没有权限加东西）。那也可以，就是export一个全局变量LD_LIBRARY_PATH，然后运行程序的时候就会去这个目录中找library。一般来讲这只是一种临时的解决方案，在没有权限或临时需要的时候使用</li></ol><p>in my case:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">add the  line below in /etc/profile:</div><div class="line">export LD_LIBRARY_PATH=/data/software/anaconda2/lib</div><div class="line"></div><div class="line">ldconfig /lib/x86_64-linux-gnu/</div></pre></td></tr></table></figure></p><p>finished!</p><blockquote><p>另外 LD_LIBRARY_PATH的优先级是最高的。</p></blockquote><p>The order is documented in the manual of the dynamic linker, which is ld.so. It is:</p><ol><li>directories from LD_LIBRARY_PATH;</li><li>directories from /etc/ld.so.conf;</li><li>/lib;</li><li>/usr/lib.<br>(I’m simplifying a little, see the manual for the full details.)</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">The order makes sense when you consider that it&apos;s the only way to override a library in a default location with a custom library.</div><div class="line">LD_LIBRARY_PATH is a user setting, it has to come before the others. /etc/ld.so.conf is a local setting, it comes before the operating system default.</div><div class="line">So as a user, if I want to run a program with a different version of a library, I can run the program with LD_LIBRARY_PATH containing the location of that different library version.</div><div class="line">And as an administrator, I can put a different version of the library in /usr/local/lib and list /usr/local/lib in /etc/ld.so.conf.</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;昨天在anaconda的虚拟环境py3 下装uwsgi&lt;/p&gt;
&lt;p&gt;但执行uwsgi命令的时候报错提示：&lt;br&gt;uwsgi: error 
      
    
    </summary>
    
    
      <category term="运维" scheme="http://mikolaje.github.io/categories/%E8%BF%90%E7%BB%B4/"/>
    
    
      <category term="ubuntu" scheme="http://mikolaje.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>Hive UDF 快速教程</title>
    <link href="http://mikolaje.github.io/2018/hive_udf.html"/>
    <id>http://mikolaje.github.io/2018/hive_udf.html</id>
    <published>2018-08-02T04:07:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="Step1"><a href="#Step1" class="headerlink" title="Step1"></a>Step1</h3><p>首先，我们创建一个目录 udf_test/;<br>创建子目录org/dennis/udf<br>在子目录里创建一个MyUpper.java文件。里面内容为：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> org.dennis.udf;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">MyUpper</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(<span class="keyword">final</span> Text s)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (s == <span class="keyword">null</span>) &#123; <span class="keyword">return</span> <span class="keyword">null</span>; &#125;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Text(s.toString().toUpperCase());</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h3 id="Step2"><a href="#Step2" class="headerlink" title="Step2"></a>Step2</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">javac -cp  /opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/hive-exec-2.1.1-cdh6.2.0.jar): \</div><div class="line">/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/jars/hadoop-common.jar \</div><div class="line">org/dennis/udf/MyUpper.java</div></pre></td></tr></table></figure><p>这里我们要把依赖的jar包写进来，这里我们依赖hive-exec*.jar 以及 hadoop-common.jar</p><p>javac -cp 的作用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">javac -cp 指明了.java文件里import的类的位置</div><div class="line"></div><div class="line">java -cp 指明了执行这个class文件所需要的所有类的包路径-即系统类加载器的路径（涉及到类加载机制）</div><div class="line"></div><div class="line">路径在linux中用：隔开  在windows中用；隔开</div></pre></td></tr></table></figure></p><h3 id="Step3"><a href="#Step3" class="headerlink" title="Step3"></a>Step3</h3><p>生成一个jar文件。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">jar -cf myudfs.jar  -C . .</div><div class="line">会在当前目录下生成myudfs.jar 文件</div></pre></td></tr></table></figure></p><h3 id="Step4"><a href="#Step4" class="headerlink" title="Step4"></a>Step4</h3><p>进入Hive命令行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hive&gt; add jar myudfs.jar;</div><div class="line">Added [myudfs.jar] to class path</div><div class="line">Added resources: [myudfs.jar]</div><div class="line">hive&gt; create temporary function dennisUpper as &apos;org.dennis.udf.MyUpper&apos;;</div><div class="line">OK</div><div class="line">Time taken: 0.067 seconds</div><div class="line">hive&gt;</div></pre></td></tr></table></figure></p><p>搞定！</p><p>另外，也可以在Hue上点击 setting后，上传对应的jar文件，然后将对应的function name 和 class name也填一下就OK了。<br><img src="/images/2018/hive_udf/hue_udf_upload.jpg" alt="Sample Image Added via Markdown"></p><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><p>有时候我们add jar后，create function时候会出现如下报错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Added resources: [myudfs.jar]</div><div class="line">hive&gt; create temporary function dennisUpper as &apos;org.dennis.udf.MyUpper&apos;;</div><div class="line">FAILED: Class org.dennis.udf.MyUpper not found</div><div class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.FunctionTask</div></pre></td></tr></table></figure></p><p>我们可以解压一下jar包检查下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">unzip myudf.jar -d check_jar</div><div class="line">解压到 check_jar目录</div></pre></td></tr></table></figure></p><p>如果有的话，check_dir下应该是有这么一个目录文件:<br><code>org/dennis/udf/MyUpper.class</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;正文&quot;&gt;&lt;a href=&quot;#正文&quot; class=&quot;headerlink&quot; title=&quot;正文&quot;&gt;&lt;/a&gt;正文&lt;/h2&gt;&lt;h3 id=&quot;Step1&quot;&gt;&lt;a href=&quot;#Step1&quot; class=&quot;headerlink&quot; title=&quot;Step1&quot;&gt;&lt;/a&gt;Step
      
    
    </summary>
    
    
      <category term="大数据" scheme="http://mikolaje.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hive" scheme="http://mikolaje.github.io/tags/hive/"/>
    
      <category term="UDF" scheme="http://mikolaje.github.io/tags/UDF/"/>
    
  </entry>
  
  <entry>
    <title>Hive 优化</title>
    <link href="http://mikolaje.github.io/2018/hive_optimization.html"/>
    <id>http://mikolaje.github.io/2018/hive_optimization.html</id>
    <published>2018-06-02T04:07:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<p>对于如果join中有小表的话，可以开启map</p><h3 id="Dynamic-Partition-Pruning-for-Hive-Map-Joins"><a href="#Dynamic-Partition-Pruning-for-Hive-Map-Joins" class="headerlink" title="Dynamic Partition Pruning for Hive Map Joins"></a>Dynamic Partition Pruning for Hive Map Joins</h3><p>You can enable dynamic partition pruning for map joins when you are running Hive on Spark (HoS), it is not available for Hive on MapReduce.<br>Dynamic partition pruning (DPP) is a database optimization that can significantly decrease the amount of data that a query scans, thereby executing your workloads faster.<br>DPP achieves this by dynamically determining and eliminating the number of partitions that a query must read from a partitioned table.</p><p>Map joins also optimize how Hive executes queries. They cause a small table to be scanned and loaded in memory as a hash table<br>so that a fast join can be performed entirely within a mapper without having to use another reduce step.<br>If you have queries that join small tables, map joins can make them execute much faster.<br>Map joins are enabled by default in CDH with the Enable MapJoin Optimization setting for HiveServer2 in Cloudera Manager.<br>Hive automatically uses map joins for join queries that involve a set of tables where:</p><ul><li>There is one large table and there is no limit on the size of that large table.</li><li>All other tables involved in the join must have an aggregate size under the value set for Hive Auto Convert Join Noconditional Size for HiveServer2, which is set to 20MB by default in Cloudera Manager.</li></ul><p>关于map-side join的配置:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">SET hive.auto.convert.join=true;</div><div class="line">SET hive.auto.convert.join.noconditionaltask.size=&lt;number_in_megabytes&gt;;</div></pre></td></tr></table></figure></p><h2 id="一次调优实战"><a href="#一次调优实战" class="headerlink" title="一次调优实战"></a>一次调优实战</h2><p>最近在ETL过程中发现有条SQL执行时间非常长，其实数据量很小的，但为什么这么长呢。我带着极度好奇，抱着死缠烂打的精神，怎么也要把<br>问题给解决掉。SQL是这样的:<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">from</span> zhihu_answer </div><div class="line"><span class="keyword">where</span> ym <span class="keyword">in</span> (<span class="keyword">select</span> <span class="keyword">distinct</span>(ym) <span class="keyword">from</span> zhihu.zhihu_answer_increment);</div></pre></td></tr></table></figure></p><p>先说说这两个表数据量吧：<br>zhihu_answer数据量大概是一亿，zhihu_answer_increment 也就是几十万条。<br>首先，我用<code>explain extended</code>查看下执行计划:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div></pre></td><td class="code"><pre><div class="line">explain extended</div><div class="line">select count(1) from zhihu_answer </div><div class="line">where ym in (select distinct(ym) from zhihu.zhihu_answer_increment);</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">Explain</div><div class="line">STAGE DEPENDENCIES:</div><div class="line">  Stage-3 is a root stage</div><div class="line">  Stage-5 depends on stages: Stage-3 , consists of Stage-6, Stage-1</div><div class="line">  Stage-6 has a backup stage: Stage-1</div><div class="line">  Stage-4 depends on stages: Stage-6</div><div class="line">  Stage-2 depends on stages: Stage-1, Stage-4</div><div class="line">  Stage-1</div><div class="line">  Stage-0 depends on stages: Stage-2</div><div class="line"></div><div class="line">STAGE PLANS:</div><div class="line">  Stage: Stage-3</div><div class="line">    Map Reduce</div><div class="line">      Map Operator Tree:</div><div class="line">          TableScan</div><div class="line">            alias: zhihu_answer_increment</div><div class="line">            filterExpr: ym is not null (type: boolean)</div><div class="line">            Statistics: Num rows: 347468 Data size: 73315748 Basic stats: COMPLETE Column stats: COMPLETE</div><div class="line">            GatherStats: false</div><div class="line">            Select Operator</div><div class="line">              expressions: ym (type: string)</div><div class="line">              outputColumnNames: ym</div><div class="line">              Statistics: Num rows: 347468 Data size: 73315748 Basic stats: COMPLETE Column stats: COMPLETE</div><div class="line">              Group By Operator</div><div class="line">                keys: ym (type: string)</div><div class="line">                mode: hash</div><div class="line">                outputColumnNames: _col0</div><div class="line">                Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE</div><div class="line">                Reduce Output Operator</div><div class="line">                  key expressions: _col0 (type: string)</div><div class="line">                  null sort order: a</div><div class="line">                  sort order: +</div><div class="line">                  Map-reduce partition columns: _col0 (type: string)</div><div class="line">                  Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE</div><div class="line">                  tag: -1</div><div class="line">                  auto parallelism: false</div><div class="line">      Execution mode: vectorized</div><div class="line">      Path -&gt; Alias:</div><div class="line">        nullscan://null/zhihu.zhihu_answer_increment/part_ym=201902_ [sq_1:zhihu_answer_increment]</div><div class="line">      Path -&gt; Partition:</div><div class="line">        nullscan://null/zhihu.zhihu_answer_increment/part_ym=201902_ </div><div class="line">          Partition</div><div class="line">            input format: org.apache.hadoop.hive.ql.io.OneNullRowInputFormat</div><div class="line">            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</div><div class="line">            partition values:</div><div class="line">              ym 201902</div><div class="line">            properties:</div><div class="line">              COLUMN_STATS_ACCURATE &#123;&quot;BASIC_STATS&quot;:&quot;true&quot;&#125;</div><div class="line">              bucket_count 256</div><div class="line">              bucket_field_name answer_id</div><div class="line">              columns admin_closed_comment,answer_content,answer_created,answer_id,answer_updated,author_headline,author_id,author_name,author_type,author_url_token,avatar_url,badge_num,can_comment,comment_count,gender,insert_time,is_advertiser,is_collapsed,is_copyable,is_org,question_created,question_id,question_title,question_type,reward_member_count,reward_total_money,voteup_count</div><div class="line">              columns.comments </div><div class="line">              columns.types boolean:string:string:string:string:string:string:string:string:string:string:smallint:boolean:int:string:string:boolean:boolean:boolean:boolean:string:string:string:string:int:int:int</div><div class="line">              file.inputformat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat</div><div class="line">              file.outputformat org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat</div><div class="line">              location hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer_increment/ym=201902</div><div class="line">              name zhihu.zhihu_answer_increment</div><div class="line">              numFiles 256</div><div class="line">              numRows 347468</div><div class="line">              partition_columns ym</div><div class="line">              partition_columns.types string</div><div class="line">              rawDataSize 9381636</div><div class="line">              serialization.ddl struct zhihu_answer_increment &#123; bool admin_closed_comment, string answer_content, string answer_created, string answer_id, string answer_updated, string author_headline, string author_id, string author_name, string author_type, string author_url_token, string avatar_url, i16 badge_num, bool can_comment, i32 comment_count, string gender, string insert_time, bool is_advertiser, bool is_collapsed, bool is_copyable, bool is_org, string question_created, string question_id, string question_title, string question_type, i32 reward_member_count, i32 reward_total_money, i32 voteup_count&#125;</div><div class="line">              serialization.format 1</div><div class="line">              serialization.lib org.apache.hadoop.hive.serde2.NullStructSerDe</div><div class="line">              totalSize 433473813</div><div class="line">              transient_lastDdlTime 1571983508</div><div class="line">            serde: org.apache.hadoop.hive.serde2.NullStructSerDe</div><div class="line">          </div><div class="line">              input format: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat</div><div class="line">              output format: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat</div><div class="line">              properties:</div><div class="line">                bucket_count 256</div><div class="line">                bucket_field_name answer_id</div><div class="line">                columns admin_closed_comment,answer_content,answer_created,answer_id,answer_updated,author_headline,author_id,author_name,author_type,author_url_token,avatar_url,badge_num,can_comment,comment_count,gender,insert_time,is_advertiser,is_collapsed,is_copyable,is_org,question_created,question_id,question_title,question_type,reward_member_count,reward_total_money,voteup_count</div><div class="line">                columns.comments </div><div class="line">                columns.types boolean:string:string:string:string:string:string:string:string:string:string:smallint:boolean:int:string:string:boolean:boolean:boolean:boolean:string:string:string:string:int:int:int</div><div class="line">                file.inputformat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat</div><div class="line">                file.outputformat org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat</div><div class="line">                location hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer_increment</div><div class="line">                name zhihu.zhihu_answer_increment</div><div class="line">                partition_columns ym</div><div class="line">                partition_columns.types string</div><div class="line">                serialization.ddl struct zhihu_answer_increment &#123; bool admin_closed_comment, string answer_content, string answer_created, string answer_id, string answer_updated, string author_headline, string author_id, string author_name, string author_type, string author_url_token, string avatar_url, i16 badge_num, bool can_comment, i32 comment_count, string gender, string insert_time, bool is_advertiser, bool is_collapsed, bool is_copyable, bool is_org, string question_created, string question_id, string question_title, string question_type, i32 reward_member_count, i32 reward_total_money, i32 voteup_count&#125;</div><div class="line">                serialization.format 1</div><div class="line">                serialization.lib org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe</div><div class="line">                transient_lastDdlTime 1571983018</div><div class="line">              serde: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe</div><div class="line">              name: zhihu.zhihu_answer_increment</div><div class="line">            name: zhihu.zhihu_answer_increment</div><div class="line">      Truncated Path -&gt; Alias:</div><div class="line">        nullscan://null/zhihu.zhihu_answer_increment/part_ym=201902_ [sq_1:zhihu_answer_increment]</div><div class="line">      Needs Tagging: false</div><div class="line">      Reduce Operator Tree:</div><div class="line">        Group By Operator</div><div class="line">          keys: KEY._col0 (type: string)</div><div class="line">          mode: mergepartial</div><div class="line">          outputColumnNames: _col0</div><div class="line">          Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE</div><div class="line">          Group By Operator</div><div class="line">            keys: _col0 (type: string)</div><div class="line">            mode: hash</div><div class="line">            outputColumnNames: _col0</div><div class="line">            Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE</div><div class="line">            File Output Operator</div><div class="line">              compressed: false</div><div class="line">              GlobalTableId: 0</div><div class="line">              directory: hdfs://device1:8020/tmp/hive/hive/7f0887a3-8c5a-44b6-b5ef-f0c7530a6b15/hive_2019-10-25_14-46-02_198_8962679143430564511-1/-mr-10004</div><div class="line">              NumFilesPerFileSink: 1</div><div class="line">              table:</div><div class="line">                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat</div><div class="line">                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</div><div class="line">                  properties:</div><div class="line">                    columns _col0</div><div class="line">                    columns.types string</div><div class="line">                    escape.delim \</div><div class="line">                    serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe</div><div class="line">                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe</div><div class="line">              TotalFiles: 1</div><div class="line">              GatherStats: false</div><div class="line">              MultiFileSpray: false</div><div class="line"></div><div class="line">  Stage: Stage-5</div><div class="line">    Conditional Operator</div><div class="line"></div><div class="line">  Stage: Stage-6</div><div class="line">    Map Reduce Local Work</div><div class="line">      Alias -&gt; Map Local Tables:</div><div class="line">        $INTNAME </div><div class="line">          Fetch Operator</div><div class="line">            limit: -1</div><div class="line">      Alias -&gt; Map Local Operator Tree:</div><div class="line">        $INTNAME </div><div class="line">          TableScan</div><div class="line">            GatherStats: false</div><div class="line">            HashTable Sink Operator</div><div class="line">              keys:</div><div class="line">                0 ym (type: string)</div><div class="line">                1 _col0 (type: string)</div><div class="line">              Position of Big Table: 0</div><div class="line"></div><div class="line">  Stage: Stage-4</div><div class="line">    Map Reduce</div><div class="line">      Map Operator Tree:</div><div class="line">          TableScan</div><div class="line">            alias: zhihu_answer</div><div class="line">            filterExpr: ym is not null (type: boolean)</div><div class="line">            Statistics: Num rows: 102075765 Data size: 21537986328 Basic stats: COMPLETE Column stats: COMPLETE</div><div class="line">            GatherStats: false</div><div class="line">            Map Join Operator</div><div class="line">              condition map:</div><div class="line">                   Left Semi Join 0 to 1</div><div class="line">              keys:</div><div class="line">                0 ym (type: string)</div><div class="line">                1 _col0 (type: string)</div><div class="line">              Position of Big Table: 0</div><div class="line">              Statistics: Num rows: 4253156 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE</div><div class="line">              Group By Operator</div><div class="line">                aggregations: count(1)</div><div class="line">                mode: hash</div><div class="line">                outputColumnNames: _col0</div><div class="line">                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE</div><div class="line">                File Output Operator</div><div class="line">                  compressed: false</div><div class="line">                  GlobalTableId: 0</div><div class="line">                  directory: hdfs://device1:8020/tmp/hive/hive/7f0887a3-8c5a-44b6-b5ef-f0c7530a6b15/hive_2019-10-25_14-46-02_198_8962679143430564511-1/-mr-10003</div><div class="line">                  NumFilesPerFileSink: 1</div><div class="line">                  table:</div><div class="line">                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat</div><div class="line">                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</div><div class="line">                      properties:</div><div class="line">                        columns _col0</div><div class="line">                        columns.types bigint</div><div class="line">                        escape.delim \</div><div class="line">                        serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe</div><div class="line">                      serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe</div><div class="line">                  TotalFiles: 1</div><div class="line">                  GatherStats: false</div><div class="line">                  MultiFileSpray: false</div><div class="line">      Local Work:</div><div class="line">        Map Reduce Local Work</div><div class="line">      Path -&gt; Alias:</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201705 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201706 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201707 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201708 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201709 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201710 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201711 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201712 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201801 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201802 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201803 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201804 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201805 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201806 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201807 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201808 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201809 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201810 [zhihu_answer]</div><div class="line">        hdfs://device1:8020/user/hive/warehouse/zhihu.db/zhihu_answer/ym=201811 [zhihu_answer]</div></pre></td></tr></table></figure></p><p>一脸懵逼，不会看呀。。<br>然后我测试了下单独执行：<code>select distinct(ym) from zhihu_answer_increment;</code>，也就不到2分钟就出结果了。为什么组合在一起就要这么长时间呢？？<br>这条SQL的执行结果就是<code>&quot;201902&quot;</code>。我把这个结果复制进去执行：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">from</span> zhihu_answer <span class="keyword">where</span> ym <span class="keyword">in</span> (<span class="number">201902</span>);</div></pre></td></tr></table></figure></p><p>可能是因为我之前执行过的原因，这条语句的执行时间基本上是秒出呀。几秒内就出结果了。</p><p>我再一次执行了那句执行时间很长的SQL，看它的执行时候的log，我发现慢原因是在Stage-4 ！！！回到上面那个explain的信息，我发现Hive在做全表扫描呀！Why?<br>为什么要做全表扫描呢？ 因为Hive还是要join的 in （select ** ） 这种子查询中用的是semi join，所以要进行join，它就会进行全表扫描。我的解释不是很详细，<br>但隐隐约约我能理解为什么Hive在这要做全表扫描了，其实如果写死的话，比如where ym in （201902）它就不会做join，也就不用全表扫描了。所以解决方案还是要能<br>拿到 <code>201902</code>这个变量，这个value，再拼接到Hive SQL中。我查了下，Hive貌似目前还不支持以SQL查询结果作为新的SQL变量。所以，暂时还是以这种办法解决吧。</p><p>让我无比开心的是，改进后，SQL执行快了N倍，因为避免了全表扫描。从原来2个小时的执行，变为了几分钟！</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过Explain打印看看执行计划有哪些；<br>通过执行的log看看到底是哪个Stage耗时比较长；</p><h1 id="Referrence"><a href="#Referrence" class="headerlink" title="Referrence"></a>Referrence</h1><p><a href="https://docs.cloudera.com/documentation/enterprise/latest/topics/admin_hos_oview.html#concept_i22_l1h_1v" target="_blank" rel="external">https://docs.cloudera.com/documentation/enterprise/latest/topics/admin_hos_oview.html#concept_i22_l1h_1v</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;对于如果join中有小表的话，可以开启map&lt;/p&gt;
&lt;h3 id=&quot;Dynamic-Partition-Pruning-for-Hive-Map-Joins&quot;&gt;&lt;a href=&quot;#Dynamic-Partition-Pruning-for-Hive-Map-Joins&quot; 
      
    
    </summary>
    
    
      <category term="大数据" scheme="http://mikolaje.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hive" scheme="http://mikolaje.github.io/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Kafka Introduction</title>
    <link href="http://mikolaje.github.io/2018/kafka_intro.html"/>
    <id>http://mikolaje.github.io/2018/kafka_intro.html</id>
    <published>2018-03-02T04:07:10.000Z</published>
    <updated>2019-11-20T06:09:57.423Z</updated>
    
    <content type="html"><![CDATA[<ul><li>最近面试经常会被问到Kafka的问题。看了很多Kafka的介绍，发现还是美团技术团队总结的最清楚易懂。下面转了美团的关于Kafka的一篇文章</li></ul><h2 id="Kafka是什么"><a href="#Kafka是什么" class="headerlink" title="Kafka是什么"></a>Kafka是什么</h2><p>Kafka是最初由Linkedin公司开发，是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统(也可以当做MQ系统)，常见可以用于web/nginx日志、访问日志，消息服务等等，Linkedin于2010年贡献给了Apache基金会并成为顶级开源项目。</p><p>一个商业化消息队列的性能好坏，其文件存储机制设计是衡量一个消息队列服务技术水平和最关键指标之一。 下面将从Kafka文件存储机制和物理结构角度，分析Kafka是如何实现高效文件存储，及实际应用效果。</p><p>Kafka部分名词解释如下：</p><ul><li>Broker：消息中间件处理结点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群。</li><li>Topic：一类消息，例如page view日志、click日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发。</li><li>Partition：topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。</li><li>Segment：partition物理上由多个segment组成，下面2.2和2.3有详细说明。</li><li>offset：每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中。partition中的每个消息都有一个连续的序列号叫做offset,用于partition唯一标识一条消息.</li></ul><p>分析过程分为以下4个步骤：</p><ul><li>topic中partition存储分布</li><li>partiton中文件存储方式</li><li>partiton中segment文件存储结构</li><li>在partition中如何通过offset查找message</li></ul><p>通过上述4过程详细分析，我们就可以清楚认识到kafka文件存储机制的奥秘。</p><h2 id="2-1-topic中partition存储分布"><a href="#2-1-topic中partition存储分布" class="headerlink" title="2.1 topic中partition存储分布"></a>2.1 topic中partition存储分布</h2><p>假设实验环境中Kafka集群只有一个broker，xxx/message-folder为数据文件存储根目录，在Kafka broker中server.properties文件配置(参数log.dirs=xxx/message-folder)，例如创建2个topic名称分别为report_push、launch_info, partitions数量都为partitions=4 存储路径和目录规则为： xxx/message-folder<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">|--report_push-0</div><div class="line">|--report_push-1</div><div class="line">|--report_push-2</div><div class="line">|--report_push-3</div><div class="line">|--launch_info-0</div><div class="line">|--launch_info-1</div><div class="line">|--launch_info-2</div><div class="line">|--launch_info-3</div></pre></td></tr></table></figure></p><p>在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。 如果是多broker分布情况，请参考kafka集群partition分布原理分析</p><h2 id="2-2-partiton中文件存储方式"><a href="#2-2-partiton中文件存储方式" class="headerlink" title="2.2 partiton中文件存储方式"></a>2.2 partiton中文件存储方式</h2><p>下面示意图形象说明了partition中文件存储方式:</p><p><img src="/images/2018/kafka_intro/kafka_partition.png" alt="Sample Image Added via Markdown"></p><ul><li>每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。但每个段segment file消息数量不一定相等，这种特性方便old segment file快速被删除。</li><li>每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。</li></ul><p>这样做的好处就是能快速删除无用文件，有效提高磁盘利用率。</p><h2 id="2-3-partiton中segment文件存储结构"><a href="#2-3-partiton中segment文件存储结构" class="headerlink" title="2.3 partiton中segment文件存储结构"></a>2.3 partiton中segment文件存储结构</h2><p>读者从2.2节了解到Kafka文件系统partition存储方式，本节深入分析partion中segment file组成和物理结构。</p><ul><li>segment file组成：由2大部分组成，分别为index file和data file，此2个文件一一对应，成对出现，后缀”.index”和“.log”分别表示为segment索引文件、数据文件.</li><li>segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值。数值最大为64位long大小，19位数字字符长度，没有数字用0填充。</li></ul><p>下面文件列表是笔者在Kafka broker上做的一个实验，创建一个topicXXX包含1 partition，设置每个segment大小为500MB,并启动producer向Kafka broker写入大量数据,如下图2所示segment文件列表形象说明了上述2个规则：</p><p><img src="/images/2018/kafka_intro/kafka_index.png" alt="Sample Image Added via Markdown"></p><p>以上述图2中一对segment file文件为例，说明segment中index&lt;—-&gt;data file对应关系物理结构如下：</p><p>上述图3中索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。 其中以索引文件中元数据3,497为例，依次在数据文件中表示第3个message(在全局partiton表示第368772个message)、以及该消息的物理偏移地址为497。</p><p>从上述图3了解到segment data file由许多message组成，下面详细说明message物理结构如下：<br><img src="/images/2018/kafka_intro/kafka_message.png" alt="Sample Image Added via Markdown"></p><p>参数说明：<br>关键字    解释说明<br>8 byte offset    在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message<br>4 byte message size    message大小<br>4 byte CRC32    用crc32校验message<br>1 byte “magic”    表示本次发布Kafka服务程序协议版本号<br>1 byte “attributes”    表示为独立版本、或标识压缩类型、或编码类型。<br>4 byte key length    表示key的长度,当key为-1时，K byte key字段不填<br>K byte key    可选<br>value bytes payload    表示实际消息数据。</p><h2 id="2-4-在partition中如何通过offset查找message"><a href="#2-4-在partition中如何通过offset查找message" class="headerlink" title="2.4 在partition中如何通过offset查找message"></a>2.4 在partition中如何通过offset查找message</h2><p>例如读取offset=368776的message，需要通过下面2个步骤查找。</p><p>第一步查找segment file 上述图2为例，其中00000000000000000000.index表示最开始的文件，起始偏移量(offset)为0.第二个文件00000000000000368769.index的消息量起始偏移量为368770 = 368769 + 1.同样，第三个文件00000000000000737337.index的起始偏移量为737338=737337 + 1，其他后续文件依次类推，以起始偏移量命名并排序这些文件，只要根据offset <strong>二分查找</strong>文件列表，就可以快速定位到具体文件。 当offset=368776时定位到00000000000000368769.index|log</p><p>第二步通过segment file查找message 通过第一步定位到segment file，当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和00000000000000368769.log的物理偏移地址，然后再通过00000000000000368769.log顺序查找直到offset=368776为止。</p><p>从上述图3可知这样做的优点，segment index file采取稀疏索引存储方式，它减少索引文件大小，通过mmap可以直接内存操作，稀疏索引为数据文件的每个对应message设置一个元数据指针,它比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间。</p><p>3 Kafka文件存储机制–实际运行效果<br>实验环境：</p><p>Kafka集群：由2台虚拟机组成<br>cpu：4核<br>物理内存：8GB<br>网卡：千兆网卡<br>jvm heap: 4GB<br>详细Kafka服务端配置及其优化请参考：kafka server.properties配置详解<br><img src="/images/2018/kafka_intro/kafka_read_disk.png" alt="Sample Image Added via Markdown"></p><p>从上述图5可以看出，Kafka运行时很少有大量读磁盘的操作，主要是定期批量写磁盘操作，因此操作磁盘很高效。这跟Kafka文件存储中读写message的设计是息息相关的。Kafka中读写message有如下特点:</p><h3 id="写message"><a href="#写message" class="headerlink" title="写message"></a>写message</h3><ul><li>消息从java堆转入page cache(即物理内存)。</li><li>由异步线程刷盘,消息从page cache刷入磁盘。</li></ul><h3 id="读message"><a href="#读message" class="headerlink" title="读message"></a>读message</h3><ul><li>消息直接从page cache转入socket发送出去。</li><li>当从page cache没有找到相应数据时，此时会产生磁盘IO,从磁 盘Load消息到page cache,然后直接从socket发出去</li></ul><h3 id="Kafka高效文件存储设计特点"><a href="#Kafka高效文件存储设计特点" class="headerlink" title="Kafka高效文件存储设计特点"></a>Kafka高效文件存储设计特点</h3><ul><li><p>Kafka把topic中一个partition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。</p></li><li><p>通过索引信息可以快速定位message和确定response的最大大小。</p></li><li>通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作。</li><li>通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。</li></ul><p>1.Linux Page Cache机制 </p><p>2.Kafka官方文档</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>面试的时候经常问到Kafka怎么防止重复消费</p><ul><li><p>比如，你拿到这个消息做数据库的insert操作，那就容易了，给这个消息做一个唯一的主键，那么就算出现重复消费的情况，就会导致主键冲突，避免数据库出现脏数据。</p></li><li><p>再比如，你拿到这个消息做Redis的set的操作，那就容易了，不用解决，因为你无论set几次结果都是一样的，set操作本来就算幂等操作。</p></li><li><p>如果上面两种情况还不行，上大招。准备一个第三方介质，来做消费记录。以Redis为例，给消息分配一个全局id，只要消费过该消息，将<id,message>以K-V形式写入Redis. 那消费者开始消费前，先去Redis中查询有没有消费记录即可。</id,message></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;最近面试经常会被问到Kafka的问题。看了很多Kafka的介绍，发现还是美团技术团队总结的最清楚易懂。下面转了美团的关于Kafka的一篇文章&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Kafka是什么&quot;&gt;&lt;a href=&quot;#Kafka是什么&quot; class=&quot;head
      
    
    </summary>
    
    
      <category term="大数据" scheme="http://mikolaje.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="kafka" scheme="http://mikolaje.github.io/tags/kafka/"/>
    
  </entry>
  
</feed>
